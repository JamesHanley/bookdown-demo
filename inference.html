<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Statistical Inference | Introduction to Statistical Analysis: a regression-from-the-outset approach</title>
  <meta name="description" content="A regression-from-the-outset based approach" />
  <meta name="generator" content="bookdown 0.18.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Statistical Inference | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A regression-from-the-outset based approach" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Statistical Inference | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  
  <meta name="twitter:description" content="A regression-from-the-outset based approach" />
  

<meta name="author" content="Sahir, Shirin and Jim" />


<meta name="date" content="2020-04-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="paras.html"/>
<link rel="next" href="CI.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">In Planning Stage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#target"><i class="fa fa-check"></i><b>0.1</b> Target</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#topicstextbooks"><i class="fa fa-check"></i><b>0.2</b> Topics/textbooks</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#regression-from-the-outset"><i class="fa fa-check"></i><b>0.3</b> Regression from the outset</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#parameters-first-data-later"><i class="fa fa-check"></i><b>0.4</b> Parameters first, data later</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#lets-switch-to-y-bar-and-drop-x-bar."><i class="fa fa-check"></i><b>0.5</b> Let’s switch to “y-bar”, and drop “x-bar”.</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#computing-from-the-outset"><i class="fa fa-check"></i><b>0.6</b> Computing from the outset</a></li>
<li class="chapter" data-level="0.7" data-path="index.html"><a href="index.html#appendix"><i class="fa fa-check"></i><b>0.7</b> Appendix:</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#goals"><i class="fa fa-check"></i><b>1.1</b> Goals</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#structure"><i class="fa fa-check"></i><b>1.2</b> Structure</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#attitudes-etc."><i class="fa fa-check"></i><b>1.3</b> Attitudes, etc….</a></li>
</ul></li>
<li class="part"><span><b>I Part I</b></span></li>
<li class="chapter" data-level="2" data-path="paras.html"><a href="paras.html"><i class="fa fa-check"></i><b>2</b> Statistical Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="paras.html"><a href="paras.html#parameters"><i class="fa fa-check"></i><b>2.1</b> Parameters</a></li>
<li class="chapter" data-level="2.2" data-path="paras.html"><a href="paras.html#parameter-contrasts"><i class="fa fa-check"></i><b>2.2</b> Parameter Contrasts</a><ul>
<li class="chapter" data-level="2.2.1" data-path="paras.html"><a href="paras.html#parameter-relations-in-numbers-and-words"><i class="fa fa-check"></i><b>2.2.1</b> Parameter relations in numbers and words</a></li>
<li class="chapter" data-level="2.2.2" data-path="paras.html"><a href="paras.html#parameter-relations-in-symbols-and-with-the-help-of-an-index-category-indicator"><i class="fa fa-check"></i><b>2.2.2</b> Parameter relations in symbols, and with the help of an index-category indicator</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="paras.html"><a href="paras.html#parameter-functions"><i class="fa fa-check"></i><b>2.3</b> Parameter functions</a></li>
<li class="chapter" data-level="2.4" data-path="paras.html"><a href="paras.html#phraseology-to-avoid"><i class="fa fa-check"></i><b>2.4</b> Phraseology to avoid</a></li>
<li class="chapter" data-level="2.5" data-path="paras.html"><a href="paras.html#summary"><i class="fa fa-check"></i><b>2.5</b> SUMMARY</a></li>
<li class="chapter" data-level="2.6" data-path="paras.html"><a href="paras.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="paras.html"><a href="paras.html#references"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html#the-bayesian-approach"><i class="fa fa-check"></i><b>3.1</b> The Bayesian Approach</a><ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#example-parameter-is-2-valued-yes-or-no"><i class="fa fa-check"></i><b>3.1.1</b> Example: parameter is 2-valued: yes or no</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference.html"><a href="inference.html#example-parameter-is-a-proportion"><i class="fa fa-check"></i><b>3.1.2</b> Example: parameter is a proportion</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference.html"><a href="inference.html#example-parameter-is-a-mean"><i class="fa fa-check"></i><b>3.1.3</b> Example: parameter is a mean</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inference.html"><a href="inference.html#frequentist-approach"><i class="fa fa-check"></i><b>3.2</b> Frequentist approach</a></li>
<li class="chapter" data-level="3.3" data-path="inference.html"><a href="inference.html#does-it-matter"><i class="fa fa-check"></i><b>3.3</b> Does it matter?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Parameter Intervals</a><ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> ‘100% confidence’ intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#more-nuanced-intervals"><i class="fa fa-check"></i><b>4.2</b> More-nuanced intervals</a></li>
<li class="chapter" data-level="4.3" data-path="CI.html"><a href="CI.html#summary-1"><i class="fa fa-check"></i><b>4.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="paraMu.html"><a href="paraMu.html"><i class="fa fa-check"></i><b>5</b> The ‘mean’ parameter <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="paraMu.html"><a href="paraMu.html#two-genres"><i class="fa fa-check"></i><b>5.1</b> Two genres</a></li>
<li class="chapter" data-level="5.2" data-path="paraMu.html"><a href="paraMu.html#fitting-these-to-data-estimating-them-from-data"><i class="fa fa-check"></i><b>5.2</b> Fitting these to data / Estimating them from data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="paraPi.html"><a href="paraPi.html"><i class="fa fa-check"></i><b>6</b> The (proportion) parameter</a><ul>
<li class="chapter" data-level="6.1" data-path="paraPi.html"><a href="paraPi.html#example-one"><i class="fa fa-check"></i><b>6.1</b> Example one</a></li>
<li class="chapter" data-level="6.2" data-path="paraPi.html"><a href="paraPi.html#example-two"><i class="fa fa-check"></i><b>6.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="paraLambda.html"><a href="paraLambda.html"><i class="fa fa-check"></i><b>7</b> The (event rate) parameter</a><ul>
<li class="chapter" data-level="7.1" data-path="paraLambda.html"><a href="paraLambda.html#etc"><i class="fa fa-check"></i><b>7.1</b> Etc</a></li>
<li class="chapter" data-level="7.2" data-path="paraLambda.html"><a href="paraLambda.html#etc-1"><i class="fa fa-check"></i><b>7.2</b> ETC</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html"><i class="fa fa-check"></i><b>8</b> Contrast: 2 mean parameters</a><ul>
<li class="chapter" data-level="8.1" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html#estimand-estimator-estimate"><i class="fa fa-check"></i><b>8.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html"><i class="fa fa-check"></i><b>9</b> Contrast: 2 proportion parameters</a><ul>
<li class="chapter" data-level="9.1" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html#estimand-estimator-estimate-1"><i class="fa fa-check"></i><b>9.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html"><i class="fa fa-check"></i><b>10</b> Contrast: 2 speed parameters</a><ul>
<li class="chapter" data-level="10.1" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html#estimand-estimator-estimate-2"><i class="fa fa-check"></i><b>10.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="part"><span><b>II Part II</b></span></li>
<li class="chapter" data-level="11" data-path="Probability.html"><a href="Probability.html"><i class="fa fa-check"></i><b>11</b> Probability</a><ul>
<li class="chapter" data-level="11.1" data-path="Probability.html"><a href="Probability.html#conditional-forwards"><i class="fa fa-check"></i><b>11.1</b> Conditional – forwards</a></li>
<li class="chapter" data-level="11.2" data-path="Probability.html"><a href="Probability.html#conditional-reverse"><i class="fa fa-check"></i><b>11.2</b> Conditional – reverse</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Distributions.html"><a href="Distributions.html"><i class="fa fa-check"></i><b>12</b> Distributions /Random Variables</a><ul>
<li class="chapter" data-level="12.1" data-path="Distributions.html"><a href="Distributions.html#gaussian-bernoulli-binomial-poisson"><i class="fa fa-check"></i><b>12.1</b> Gaussian Bernoulli-Binomial Poisson</a></li>
<li class="chapter" data-level="12.2" data-path="Distributions.html"><a href="Distributions.html#expectation-and-variance"><i class="fa fa-check"></i><b>12.2</b> Expectation and Variance</a></li>
<li class="chapter" data-level="12.3" data-path="Distributions.html"><a href="Distributions.html#functionscombinations-of-random-variables"><i class="fa fa-check"></i><b>12.3</b> Functions/combinations of random variables</a></li>
</ul></li>
<li class="part"><span><b>III Part III</b></span></li>
<li class="chapter" data-level="13" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>13</b> Mathematics</a><ul>
<li class="chapter" data-level="13.1" data-path="math.html"><a href="math.html#notation"><i class="fa fa-check"></i><b>13.1</b> Notation</a></li>
<li class="chapter" data-level="13.2" data-path="math.html"><a href="math.html#powers-logarithms-and-antilogarithms"><i class="fa fa-check"></i><b>13.2</b> Powers, Logarithms and Anti–logarithms</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="computing01.html"><a href="computing01.html"><i class="fa fa-check"></i><b>14</b> Computing Session 1</a><ul>
<li class="chapter" data-level="14.1" data-path="computing01.html"><a href="computing01.html#biological-background"><i class="fa fa-check"></i><b>14.1</b> Biological background</a></li>
<li class="chapter" data-level="14.2" data-path="computing01.html"><a href="computing01.html#statistical-task"><i class="fa fa-check"></i><b>14.2</b> Statistical Task</a><ul>
<li class="chapter" data-level="14.2.1" data-path="computing01.html"><a href="computing01.html#the-p-and-q-functions-an-orientation"><i class="fa fa-check"></i><b>14.2.1</b> The p and q functions: an orientation</a></li>
<li class="chapter" data-level="14.2.2" data-path="computing01.html"><a href="computing01.html#exercises-1"><i class="fa fa-check"></i><b>14.2.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="computing01.html"><a href="computing01.html#summary-2"><i class="fa fa-check"></i><b>14.3</b> SUMMARY</a><ul>
<li class="chapter" data-level="14.3.1" data-path="computing01.html"><a href="computing01.html#computing"><i class="fa fa-check"></i><b>14.3.1</b> Computing</a></li>
<li class="chapter" data-level="14.3.2" data-path="computing01.html"><a href="computing01.html#statistical-concepts-and-principles"><i class="fa fa-check"></i><b>14.3.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="computing02.html"><a href="computing02.html"><i class="fa fa-check"></i><b>15</b> Computing: Session No. 2</a><ul>
<li class="chapter" data-level="15.1" data-path="computing02.html"><a href="computing02.html#scientific-background"><i class="fa fa-check"></i><b>15.1</b> Scientific background</a></li>
<li class="chapter" data-level="15.2" data-path="computing02.html"><a href="computing02.html#random-variation"><i class="fa fa-check"></i><b>15.2</b> Random Variation</a><ul>
<li class="chapter" data-level="15.2.1" data-path="computing02.html"><a href="computing02.html#measurement-errors"><i class="fa fa-check"></i><b>15.2.1</b> Measurement errors</a></li>
<li class="chapter" data-level="15.2.2" data-path="computing02.html"><a href="computing02.html#biological-variation"><i class="fa fa-check"></i><b>15.2.2</b> Biological variation</a></li>
<li class="chapter" data-level="15.2.3" data-path="computing02.html"><a href="computing02.html#example-2"><i class="fa fa-check"></i><b>15.2.3</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="computing02.html"><a href="computing02.html#when-these-laws-dont-apply"><i class="fa fa-check"></i><b>15.3</b> When these Laws don’t apply</a></li>
<li class="chapter" data-level="15.4" data-path="computing02.html"><a href="computing02.html#summary-3"><i class="fa fa-check"></i><b>15.4</b> SUMMARY</a><ul>
<li class="chapter" data-level="15.4.1" data-path="computing02.html"><a href="computing02.html#computing-1"><i class="fa fa-check"></i><b>15.4.1</b> Computing</a></li>
<li class="chapter" data-level="15.4.2" data-path="computing02.html"><a href="computing02.html#statistical-concepts-and-principles-1"><i class="fa fa-check"></i><b>15.4.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="computing03.html"><a href="computing03.html"><i class="fa fa-check"></i><b>16</b> Computing Week3</a><ul>
<li class="chapter" data-level="16.1" data-path="computing03.html"><a href="computing03.html#ages-of-books"><i class="fa fa-check"></i><b>16.1</b> Ages of books</a></li>
<li class="chapter" data-level="16.2" data-path="computing03.html"><a href="computing03.html#ngrams"><i class="fa fa-check"></i><b>16.2</b> ngrams</a></li>
<li class="chapter" data-level="16.3" data-path="computing03.html"><a href="computing03.html#ice-breakup-dates"><i class="fa fa-check"></i><b>16.3</b> Ice Breakup Dates</a><ul>
<li class="chapter" data-level="16.3.1" data-path="computing03.html"><a href="computing03.html#the-2018-book-of-guesses"><i class="fa fa-check"></i><b>16.3.1</b> The 2018 Book of Guesses</a></li>
<li class="chapter" data-level="16.3.2" data-path="computing03.html"><a href="computing03.html#trends-over-the-last-100-years"><i class="fa fa-check"></i><b>16.3.2</b> Trends over the last 100 years</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="computing03.html"><a href="computing03.html#galtons-data-on-family-heights"><i class="fa fa-check"></i><b>16.4</b> Galton’s data on family heights</a></li>
<li class="chapter" data-level="16.5" data-path="computing03.html"><a href="computing03.html#temperature-perceptions"><i class="fa fa-check"></i><b>16.5</b> Temperature perceptions</a></li>
<li class="chapter" data-level="16.6" data-path="computing03.html"><a href="computing03.html#natural-history-of-prostate-cancer"><i class="fa fa-check"></i><b>16.6</b> Natural history of prostate cancer</a></li>
<li class="chapter" data-level="16.7" data-path="computing03.html"><a href="computing03.html#serial-psa-values"><i class="fa fa-check"></i><b>16.7</b> Serial PSA values</a></li>
<li class="chapter" data-level="16.8" data-path="computing03.html"><a href="computing03.html#graphics"><i class="fa fa-check"></i><b>16.8</b> Graphics</a></li>
<li class="chapter" data-level="16.9" data-path="computing03.html"><a href="computing03.html#possible-body-mass-indices"><i class="fa fa-check"></i><b>16.9</b> Possible Body Mass Indices</a></li>
<li class="chapter" data-level="16.10" data-path="computing03.html"><a href="computing03.html#galton"><i class="fa fa-check"></i><b>16.10</b> Galton</a></li>
<li class="chapter" data-level="16.11" data-path="computing03.html"><a href="computing03.html#epidemics"><i class="fa fa-check"></i><b>16.11</b> Epidemics</a></li>
<li class="chapter" data-level="16.12" data-path="computing03.html"><a href="computing03.html#duplicate-birthdays"><i class="fa fa-check"></i><b>16.12</b> Duplicate Birthdays</a></li>
<li class="chapter" data-level="16.13" data-path="computing03.html"><a href="computing03.html#lottery-payoffs"><i class="fa fa-check"></i><b>16.13</b> Lottery payoffs</a></li>
<li class="chapter" data-level="16.14" data-path="computing03.html"><a href="computing03.html#chevalier-de-méré"><i class="fa fa-check"></i><b>16.14</b> Chevalier de Méré</a></li>
<li class="chapter" data-level="16.15" data-path="computing03.html"><a href="computing03.html#detecting-a-fake-bernoulli-sequenece"><i class="fa fa-check"></i><b>16.15</b> Detecting a fake Bernoulli sequenece</a></li>
<li class="chapter" data-level="16.16" data-path="computing03.html"><a href="computing03.html#cell-occupancy"><i class="fa fa-check"></i><b>16.16</b> Cell occupancy</a></li>
<li class="chapter" data-level="16.17" data-path="computing03.html"><a href="computing03.html#life-tables"><i class="fa fa-check"></i><b>16.17</b> Life Tables</a></li>
<li class="chapter" data-level="16.18" data-path="computing03.html"><a href="computing03.html#carrier-status-genetics"><i class="fa fa-check"></i><b>16.18</b> Carrier Status (genetics)</a></li>
<li class="chapter" data-level="16.19" data-path="computing03.html"><a href="computing03.html#diagnostic-and-statistical-tests"><i class="fa fa-check"></i><b>16.19</b> Diagnostic and statistical tests</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="dalite.html"><a href="dalite.html"><i class="fa fa-check"></i><b>17</b> DALITE</a><ul>
<li class="chapter" data-level="17.1" data-path="dalite.html"><a href="dalite.html#aim"><i class="fa fa-check"></i><b>17.1</b> Aim</a></li>
<li class="chapter" data-level="17.2" data-path="dalite.html"><a href="dalite.html#how-it-works"><i class="fa fa-check"></i><b>17.2</b> How it works</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Analysis: a regression-from-the-outset approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Statistical Inference</h1>
<p>Google gives the following defintion</p>
<blockquote>
<p>The theory, methods, and practice of forming judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling.</p>
</blockquote>
<p>The Oxford English Dictionary defines it as</p>
<blockquote>
<p>The drawing of inferences about a population based on data taken from a sample of that population; an inference drawn in this way; the branch of statistics concerned with this procedure.</p>
</blockquote>
<p>We would add to these numerical statements about unknown (and unknowable) constants, as well as the mechanism or process that generated the limited data you got/get to observe.</p>
<p>Some example parameters – some scientific, some more personal or particularistic – include</p>
<ul>
<li>Whether</li>
<li>a potential hemophilia carrier is in fact a carrier</li>
<li>a particular email is malicious</li>
<li>a person committed the crime they are accused of</li>
<li><p>a person has been infected with a certain virus</p></li>
<li>The proportion of</li>
<li>thumbtacks that land on their back when tossed</li>
<li>your time that you are being productive</li>
<li>the earth’s surface that is covered by water</li>
<li>your driving time that you are on the phone</li>
<li>your time (over the entire year) that you spend inside</li>
<li>patients whose disease would respond to a medication</li>
<li><p>people who would volunteer for a demanding survey or long-term research study</p></li>
<li>The numerical value for</li>
<li>the density of the Earth,relative to water</li>
<li>the age of a person whom you have just met</li>
<li>your cholesterol level</li>
<li>the mean depth of the ocean</li>
<li>the 20th percentile of the depths of the ocean</li>
<li><p>the median age of a population</p></li>
</ul>
<p>To address the uncertainties involved in the judgements/inferences, some use of probabilities is required.</p>
<p>In their preamble to their chapter on inferemce, Clayton and Hills tell us that</p>
<blockquote>
<p>There are <strong>two radically different approaches</strong> to associating a probability with a range of parameter values, reflecting a deep philosophical division amongst mathematicians and scientists about the nature of probability. We shall start with the more orthodox view within biomedical science.</p>
</blockquote>
<p>Clayton and Hills completed their book in 1993. Since then, propelled by greater computer power, and by people like Clayton’s Cambridge colleague David Spiegelhalter, whose book we will start with, the Bayesian approach to ‘associating a probability with a range of parameter values’ has become more common. It has not yet reached the status of ‘customary or conventional, as a means or method; established.’ that the dictionaries give as the meaning of orthodox. In any case, we should not take Clayton and Hills’ use of the phrase ‘more orthodox’ to describe the <em>frequentist</em> approach to mean that the Bayesian approach <code>does not  conform to the approved form of analysis' or is in some sense</code>wrong.’</p>
<p>The first-established of the two ‘schools’ (or ‘churches’) of statistical inference** makes <strong>direct probabilistic statements about the possible parameter values</strong>. This approach goes back at least as far as the mid-1700’s essay ‘A method of calculating the exact probability of all conclusions based on induction’; ironically the author was a Presbyterian minister.</p>
<p>The developments since then are nicely told in the very readable book <a href="https://www.amazon.com/Theory-That-Would-Not-Die-ebook/dp/B0050QB3EQ"><em>The Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy</em></a> by Sharon Bertsch McGrayne, and in her Microsoft lecture</p>
<iframe width="720" height="405" src="https://www.youtube.com/embed/2o-_BGqYM5U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>and her Google lecture <iframe width="720" height="405" src="https://www.youtube.com/embed/8oD6eBkjF9o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
<p>Maybe, by calling it the more ‘orthodox’, all that Clayton and Hills mean is that the frequentist approach is more popular method today. It got a slow start, and dates from the early 20th century. (In one of our sampling exercises, we will try to determine the relative frequencies of the two approaches in the epidemiology and medical literature).</p>
<p>Interestingly, if you use <a href="https://books.google.com/ngrams/graph?content=frequentist+approach%2C+bayesian+approach&amp;case_insensitive=on&amp;year_start=1800&amp;year_end=2008&amp;corpus=15&amp;smoothing=3&amp;share=&amp;direct_url=t4%3B%2Cfrequentist%20approach%3B%2Cc0%3B%2Cs0%3B%3Bfrequentist%20approach%3B%2Cc0%3B%3BFrequentist%20approach%3B%2Cc0%3B%3BFrequentist%20Approach%3B%2Cc0%3B.t4%3B%2Cbayesian%20approach%3B%2Cc0%3B%2Cs0%3B%3BBayesian%20approach%3B%2Cc0%3B%3BBayesian%20Approach%3B%2Cc0%3B%3BBAYESIAN%20APPROACH%3B%2Cc0%3B%3Bbayesian%20approach%3B%2Cc0">Google Books Ngram Viewer</a> you get a different sense. Maybe this is because the majority don’t need to justify the methods they use!</p>
<p>Frequentist statements are <strong>indirect</strong>. They are (conditional) probabilistic statements about the <strong>data</strong> and about the performance of the <strong>procedure</strong> used to bracket the parameter values. A variant on it ranks the various possible parameter values according to how probable the observed data would be under each of these, but does not make direct probabilistic statements about the parameter values themselves. Because it is indirect, conditional, the results are often interpreted incorrectly.</p>
<p>We begin with the direct method, one that studies tell us we are born with, and use throughout our lives, both consciously and subconsciously, to continue to learn/update.</p>
<blockquote>
<p>When we learn a new motor skill, such as playing an approaching tennis ball, both our sensors and the task possess variability. […] We show that subjects internally represent both the statistical distribution of the task and their sensory uncertainty, combining them in a manner consistent with a performance-optimizing bayesian process. The central nervous system therefore employs probabilistic models during sensorimotor learning. <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/nature02169.pdf">Bayesian integration in sensorimotor learning</a></p>
</blockquote>
<p>leading to this New York Times headline</p>
<blockquote>
<p><a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/AthletesStatisticians.pdf">Subconsciously, Athletes May Play Like Statisticians</a></p>
</blockquote>
<div id="the-bayesian-approach" class="section level2">
<h2><span class="header-section-number">3.1</span> The Bayesian Approach</h2>
<p>to probability statements concerning parameter values.</p>
<p>This paragraph is taken from this chapter <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/an%20overview%20of%20the%20Bayesian%20approach.pdf">An Overview of the Bayesian Approach</a> in the book Bayesian Approaches to Clinical Trials and Health-Care Evaluation by David Speigelhalter et al, describes it well:</p>
<blockquote>
<p>The standard interpretation of probability describes long-run properties of repeated random events (Section 2.1.1). This is known as the frequency interpretation of probability, and standard statistical methods are sometimes referred to as ‘frequentist’. In contrast, the <strong>Bayesian approach</strong> rests on an essentially ‘subjective’ interpretation of probability, which is allowed to express generic uncertainty or ‘degree of belief’ about any unknown but potentially observable quantity, whether or not it is one of a number of repeatable experiments. For example, it is quite reasonable from a subjective perspective to think of a probability of the event ‘Earth will be openly visited by aliens in the next ten years’, whereas it may be difficult to interpret this potential event as part of a ‘long-run’ series. Methods of assessing subjective probabilities and probability distributions will be discussed in Section 5.2.</p>
</blockquote>
<p>Section 3.1 SUBJECTIVITY AND CONTEXT emphasizes that ‘the vital point of the subjective interpretation is that <strong>Your probability</strong> for an event is a property of <strong>Your</strong> relationship to that event, and not an objective property of the event itself.’ Moreover, ‘pedantically speaking, one should always refer to probabilities <strong>for</strong> events rather than probabilities <strong>of</strong> events, and the <strong>conditioning context</strong> used in Section 2.1.1 <strong>includes the observer and all their background knowledge and assumptions.</strong>’</p>
<p>That there is ‘always a context’ goes along with what we read in Alan Turing’s recently de-classified essay The Applications of Probability to Cryptography. Under section 1.2 (‘Meaning of probability and odds’) he starts out</p>
<blockquote>
<p>I shall not attempt to give a systematic account of the theory of probability, but it may be worth while to define shortly probability and odds. The probability of an event <strong>on certain evidence</strong> is the proportion of cases in which that event may be expected to happen given that evidence. For instance if it is known the 20% of men live to the age of 70, then knowing of Hitler only Hitler is a man we can say that the probability of Hitler living to the age of 70 is 0.2. Suppose that we know that Hitler is now of age 52 the probability will be quite different, say 0.5, because 50% of men of 52 live to 70.</p>
</blockquote>
<p><strong>Not all context is subjective</strong>. We will start with a context where the initial (starting out, pre-new-data) probability is <strong>objective</strong>.</p>
<p>Just before we do, we include this passage from Clayton and Hills, in subchapter 10.2 Subjective probablity, which they denote as optional material.</p>
<blockquote>
<p>The second approach to the problem of assigning a probability to a range of values for a parameter is based on the philosophical position that probability is a subjective measure of ignorance. The investigator uses probability as a measure of subjective degree of belief in the different values which the parameter might take. With this view it is perfectly logical to say that there is a probability of 0.9 that the parameter lies within a stated range.<br />
Before observing the data, the investigator will have certain beliefs about the parameter value and these can be measured by a priori probabilities. Because they are subjective every scientist would be permitted to give different probabilities to different parameter values. However, the idea of scientific objectivity is not completely rejected. In this approach objectivity lies in the rule used to modify the a priori probabilities in the light of the data from the study. This is Bayes’ rule and statisticians who take this philosophical position call themselves Bayesians.<br />
Bayes’ rule was described in Chapter 2, where it was used to calcu- late the probabilities of exposure given outcome from the probabilities of outcome given exposure. Once we are prepared to assign probabilities to parameter values, Bayes’ rule can be used to calculate the probability of each value of a parameter (<span class="math inline">\(\theta\)</span>) given the data, from the probability of the data given the value of the parameter.<br />
The argument is illustrated by two tree diagrams. Fig. 10.2 illustrates the direction in which probabilities are specified in the statistical model — given the choice of the value of the parameter, <span class="math inline">\(\theta\)</span>, the model tells us the probability of the data. The probability of any particular combination of data and parameter value is then the product of the probability of the parameter value and the probability of data given the parameter value. In this product, the first term, Pr(<span class="math inline">\(\theta\)</span>), represents the a priori degree of belief for the value of <span class="math inline">\(\theta\)</span> and the second term, Pr(Data | <span class="math inline">\(\theta\)</span> ), is the likelihood. Fig. 10.3 reverses the conditioning argument, and expresses the joint probabilityas the product of the overall probability of the data multiplied by the probability of the parameter given the data. This latter term, Pr( <span class="math inline">\(\theta\)</span> | Data), represents the posterior degree of belief in the parameter value once the data have been observed. Since the joint probability of data and parameter value is the same no matter which way we argue, so that <span class="math display">\[Pr(\theta) \times Pr(Data | \theta) = Pr(Data) \times Pr(\theta | Data),\]</span> so that <span class="math display">\[Pr(\theta | Data) = \frac{Pr(\theta) \times Pr(Data | \theta)}{Pr(Data)}\]</span> Thus elementary probability theory tells us how prior beliefs about the value of a parameter should be modified after the observation of data.</p>
</blockquote>
<p>Their 2 figures nicely show that the difference is in the directionality or conditioning, i.e. is the object <span class="math display">\[Pr(\theta | data) \ \ \ OR \ \ \   Pr(data | \theta) \ \ ?\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="images/ClaytonHillsFig102103.png" alt="From Chapter 10.2 of Clayton and Hills" width="604" />
<p class="caption">
Figure 3.1: From Chapter 10.2 of Clayton and Hills
</p>
</div>
<p><strong>Without getting into the details of the calculations, we will apply this approach to the first example in each of the parameter genres listed above. The point is to illustrate how direct and unambigous the answer is in each case.</strong></p>
<div id="example-parameter-is-2-valued-yes-or-no" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Example: parameter is 2-valued: yes or no</h3>
<p>In the first genre, the parameter is personal or particular. In each of the examples, the true state is binary. The potential hemophilia carrier <strong>is</strong> a hemophia carrier or <strong>is not</strong>; the particular email is either malicious or is not; the person in question either committed the crime ot did not. So, there are just two possible parameter values: yes or no, <strong>is</strong> or <strong>is not</strong>.</p>
<p>From the outset, just like in Turing’s example, there is a given context. For example, suppose a woman’s brother is known to have haemophilia.</p>
<blockquote>
<p>hemophilia: a medical condition in which the ability of the blood to clot is severely reduced, causing the sufferer to bleed severely from even a slight injury. The condition is typically caused by a hereditary lack of a coagulation factor, caused by a mutation in one of the genes located on the X chromosome. - Google</p>
</blockquote>
<p>Just knowing this, the probability that the woman is a hemophilia carrier is 50% or 1/2.</p>
<p>Today, genetic testing of the carrier can help determine whether the woman is a carrier. But when JH first taught 607, the only time to learn more about her carrier status (and move her probability to 1, or towards 0) was after the births of her sons: their status was knowable virtually immedediately.</p>
<p>If it is determined that the first son has hemophilia, it establishes that she IS a carrier, thereby moving the probability up to 1. If he was not, it moves the probability down to 1/3: in other words, among ‘women like her’, i.e, other potential carriers who also have had 1 son who turned out to be Normal (NL), 1/3 of the sons are the sons of carriers, and 2/3 are the sons of non-carriers.</p>
<p>The <strong>continued updating</strong> as the women with a NL son gave birth to a second son, and so on, is shown in the diagram below, with <strong>C</strong> used as shorthand for ‘<strong>Is</strong> a <strong>C</strong>arrier.’ Technically speaking, each sequential P[C] should indicate that it is ‘conditioned on’ – and thus reflects the information in – the history up to that point. In other words, the 1/5 probability refers to P[C | both sons are NL], where “|” stands for ‘given that’, or – to use Turing’s phrase – ‘on the evidence that’.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="statbook_files/figure-html/unnamed-chunk-7-1.png" alt="At the outset, each woman had a 50:50 chance of being a haemophilia carrier. Accumulating information from the hemophilia status of the sons increasingly 'sorts' or segregates the women by moving their probabilities of being a carrier TO 1 (100%) or FURTHER TOWARDS 0 (0%). It 'updates' the probablity of  being a carrier, P[C]. For brevity,  the ' | data' in each P[C | data] is omitted." width="864" />
<p class="caption">
Figure 3.2: At the outset, each woman had a 50:50 chance of being a haemophilia carrier. Accumulating information from the hemophilia status of the sons increasingly ‘sorts’ or segregates the women by moving their probabilities of being a carrier TO 1 (100%) or FURTHER TOWARDS 0 (0%). It ‘updates’ the probablity of being a carrier, P[C]. For brevity, the ‘| data’ in each P[C | data] is omitted.
</p>
</div>
<blockquote>
<p>ASIDE: This is similar to how researchers develop strains of “transgenic” mice, by introducing an altered gene (transgene) into the genome. In order to breed true, theanimals must be made to be homozygous, i.e., to have two copies of the introduced gene (+ +). Molecular biology techniques can detect whether the transgene is present in an individual animal (without having to sacrifice the animal), but cannot distinguish a hemizygote, with one copy of the gene (+ -), from a homozygote (+ +). This difference can only be detected by breeding strategies. First generations: A copy of the transgene is injected into the pronucleus of a newly fertilized ovum, prior to fusion with the male pronucleus. Thus all animals that develop from these zygotes can have at most one copy of the gene, from the ovum. After birth, screening is performed to detect these ‘positive’ animals, called founders. After sexual maturation, all founders are bred to normal ‘wild type’ (WT) animals, to ensure that the transgene has been incorporated in such a way as to be heritable. Pairs of positive (hemizygous) animals in this F1 generation are then bred to each other. By Mendelian genetics, the distri- bution of F2 offspring should be 1:2:1, homozygous transgenic : hemizygous transgenic : homozygous normal. The homozygous normal animals are not used. The question is, how to tell the homozygous transgenic mice (the desired ones) from the hemizygous transgenic ones? Note that the mix in this reduced population is 1 homozygous transgenic to 2 hemizygous transgenic. F2 breeding: All ’positive’ F2 animals (i.e. all homozygous and hemizygous animals) are bred to wild type. Possible F3 genotypes are as follows: (by Mendelian genetics) Hemizygous (which comprise 2/3 of the F2 animals used) x wild type = 50:50, hemizygous (and therefore ‘positive’) : normal (and therefore ‘negative’), Homozygous (which comprise 1/3 of the F2 animals used) x wild type = all hemizygous (and therefore ‘positive’). That is, while only half of the offspring from a Hemi x WT pair will be ‘positive’ when screened, all of the offspring of a Homo x WT pair will be ‘positive’. The question: How many F3 offspring from a particular pairing does the researcher have to screen before declaring the positive parent as homozygous? Note: as soon as an offspring is screened as ‘negative,’ one knows the parent must have been hemizygous. A variant on the above diagram can help withe probabilities. Furthe details are avilable on the bios601 website, in the ‘probability’ chapter.</p>
</blockquote>
<p>Before moving on the the next type of parameter, a few points</p>
<ul>
<li><p>In both the hemophilia and transgenic mice examples, the ‘starting’ probability is objective and the post-data probabilities have a ‘long-run’ or ‘in large numbers of similar instances’ interpretation. One could make a diagram that shows the expected numbers ‘in every 100 women like this.’</p></li>
<li><p>There is nothing special about the ‘starting out’ probability P[<strong>C</strong>] of 0.5. Before a pregnancy test, or a pre-natal diagnostic test, for example, the probability of the target <strong>C</strong>ondition/state of interest/concern would be a function of many other factors, and could in theory take on any value between 0 and 1. The (starting out, pre-filter) proportion of malicious emails would depend on which of a person’s email accounts it was.</p></li>
<li><p>In the language of diagnostic tests, each ‘Son as a test of the mother’s carrier status’ has 50% sensitivity and 100% ‘specificity’. For sensitivity, this puts it on par with the Pap test for cervical cancer: the main problem withe latter is in the sampling. For specificity, it is better than most tests.</p></li>
<li><p>The shiny app <a href="https://jameshanley.shinyapps.io/FromPreTestToPostTestProbabilities/">From Pre-test to Post-test Probabilities</a> shows how the initial average (pre-test) probability is segregated into 2 post-test probabilities by the 2 possible test results, and the role of the 2 error-probabilities (just about all tests are fallible) in how well they push them out.</p></li>
<li><p>The ‘starting out’ probability could be subjective. For example it could be one’s impression (before getting to see up close how wrinkled their face is) as whether the person is a smoker, or one’s assessment of the probability that the accused is guilty (before getting to hear the DNA expert, or lie detection report) based on how credible the accused appears to be, and all of the other evidence to date.</p></li>
<li><p>The main point is that we are merging (adding) two sources of information.</p></li>
</ul>
</div>
<div id="example-parameter-is-a-proportion" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Example: parameter is a proportion</h3>
<p>In theory, in this genre, the true parameter value could in theory lie anywhere between 0 and 1, But again, just like in Turing’s example, we seldom start from complete ignorance, or with – in the title of pscychologist Stephen Pinker’s book – a blank slate. Even if you have never seen thumbtacks tossed onto on a surface, you can reason informally, and indicate what proportions are unlikely and likely, and where along the (0,1) scale you would ‘put your money’.</p>
<p>You could do the same when asked what proportions of your time that you are being productive, or on the phone, or sedentary, or indoors. Mind you, you might be ‘way off’ with your claims, but the nice thing is that — and this is the point of this course – you can generate data to narrow down the true proportion.</p>
<p>The other nice thing with the Bayesian approach in particular is that – no matter whether you believe the proportion is low or medium or high, we can work out what your post-data beliefs should be. It is a matter of mathematics. If, before collecting any new data, we have ‘no idea’ – a common phrase among todays’s generation, one that, if it is uttered with empahsis on the ‘no’, irks JH to no end – what the true parameter value is, that is easily handled. Moreover, enough valid data will (or should!) trump the pre-data beliefs.</p>
<blockquote>
<p>On a side note: Dick Pound, a former chancellor of McGill University, and first president of the World Anti-Doping Agency is a staunch advocate of strict drug testing for athletes. Discussing the National Hockey League in November 2005, Pound said, ‘you wouldn’t be far wrong if you said a third of hockey players are gaining some pharmaceutical assistance.’ Pound would later admit that he completely invented the figure. Both the NHL and NHLPA have denied the claims, demanding Pound provide evidence rather than make what they term unsubstantiated claims. Since his comments were made, some NHL players have tested positive for banned substances, including Bryan Berard, José Théodore, and two of 250 players involved in Olympic testing. As of June 2006, there had been 1,406 tests in the program jointly administered by the league and the union, and none has come up with banned substances under NHL rules. Pound remained skeptical, claiming the NHL rules were too lax and unclear, as they do not test for some banned substance, including certain stimulants. In an interview with hockey blogger, B. D. Gallof, of Hockeybuzz on December 19, 2007, Pound was asked to expand on the 30% comment and subsequent reaction, expounded that stimulants was ‘the NHL’s drug of choice’. He also cited that the NHL will have no credibility on a drug policy if it, and other sports, continue to run things ‘in-house’. <a href="https://en.wikipedia.org/wiki/Dick_Pound" class="uri">https://en.wikipedia.org/wiki/Dick_Pound</a> and <a href="https://www.cbc.ca/sports/hockey/dick-pound-slams-nhl-s-drug-policy-1.557993" class="uri">https://www.cbc.ca/sports/hockey/dick-pound-slams-nhl-s-drug-policy-1.557993</a></p>
</blockquote>
<p>Even before studying/asking them, investigators would have some sense of the proportions of patients whose disease would respond to a medication, or people who would volunteer for a survey or research study. These iimpressions would probably be based on previous analogous situations, and the ‘literature’, but would vary from pundit to pundit. But ultimately, they could be much improved and narrowed (and even replaced entirely) by new-data-based ones.</p>
<p>The proportion of the earth’s surface that is covered by water is easy to determine: just look up a reputable source. But what if you weren’t able to, but did have access to the database of 933 million recordings in the <a href="https://topex.ucsd.edu/cgi-bin/get_srtm30.cg">SRTM30PLUS database</a>. It has altitude/depth measurements for 43,200 x 21,600 = 933,120,000 locations. This database is so large that you would have to sample from it. From a thousand randomly chosen loactions, you would be able to ‘trust’ the first decimal in your estimate; from a million you should be able to trust the second – and maybe the third.</p>
<p>Since we already know/remember from high school ‘roughly’ what the proportion is, we will leave it for an exercsie in another chapter. In this chapter, following the advice of master-teacher Fred Mosteller, we use examples where the <strong>correct answer is not known with any precision</strong>. The proportion of these we probably know the least about is the thumbtack one. However, it has fewer personal benefits than knowing what proportion of your time you are being productive. Moreover, we have a nice written account of how you might go about learning this personal proportion.</p>
<p>In his book Elementary Bayesian Statistics, Gordon Antelman informally introduces and illustrate a Bayesian analysis of an uncertain proportion with a slightly modified version of a novel and useful application of work sampling discussed by Fuller (1985). We have changed his notation for the proportion of your time spent in productive work, and called it <span class="math inline">\(\pi\)</span>, and also modified some of his words.</p>
<blockquote>
<p>Suppose you, as a good up-to-date manager practicing continuous quality and productivity improvement, have some ideas on improving your own productivity. To see if these ideas have any merit, you would like to compare some ‘before’ measure of productivity with a comparable ‘after’ measure of productivity.<br />
For now — we shall come back to this example several times — let us focus on just a ‘before’ measure. The measure to be used is the proportion of your time spent in productive work, call it <span class="math inline">\(\pi\)</span>, as opposed to time spent doing something that would not have needed doing if things had been done right the first time. Examples of the latter might include searching for a misplaced document, recreating a deleted computer file, following up on a customer’s complaint, or waiting past a scheduled time for a meeting to start. [Today, we would add being on social media, or browsing the web for non-work-related matters] Rather than saying <span class="math inline">\(\pi\)</span> is not (precisely) ‘known’, it is better to say that ‘<span class="math inline">\(\pi\)</span> is uncertain’; from your job experience, you would really know quite a lot about p. For example, you might be almost certain that it is greater than 0.50, less than 0.90, and you might assess your odds that <span class="math inline">\(\pi\)</span> is between, say, 0.60 and 0.80 to be about 9 to 1. A precise statement of these beliefs will be your prior distribution for <span class="math inline">\(\pi\)</span>.<br />
You would probably feel uncomfortable — most people do — about assessing this prior distribution, especially since there are an infinite number of states; viz., all of the values between zero and one. But, without any real loss, you can bypass the infinite-number problem by rounding the values of <span class="math inline">\(\pi\)</span> to the nearest 5% or 10%, making the problem discrete. Then you have a contemplatable Bayes’ theorem, like those discussed in Chapter 4, with the finitely many <span class="math inline">\(\pi\)</span>-values as the possible “states”. (When we reconsider this example later in this chapter, you will see that, with a little theory, the infinite number of <span class="math inline">\(\pi\)</span>-values can almost always be handled very neatly and more easily.)</p>
</blockquote>
<p><strong>PRIOR BELIEFS</strong> For illustration, he supposes you choose just five possible values for <span class="math inline">\(\pi\)</span>, and assess your prior distribution. Since <code>R</code> does not allow Greek symbols, we will refer to it by uppercase P and assess your prior distribution. This possible prior distribution, shown below, would reflect, for example, that your judgment is that there is only about one chance in 20 that <span class="math inline">\(\pi\)</span> rounds to 0.50, about one chance in 20 that <span class="math inline">\(\pi\)</span> rounds to 0.90, about one chance in four that it rounds to 0.60, a little more than one chance in three that it rounds to 0.70, and a little less than one chance in three that it rounds to 0.80.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">P =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.60</span>, <span class="fl">0.70</span>, <span class="fl">0.80</span>, <span class="fl">0.90</span>)
PriorProbForP =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.35</span>, <span class="fl">0.30</span>, <span class="fl">0.05</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">plot</span>(P, PriorProbForP, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),
     <span class="dt">lwd=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>),  <span class="dt">lend=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;grey80&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;P: Proportion of time that I am being productive&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;My probability that P rounds to... &quot;</span>,
     <span class="dt">cex.lab =</span> <span class="fl">1.25</span>, <span class="dt">cex.axis =</span> <span class="fl">1.25</span>)
<span class="kw">text</span>(<span class="fl">0.5</span>,<span class="fl">0.25</span>, <span class="st">&quot;Prior Probabilities&quot;</span>, <span class="dt">font=</span><span class="dv">2</span>,
     <span class="dt">col=</span><span class="st">&quot;grey75&quot;</span>, <span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">cex=</span><span class="fl">1.75</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="statbook_files/figure-html/unnamed-chunk-8-1.png" alt="Prior Probabilities for the parameter P, the proportion of time that I am being productive." width="864" />
<p class="caption">
Figure 3.3: Prior Probabilities for the parameter P, the proportion of time that I am being productive.
</p>
</div>
<p><strong>DATA</strong>: Suppose you are fitted with a beeper set to beep at random times; when the beeper beeps, you classify the task being worked on as <strong>W</strong> — for ‘productive <strong>Work</strong>’, or <strong>F</strong> — for ‘<strong>Fixing</strong>’ (or today we mght say ‘<strong>F</strong>iddling’ or ‘<strong>F</strong>ooling around’ or wasting time).</p>
<p>Although we will skip the technicalities, it is important that the experiment be designed so that the trials are independent. Beeps should be unpredictable so you do not arrange, possibly subconsciously, to be doing productive work at the beep. They probably also should not be too close together to make the independence assumption more reasonable.</p>
<p><strong>Suppose the first four trials give the data <span class="math inline">\(F_1, F_2, W_3\)</span>, and <span class="math inline">\(F_4\)</span>.</strong></p>
<p>Below is a picture showing the effects of the data FFWF on the prior distribution. Three F’s in four trials increase your probabilities for the two smaller values of P and decrease your probabilities for the three larger ones.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Lik =<span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>P)<span class="op">^</span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>P
PosteriorProbForP =<span class="st"> </span>(PriorProbForP <span class="op">*</span><span class="st"> </span>Lik) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(PriorProbForP <span class="op">*</span><span class="st"> </span>Lik)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">plot</span>(P, PriorProbForP, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),
     <span class="dt">lwd=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>),  <span class="dt">lend=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;grey80&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;P: Proportion of time that I am being productive&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;My probability that P rounds to... &quot;</span>,
     <span class="dt">cex.lab =</span> <span class="fl">1.25</span>, <span class="dt">cex.axis =</span> <span class="fl">1.25</span>)
<span class="kw">text</span>(<span class="fl">0.5</span>,<span class="fl">0.25</span>, <span class="st">&quot;Prior Probabilities&quot;</span>, <span class="dt">font=</span><span class="dv">2</span>,
     <span class="dt">col=</span><span class="st">&quot;grey75&quot;</span>, <span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">cex=</span><span class="fl">1.75</span>)
<span class="kw">lines</span>(P, PosteriorProbForP, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">lwd=</span><span class="dv">6</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lend=</span><span class="dv">1</span>)
<span class="kw">text</span>(<span class="fl">0.5</span>,<span class="fl">0.4</span>, <span class="st">&quot;Posterior Probabilities</span><span class="ch">\n</span><span class="st">after observing 1W, 3F&quot;</span>, <span class="dt">font=</span><span class="dv">2</span>,
     <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">cex=</span><span class="fl">1.75</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="statbook_files/figure-html/unnamed-chunk-9-1.png" alt="Prior Probabilities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior probabilities, after observing that in n = 4 randomly sampled occasions, I was actually productive in only 1 of the 4." width="864" />
<p class="caption">
Figure 3.4: Prior Probabilities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior probabilities, after observing that in n = 4 randomly sampled occasions, I was actually productive in only 1 of the 4.
</p>
</div>
<p>The sample alone most strongly supports a value for P of 0.25 (one W in four trials); had the prior included a value of P of 0.25, the (relative) increase in going from prior to posterior would have been greatest for that value.</p>
<p>For the assumed prior, in which only p’s of 0.50, 0.60, 0.70, 0.80, or 0.90 are considered, the sample evidence FFWF in favor of a P near 0.25 can only push up the posterior probabilities for the nearest possible values - 0.50 and 0.60. (The seemingly harder consideration of <strong>all possible p’s between zero and one</strong> will handle this kind of situation more logically.)</p>
<p>Below we show the ‘<strong>continuous P </strong>’ version Antelman refers to. To make this, we calculated the mean and variance of his discrete (5-point) prior distribution, and converted them to the 2 parameters, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, of the beta distribution with the same mean and variance.</p>
<p>Conveniently, the posterior density is also a beta distribution, but with parameters <span class="math inline">\(a+1\)</span> and <span class="math inline">\(b+3\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean.P.prior =<span class="st"> </span><span class="kw">sum</span>( P<span class="op">*</span><span class="st"> </span>PriorProbForP )
var.P.prior  =<span class="st"> </span><span class="kw">sum</span>( (P<span class="op">-</span>mean.P.prior)<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>PriorProbForP )

a.plus.b =<span class="st"> </span>mean.P.prior <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>mean.P.prior) <span class="op">/</span><span class="st"> </span>var.P.prior  <span class="op">-</span><span class="st"> </span><span class="dv">1</span>
a =<span class="st"> </span>mean.P.prior <span class="op">*</span><span class="st"> </span>a.plus.b
b =<span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>mean.P.prior) <span class="op">*</span><span class="st"> </span>a.plus.b

P =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)
prior.density =<span class="st"> </span><span class="kw">dbeta</span>(P,a,b)
posterior.density =<span class="st"> </span><span class="kw">dbeta</span>(P,a<span class="op">+</span><span class="dv">1</span>,b<span class="op">+</span><span class="dv">3</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-11"></span>
<img src="statbook_files/figure-html/unnamed-chunk-11-1.png" alt="Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 4 randomly sampled occasions, I was actually productive  (W) in only 1 of the 4." width="864" />
<p class="caption">
Figure 3.5: Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 4 randomly sampled occasions, I was actually productive (W) in only 1 of the 4.
</p>
</div>
<p>Before moving on the the next type of parameter, a few points:</p>
<ul>
<li><p>The beta distribution nicely shows <strong>how the prior information/impression and the new data get combined</strong>. The <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> parameters of the prior distribution are 14.8 and 6.2. Together, they determine the mean, <span class="math inline">\(a/(a=b)\)</span>, the median, the mode, <span class="math inline">\((a-1)/(a+b-2)\)</span>, and the variance, <span class="math inline">\(a^2b^2/(a+b+1)^2\)</span> of the prior distribution. Their conterparts in the data-likelihood are 1 and 3. The ‘<span class="math inline">\(a\)</span>’ and ‘<span class="math inline">\(b\)</span>’ parameters of the posterior distribution are 15.8 and 9.2: the <span class="math inline">\(a\)</span>’s <strong>add</strong>, and the <span class="math inline">\(b\)</span>’s <strong>add</strong>. In other words, the distribution of one’s pre-data beliefs is the distribution one would have after ‘seeing’ 14.8 W’s and 6.2 F’s; the distribution of one’s post-data beliefs is the distribution one would have after ‘seeing’ 15.8 W’s and 9.2 F’s. <strong>The (synthetic) experience-equivalent of the numbers of Ws and F’s in the prior are added to the actual (observed) numbers of Ws and F’s in the data to arrive at the (new) posterior distribution.</strong></p></li>
<li><p>You are probably wondering what the posterior distribution would look like <strong>with more data</strong>. Here is what it would look like after observing 7 out of 25 or 13 out of 25. The modes of the posterior distributions are still somewhat influenced by the prior – as they are still well above P=7/25 = 0.28 and P=13/25 = 0.52. If the data were 70/250 or 13/250, the modes would be closer to P= 0.28 or P = 0.52; in other words, the data would ‘swamp’ – or ‘trump’ – the prior.</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="statbook_files/figure-html/unnamed-chunk-12-1.png" alt="Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 25 randomly sampled occasions, I was actually productive (W) in only 7 of the 25, or 13 of the 25." width="864" />
<p class="caption">
Figure 3.6: Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 25 randomly sampled occasions, I was actually productive (W) in only 7 of the 25, or 13 of the 25.
</p>
</div>
<ul>
<li><p>Be thinking about your prior for the proportion of thumbtacks that land on their back, and the proportion of the Earth’s surface that is covered by water, or [these words written on March 30, 2020, before any trial data] the proportion of patients with mild symptoms of covid-19 who would benefit from chloroquine.</p></li>
<li><p>Think about how you might elicit a prior distribution. You might want to Google ‘tools for eliciting prior distributions’ – or consult Chapter 5 of Spiegelhalter’s book.</p></li>
</ul>
<p>We now move on to last parameter genre we will consider.</p>
</div>
<div id="example-parameter-is-a-mean" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Example: parameter is a mean</h3>
<p>We will start with a discrete version of a commonly-encountered parameter, the age of a person whom you have just met, or seen a photo of. We will then go on to a full numerical example, your average cholesterol or blood pressure level.</p>
<p><strong>Example 1</strong></p>
<p><strong>How old</strong> (or <strong>what age</strong> – if you prefer to avoid speaking of ‘old’) do you think this IBC2006 attendee was when this photo was taken? This is the <em>parameter of interest</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="images/IMG_DS.png" alt="An attendee at the International Biometrics Conference, held at McGill in July 2006" width="913" />
<p class="caption">
Figure 3.7: An attendee at the International Biometrics Conference, held at McGill in July 2006
</p>
</div>
<p>He tells you he <strong>got his PhD 32 years earlier</strong>. Based on the distribution of ages at which people get their PhD (shown in grey below), that puts his current age somewhere in the blue distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="statbook_files/figure-html/unnamed-chunk-14-1.png" alt="Current ages of persons who obtained PhD 32 years earlier." width="864" />
<p class="caption">
Figure 3.8: Current ages of persons who obtained PhD 32 years earlier.
</p>
</div>
<p>This is a somewhat unusual example, as the blue distribution is very wide – partly because we could not find age-at-graduation data specific to PhD graduates in statistics in 1974. We suspect that that specific distribution is a good deal narrower than the one shown. In the next chapter, you will see that other indirect measures of age are a good bit tighter than this.</p>
<p>Nevertheless, it emphasizes that, depending on what impression you got form the photo alone, you may now wish to revise your estimate of the person’s age. We suspect that many of you would have initially though he <strong>looks like he was in mid 50s</strong>, and so would have made guesses like those shown in red in the top panel. If you are one of those, then you will want to <strong>revise the age upwards</strong>, as we do in the green in the bottom panel.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-16"></span>
<img src="statbook_files/figure-html/unnamed-chunk-16-1.png" alt="If he looked like he was in mid 50s" width="864" />
<p class="caption">
Figure 3.9: If he looked like he was in mid 50s
</p>
</div>
<p>If you initially thought he looked like he was ‘around 60’ you would have made guesses like those shown in red in the top panel. If you did, then you will want to <strong>revise upwards a little</strong>, as is shown in green, and now put hime him somewhere around 60, or a bit above.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-17"></span>
<img src="statbook_files/figure-html/unnamed-chunk-17-1.png" alt="If If he looked like he was around 60." width="864" />
<p class="caption">
Figure 3.10: If If he looked like he was around 60.
</p>
</div>
<p>If you initially thought he looked like he was ‘in his mid 60s’ your estimate is more in line with the age-at-PhD data, and so you would not revise as much. You might bet a bit more on the ‘around 60’ age-bracket.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-18"></span>
<img src="statbook_files/figure-html/unnamed-chunk-18-1.png" alt="If If he looked like he was in his mid 60s." width="864" />
<p class="caption">
Figure 3.11: If If he looked like he was in his mid 60s.
</p>
</div>
<p>As we noted, the PhD data have too much of a right tail, and so it is driving up the estimates. If you are now curious as how keen your ‘age-estimation’ skills are, here is a link to the Google Scholar page of the <a href="https://scholar.google.com/citations?user=e8DARmEAAAAJ&amp;hl=en">statistician whose age we have been trying to determine</a>.</p>
<p>Age estimation via face images (image-based age estimation) is a growing research area, with many possible applications.</p>
<p>We now describe 2 more classical examples</p>
<p><strong>Example 2</strong></p>
<p>Spiegelhalter et al. address this in their Example 3.4: ‘Suppose we are interested in the <strong>long-term systolic blood pressure (SBP) in mmHg of a particular 60-year-old female</strong>.’</p>
<blockquote>
<p>We take two independent readings 6 weeks apart, and their mean is 130. We know that SBP is measured with a standard deviation <span class="math inline">\(\sigma = 5.\)</span> What should we estimate her SBP to be?</p>
</blockquote>
<p>They then go on to give the frequentist (‘standard’) 95% confidence interval, of 123.1 to 136.9, centered on the measured value of 130 [we will come later to how they calculated this]. They continue, …</p>
<blockquote>
<p><strong>However</strong>, we may have considerable additional information about SBPs which we can express as a prior distribution. Suppose that a survey in the same population revealed that females aged 60 had a mean long-term SBP of 120 with standard deviation 10. This population distribution can be considered as a prior distribution for the specific individual, and is shown in Figure 3.3(a):</p>
</blockquote>
<p>The posterior distribution, computed from the combination of the 130 measured on the woman, and the prior, is centered on 128.9 and the 95% interval is 122.4 to 135.4.</p>

<p>It’s a pity that these authors did not give an actual source for this prior, or be a bit more realistic about ``a same population of females aged 60 with a mean long-term SBP of 120’’. This must be a somewhat selected, healthier-than-average population, since we might find such a mean of 120 in 25-year old women; in the general population of 60-year old women, it is higher than that.</p>
<p><strong>Example 3</strong></p>
<p>a person’s cholesterol level – Irwig article</p>
</div>
</div>
<div id="frequentist-approach" class="section level2">
<h2><span class="header-section-number">3.2</span> Frequentist approach</h2>
<p>2 separate issues Ho and intervals (treated together in Bayesian)</p>
<p>• Statistical Quality Control procedures [for Decisions] • Sample survey organizations: Confidence intervals • Statistical Tests of Hypotheses Unlike Bayesian inference, there is no quantified pre-test or pre- data “impression”; the ultimate statements are about data, conditional on an assumed null or other hypothesis. Thus, an explanation of a p-value must start with the conditional “IF the parameter is … the probability that the data woul</p>
</div>
<div id="does-it-matter" class="section level2">
<h2><span class="header-section-number">3.3</span> Does it matter?</h2>
<p>semantics</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="paras.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="CI.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02a-inference.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["statbook.pdf", "statbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
