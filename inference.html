<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Statistical Inference | Introduction to Statistical Analysis: a regression-from-the-outset approach</title>
  <meta name="description" content="A regression-from-the-outset based approach" />
  <meta name="generator" content="bookdown 0.18.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Statistical Inference | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A regression-from-the-outset based approach" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Statistical Inference | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  
  <meta name="twitter:description" content="A regression-from-the-outset based approach" />
  

<meta name="author" content="Sahir, Shirin and Jim" />


<meta name="date" content="2020-04-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="paras.html"/>
<link rel="next" href="CI.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">In Planning Stage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#target"><i class="fa fa-check"></i><b>0.1</b> Target</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#topicstextbooks"><i class="fa fa-check"></i><b>0.2</b> Topics/textbooks</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#regression-from-the-outset"><i class="fa fa-check"></i><b>0.3</b> Regression from the outset</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#parameters-first-data-later"><i class="fa fa-check"></i><b>0.4</b> Parameters first, data later</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#lets-switch-to-y-bar-and-drop-x-bar."><i class="fa fa-check"></i><b>0.5</b> Let’s switch to “y-bar”, and drop “x-bar”.</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#computing-from-the-outset"><i class="fa fa-check"></i><b>0.6</b> Computing from the outset</a></li>
<li class="chapter" data-level="0.7" data-path="index.html"><a href="index.html#appendix"><i class="fa fa-check"></i><b>0.7</b> Appendix:</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#goals"><i class="fa fa-check"></i><b>1.1</b> Goals</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#structure"><i class="fa fa-check"></i><b>1.2</b> Structure</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#attitudes-etc."><i class="fa fa-check"></i><b>1.3</b> Attitudes, etc….</a></li>
</ul></li>
<li class="part"><span><b>I Part I</b></span></li>
<li class="chapter" data-level="2" data-path="paras.html"><a href="paras.html"><i class="fa fa-check"></i><b>2</b> Statistical Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="paras.html"><a href="paras.html#parameters"><i class="fa fa-check"></i><b>2.1</b> Parameters</a></li>
<li class="chapter" data-level="2.2" data-path="paras.html"><a href="paras.html#parameter-contrasts"><i class="fa fa-check"></i><b>2.2</b> Parameter Contrasts</a><ul>
<li class="chapter" data-level="2.2.1" data-path="paras.html"><a href="paras.html#parameter-relations-in-numbers-and-words"><i class="fa fa-check"></i><b>2.2.1</b> Parameter relations in numbers and words</a></li>
<li class="chapter" data-level="2.2.2" data-path="paras.html"><a href="paras.html#parameter-relations-in-symbols-and-with-the-help-of-an-index-category-indicator"><i class="fa fa-check"></i><b>2.2.2</b> Parameter relations in symbols, and with the help of an index-category indicator</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="paras.html"><a href="paras.html#parameter-functions"><i class="fa fa-check"></i><b>2.3</b> Parameter functions</a></li>
<li class="chapter" data-level="2.4" data-path="paras.html"><a href="paras.html#phraseology-to-avoid"><i class="fa fa-check"></i><b>2.4</b> Phraseology to avoid</a></li>
<li class="chapter" data-level="2.5" data-path="paras.html"><a href="paras.html#summary"><i class="fa fa-check"></i><b>2.5</b> SUMMARY</a></li>
<li class="chapter" data-level="2.6" data-path="paras.html"><a href="paras.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="paras.html"><a href="paras.html#references"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html#the-bayesian-approach"><i class="fa fa-check"></i><b>3.1</b> The Bayesian Approach</a><ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#example-parameter-is-2-valued-yes-or-no"><i class="fa fa-check"></i><b>3.1.1</b> Example: parameter is 2-valued: yes or no</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference.html"><a href="inference.html#example-parameter-is-a-proportion"><i class="fa fa-check"></i><b>3.1.2</b> Example: parameter is a proportion</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference.html"><a href="inference.html#examples-parameter-is-a-personal-number-or-population-mean"><i class="fa fa-check"></i><b>3.1.3</b> Examples: parameter is a personal number or population mean</a></li>
<li class="chapter" data-level="3.1.4" data-path="inference.html"><a href="inference.html#the-bayesian-bottom-line"><i class="fa fa-check"></i><b>3.1.4</b> The Bayesian Bottom Line</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inference.html"><a href="inference.html#frequentist-approach"><i class="fa fa-check"></i><b>3.2</b> Frequentist approach</a><ul>
<li class="chapter" data-level="3.2.1" data-path="inference.html"><a href="inference.html#frequentist-test-of-a-null-hypothesis"><i class="fa fa-check"></i><b>3.2.1</b> (Frequentist) Test of a Null Hypothesis</a></li>
<li class="chapter" data-level="3.2.2" data-path="inference.html"><a href="inference.html#the-ingredients-and-the-methods-of-procedure-in-a-statistical-test"><i class="fa fa-check"></i><b>3.2.2</b> The ingredients and the methods of procedure in a statistical test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="inference.html"><a href="inference.html#does-the-approach-matter"><i class="fa fa-check"></i><b>3.3</b> Does the approach matter?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Parameter Intervals</a><ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> ‘100% confidence’ intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#more-nuanced-intervals"><i class="fa fa-check"></i><b>4.2</b> More-nuanced intervals</a></li>
<li class="chapter" data-level="4.3" data-path="CI.html"><a href="CI.html#summary-1"><i class="fa fa-check"></i><b>4.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="paraMu.html"><a href="paraMu.html"><i class="fa fa-check"></i><b>5</b> The ‘mean’ parameter <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="paraMu.html"><a href="paraMu.html#two-genres"><i class="fa fa-check"></i><b>5.1</b> Two genres</a></li>
<li class="chapter" data-level="5.2" data-path="paraMu.html"><a href="paraMu.html#fitting-these-to-data-estimating-them-from-data"><i class="fa fa-check"></i><b>5.2</b> Fitting these to data / Estimating them from data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="paraPi.html"><a href="paraPi.html"><i class="fa fa-check"></i><b>6</b> The (proportion) parameter</a><ul>
<li class="chapter" data-level="6.1" data-path="paraPi.html"><a href="paraPi.html#example-one"><i class="fa fa-check"></i><b>6.1</b> Example one</a></li>
<li class="chapter" data-level="6.2" data-path="paraPi.html"><a href="paraPi.html#example-two"><i class="fa fa-check"></i><b>6.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="paraLambda.html"><a href="paraLambda.html"><i class="fa fa-check"></i><b>7</b> The (event rate) parameter</a><ul>
<li class="chapter" data-level="7.1" data-path="paraLambda.html"><a href="paraLambda.html#etc"><i class="fa fa-check"></i><b>7.1</b> Etc</a></li>
<li class="chapter" data-level="7.2" data-path="paraLambda.html"><a href="paraLambda.html#etc-1"><i class="fa fa-check"></i><b>7.2</b> ETC</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html"><i class="fa fa-check"></i><b>8</b> Contrast: 2 mean parameters</a><ul>
<li class="chapter" data-level="8.1" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html#estimand-estimator-estimate"><i class="fa fa-check"></i><b>8.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html"><i class="fa fa-check"></i><b>9</b> Contrast: 2 proportion parameters</a><ul>
<li class="chapter" data-level="9.1" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html#estimand-estimator-estimate-1"><i class="fa fa-check"></i><b>9.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html"><i class="fa fa-check"></i><b>10</b> Contrast: 2 speed parameters</a><ul>
<li class="chapter" data-level="10.1" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html#estimand-estimator-estimate-2"><i class="fa fa-check"></i><b>10.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="part"><span><b>II Part II</b></span></li>
<li class="chapter" data-level="11" data-path="Probability.html"><a href="Probability.html"><i class="fa fa-check"></i><b>11</b> Probability</a><ul>
<li class="chapter" data-level="11.1" data-path="Probability.html"><a href="Probability.html#conditional-forwards"><i class="fa fa-check"></i><b>11.1</b> Conditional – forwards</a></li>
<li class="chapter" data-level="11.2" data-path="Probability.html"><a href="Probability.html#conditional-reverse"><i class="fa fa-check"></i><b>11.2</b> Conditional – reverse</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Distributions.html"><a href="Distributions.html"><i class="fa fa-check"></i><b>12</b> Distributions /Random Variables</a><ul>
<li class="chapter" data-level="12.1" data-path="Distributions.html"><a href="Distributions.html#gaussian-bernoulli-binomial-poisson"><i class="fa fa-check"></i><b>12.1</b> Gaussian Bernoulli-Binomial Poisson</a></li>
<li class="chapter" data-level="12.2" data-path="Distributions.html"><a href="Distributions.html#expectation-and-variance"><i class="fa fa-check"></i><b>12.2</b> Expectation and Variance</a></li>
<li class="chapter" data-level="12.3" data-path="Distributions.html"><a href="Distributions.html#functionscombinations-of-random-variables"><i class="fa fa-check"></i><b>12.3</b> Functions/combinations of random variables</a></li>
</ul></li>
<li class="part"><span><b>III Part III</b></span></li>
<li class="chapter" data-level="13" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>13</b> Mathematics</a><ul>
<li class="chapter" data-level="13.1" data-path="math.html"><a href="math.html#notation"><i class="fa fa-check"></i><b>13.1</b> Notation</a></li>
<li class="chapter" data-level="13.2" data-path="math.html"><a href="math.html#powers-logarithms-and-antilogarithms"><i class="fa fa-check"></i><b>13.2</b> Powers, Logarithms and Anti–logarithms</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="computing01.html"><a href="computing01.html"><i class="fa fa-check"></i><b>14</b> Computing Session 1</a><ul>
<li class="chapter" data-level="14.1" data-path="computing01.html"><a href="computing01.html#biological-background"><i class="fa fa-check"></i><b>14.1</b> Biological background</a></li>
<li class="chapter" data-level="14.2" data-path="computing01.html"><a href="computing01.html#statistical-task"><i class="fa fa-check"></i><b>14.2</b> Statistical Task</a><ul>
<li class="chapter" data-level="14.2.1" data-path="computing01.html"><a href="computing01.html#the-p-and-q-functions-an-orientation"><i class="fa fa-check"></i><b>14.2.1</b> The p and q functions: an orientation</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="computing01.html"><a href="computing01.html#shapes-of-distributions"><i class="fa fa-check"></i><b>14.3</b> Shapes of Distributions</a></li>
<li class="chapter" data-level="14.4" data-path="computing01.html"><a href="computing01.html#exercises-1"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
<li class="chapter" data-level="14.5" data-path="computing01.html"><a href="computing01.html#summary-2"><i class="fa fa-check"></i><b>14.5</b> SUMMARY</a><ul>
<li class="chapter" data-level="14.5.1" data-path="computing01.html"><a href="computing01.html#computing"><i class="fa fa-check"></i><b>14.5.1</b> Computing</a></li>
<li class="chapter" data-level="14.5.2" data-path="computing01.html"><a href="computing01.html#statistical-concepts-and-principles"><i class="fa fa-check"></i><b>14.5.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="computing02.html"><a href="computing02.html"><i class="fa fa-check"></i><b>15</b> Computing: Session No. 2</a><ul>
<li class="chapter" data-level="15.1" data-path="computing02.html"><a href="computing02.html#scientific-background"><i class="fa fa-check"></i><b>15.1</b> Scientific background</a></li>
<li class="chapter" data-level="15.2" data-path="computing02.html"><a href="computing02.html#random-variation"><i class="fa fa-check"></i><b>15.2</b> Random Variation</a><ul>
<li class="chapter" data-level="15.2.1" data-path="computing02.html"><a href="computing02.html#measurement-errors"><i class="fa fa-check"></i><b>15.2.1</b> Measurement errors</a></li>
<li class="chapter" data-level="15.2.2" data-path="computing02.html"><a href="computing02.html#biological-variation"><i class="fa fa-check"></i><b>15.2.2</b> Biological variation</a></li>
<li class="chapter" data-level="15.2.3" data-path="computing02.html"><a href="computing02.html#example-2"><i class="fa fa-check"></i><b>15.2.3</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="computing02.html"><a href="computing02.html#when-these-laws-dont-apply"><i class="fa fa-check"></i><b>15.3</b> When these Laws don’t apply</a></li>
<li class="chapter" data-level="15.4" data-path="computing02.html"><a href="computing02.html#summary-3"><i class="fa fa-check"></i><b>15.4</b> SUMMARY</a><ul>
<li class="chapter" data-level="15.4.1" data-path="computing02.html"><a href="computing02.html#computing-1"><i class="fa fa-check"></i><b>15.4.1</b> Computing</a></li>
<li class="chapter" data-level="15.4.2" data-path="computing02.html"><a href="computing02.html#statistical-concepts-and-principles-1"><i class="fa fa-check"></i><b>15.4.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="computing03.html"><a href="computing03.html"><i class="fa fa-check"></i><b>16</b> Computing Week3</a><ul>
<li class="chapter" data-level="16.1" data-path="computing03.html"><a href="computing03.html#ages-of-books"><i class="fa fa-check"></i><b>16.1</b> Ages of books</a></li>
<li class="chapter" data-level="16.2" data-path="computing03.html"><a href="computing03.html#ngrams"><i class="fa fa-check"></i><b>16.2</b> ngrams</a></li>
<li class="chapter" data-level="16.3" data-path="computing03.html"><a href="computing03.html#ice-breakup-dates"><i class="fa fa-check"></i><b>16.3</b> Ice Breakup Dates</a><ul>
<li class="chapter" data-level="16.3.1" data-path="computing03.html"><a href="computing03.html#the-2018-book-of-guesses"><i class="fa fa-check"></i><b>16.3.1</b> The 2018 Book of Guesses</a></li>
<li class="chapter" data-level="16.3.2" data-path="computing03.html"><a href="computing03.html#trends-over-the-last-100-years"><i class="fa fa-check"></i><b>16.3.2</b> Trends over the last 100 years</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="computing03.html"><a href="computing03.html#galtons-data-on-family-heights"><i class="fa fa-check"></i><b>16.4</b> Galton’s data on family heights</a></li>
<li class="chapter" data-level="16.5" data-path="computing03.html"><a href="computing03.html#temperature-perceptions"><i class="fa fa-check"></i><b>16.5</b> Temperature perceptions</a></li>
<li class="chapter" data-level="16.6" data-path="computing03.html"><a href="computing03.html#natural-history-of-prostate-cancer"><i class="fa fa-check"></i><b>16.6</b> Natural history of prostate cancer</a></li>
<li class="chapter" data-level="16.7" data-path="computing03.html"><a href="computing03.html#serial-psa-values"><i class="fa fa-check"></i><b>16.7</b> Serial PSA values</a></li>
<li class="chapter" data-level="16.8" data-path="computing03.html"><a href="computing03.html#graphics"><i class="fa fa-check"></i><b>16.8</b> Graphics</a></li>
<li class="chapter" data-level="16.9" data-path="computing03.html"><a href="computing03.html#possible-body-mass-indices"><i class="fa fa-check"></i><b>16.9</b> Possible Body Mass Indices</a></li>
<li class="chapter" data-level="16.10" data-path="computing03.html"><a href="computing03.html#galton"><i class="fa fa-check"></i><b>16.10</b> Galton</a></li>
<li class="chapter" data-level="16.11" data-path="computing03.html"><a href="computing03.html#epidemics"><i class="fa fa-check"></i><b>16.11</b> Epidemics</a></li>
<li class="chapter" data-level="16.12" data-path="computing03.html"><a href="computing03.html#duplicate-birthdays"><i class="fa fa-check"></i><b>16.12</b> Duplicate Birthdays</a></li>
<li class="chapter" data-level="16.13" data-path="computing03.html"><a href="computing03.html#lottery-payoffs"><i class="fa fa-check"></i><b>16.13</b> Lottery payoffs</a></li>
<li class="chapter" data-level="16.14" data-path="computing03.html"><a href="computing03.html#chevalier-de-méré"><i class="fa fa-check"></i><b>16.14</b> Chevalier de Méré</a></li>
<li class="chapter" data-level="16.15" data-path="computing03.html"><a href="computing03.html#detecting-a-fake-bernoulli-sequenece"><i class="fa fa-check"></i><b>16.15</b> Detecting a fake Bernoulli sequenece</a></li>
<li class="chapter" data-level="16.16" data-path="computing03.html"><a href="computing03.html#cell-occupancy"><i class="fa fa-check"></i><b>16.16</b> Cell occupancy</a></li>
<li class="chapter" data-level="16.17" data-path="computing03.html"><a href="computing03.html#life-tables"><i class="fa fa-check"></i><b>16.17</b> Life Tables</a></li>
<li class="chapter" data-level="16.18" data-path="computing03.html"><a href="computing03.html#carrier-status-genetics"><i class="fa fa-check"></i><b>16.18</b> Carrier Status (genetics)</a></li>
<li class="chapter" data-level="16.19" data-path="computing03.html"><a href="computing03.html#diagnostic-and-statistical-tests"><i class="fa fa-check"></i><b>16.19</b> Diagnostic and statistical tests</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="dalite.html"><a href="dalite.html"><i class="fa fa-check"></i><b>17</b> DALITE</a><ul>
<li class="chapter" data-level="17.1" data-path="dalite.html"><a href="dalite.html#aim"><i class="fa fa-check"></i><b>17.1</b> Aim</a></li>
<li class="chapter" data-level="17.2" data-path="dalite.html"><a href="dalite.html#how-it-works"><i class="fa fa-check"></i><b>17.2</b> How it works</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Analysis: a regression-from-the-outset approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Statistical Inference</h1>
<p>Google gives the following defintion</p>
<blockquote>
<p>The theory, methods, and practice of forming judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling.</p>
</blockquote>
<p>The Oxford English Dictionary defines it as</p>
<blockquote>
<p>The drawing of inferences about a population based on data taken from a sample of that population; an inference drawn in this way; the branch of statistics concerned with this procedure.</p>
</blockquote>
<p>Gelman and Hills start their regresssion textbook this way:</p>
<blockquote>
<p>The goal of statistical inference for the sorts of parametric models that we use is to estimate underlying parameters and summarize our uncertainty in these estimates. We discuss inference more formally in Chapter 18; here it is enough to say that we typically understand a fitted model by plugging in estimates of its parameters, and then we consider the uncertainty in the parameter estimates when <strong>assessing how much we actually have learned from a given dataset.</strong> Gelman and Hills 2007.</p>
</blockquote>
<p>We would add to these numerical statements about unknown (and unknowable) constants, as well as the mechanism or process that generated the limited data you got/get to observe.</p>
<p>Some example parameters – some scientific, some more personal or particularistic – include</p>
<ul>
<li>Whether</li>
<li>a potential hemophilia carrier is in fact a carrier</li>
<li>a particular email is malicious</li>
<li>a person committed the crime they are accused of</li>
<li><p>a person has been infected with a certain virus</p></li>
<li>The proportion of</li>
<li>thumbtacks that land on their back when tossed</li>
<li>your time that you are being productive</li>
<li>the earth’s surface that is covered by water</li>
<li>your driving time that you are on the phone</li>
<li>your time (over the entire year) that you spend inside</li>
<li>patients whose disease would respond to a medication</li>
<li><p>people who would volunteer for a demanding survey or long-term research study</p></li>
<li>The numerical value for</li>
<li>the density of the Earth,relative to water</li>
<li>the age of a person whom you have just met</li>
<li>your cholesterol level</li>
<li>the mean depth of the ocean</li>
<li>the 20th percentile of the depths of the ocean</li>
<li><p>the median age of a population</p></li>
</ul>
<p>To address the uncertainties involved in the judgements/inferences, some use of probabilities is required.</p>
<p>In their preamble to their chapter on inferemce, Clayton and Hills tell us that</p>
<blockquote>
<p>There are <strong>two radically different approaches</strong> to associating a probability with a range of parameter values, reflecting a deep philosophical division amongst mathematicians and scientists about the nature of probability. We shall start with the more orthodox view within biomedical science.</p>
</blockquote>
<p>Clayton and Hills completed their book in 1993. Since then, propelled by greater computer power, and by people like Clayton’s Cambridge colleague David Spiegelhalter, whose book we will start with, the Bayesian approach to ‘associating a probability with a range of parameter values’ has become more common. It has not yet reached the status of ‘customary or conventional, as a means or method; established.’ that the dictionaries give as the meaning of orthodox. In any case, we should not take Clayton and Hills’ use of the phrase ‘more orthodox’ to describe the <em>frequentist</em> approach to mean that the Bayesian approach <code>does not  conform to the approved form of analysis' or is in some sense</code>wrong.’</p>
<p>The first-established of the two ‘schools’ (or ‘churches’) of statistical inference** makes <strong>direct probabilistic statements about the possible parameter values</strong>. This approach goes back at least as far as the mid-1700’s essay ‘A method of calculating the exact probability of all conclusions based on induction’; ironically the author was a Presbyterian minister.</p>
<p>The developments since then are nicely told in the very readable book <a href="https://www.amazon.com/Theory-That-Would-Not-Die-ebook/dp/B0050QB3EQ"><em>The Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy</em></a> by Sharon Bertsch McGrayne, and in her <a href="https://www.youtube.com/embed/2o-_BGqYM5U">Microsoft lecture</a> and her <a href="https://www.youtube.com/embed/8oD6eBkjF9o">Google lecture</a>.</p>
<p>Maybe, by calling it the more ‘orthodox’, all that Clayton and Hills mean is that the frequentist approach is more popular method today. It got a slow start, and dates from the early 20th century. (In one of our sampling exercises, we will try to determine the relative frequencies of the two approaches in the epidemiology and medical literature).</p>
<p>Interestingly, if you use <a href="https://books.google.com/ngrams/graph?content=frequentist+approach%2C+bayesian+approach&amp;case_insensitive=on&amp;year_start=1800&amp;year_end=2008&amp;corpus=15&amp;smoothing=3&amp;share=&amp;direct_url=t4%3B%2Cfrequentist%20approach%3B%2Cc0%3B%2Cs0%3B%3Bfrequentist%20approach%3B%2Cc0%3B%3BFrequentist%20approach%3B%2Cc0%3B%3BFrequentist%20Approach%3B%2Cc0%3B.t4%3B%2Cbayesian%20approach%3B%2Cc0%3B%2Cs0%3B%3BBayesian%20approach%3B%2Cc0%3B%3BBayesian%20Approach%3B%2Cc0%3B%3BBAYESIAN%20APPROACH%3B%2Cc0%3B%3Bbayesian%20approach%3B%2Cc0">Google Books Ngram Viewer</a> you get a different sense. Maybe this is because the majority don’t need to justify the methods they use!</p>
<p>Frequentist statements are <strong>indirect</strong>. They are (conditional) probabilistic statements about the <strong>data</strong> and about the performance of the <strong>procedure</strong> used to bracket the parameter values. A variant on it ranks the various possible parameter values according to how probable the observed data would be under each of these, but does not make direct probabilistic statements about the parameter values themselves. Because it is indirect, conditional, the results are often interpreted incorrectly.</p>
<p>We begin with the direct method, one that studies tell us we are born with, and use throughout our lives, both consciously and subconsciously, to continue to learn/update.</p>
<blockquote>
<p>When we learn a new motor skill, such as playing an approaching tennis ball, both our sensors and the task possess variability. […] We show that subjects internally represent both the statistical distribution of the task and their sensory uncertainty, combining them in a manner consistent with a performance-optimizing bayesian process. The central nervous system therefore employs probabilistic models during sensorimotor learning. <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/nature02169.pdf">Bayesian integration in sensorimotor learning</a></p>
</blockquote>
<p>leading to this New York Times headline</p>
<blockquote>
<p><a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/AthletesStatisticians.pdf">Subconsciously, Athletes May Play Like Statisticians</a></p>
</blockquote>
<div id="the-bayesian-approach" class="section level2">
<h2><span class="header-section-number">3.1</span> The Bayesian Approach</h2>
<p>to probability statements concerning parameter values.</p>
<blockquote>
<p>Bayesian inference refers to statistical procedures that <strong>model unknown parameters […] as random variables</strong>. […] Bayesian inference starts with a prior distribution on the unknown parameters and updates this with the likelihood of the data, yielding a posterior distribution which is used for inferences and predictions. Gelman and Hills, 2007, page 143.</p>
</blockquote>
<p>This paragraph is taken from this chapter <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/an%20overview%20of%20the%20Bayesian%20approach.pdf">An Overview of the Bayesian Approach</a> in the book Bayesian Approaches to Clinical Trials and Health-Care Evaluation by David Speigelhalter et al, describes it well:</p>
<blockquote>
<p>The standard interpretation of probability describes long-run properties of repeated random events (Section 2.1.1). This is known as the frequency interpretation of probability, and standard statistical methods are sometimes referred to as ‘frequentist’. In contrast, the <strong>Bayesian approach</strong> rests on an essentially ‘subjective’ interpretation of probability, which is allowed to express generic uncertainty or ‘degree of belief’ about any unknown but potentially observable quantity, whether or not it is one of a number of repeatable experiments. For example, it is quite reasonable from a subjective perspective to think of a probability of the event ‘Earth will be openly visited by aliens in the next ten years’, whereas it may be difficult to interpret this potential event as part of a ‘long-run’ series. Methods of assessing subjective probabilities and probability distributions will be discussed in Section 5.2.</p>
</blockquote>
<p>Section 3.1 SUBJECTIVITY AND CONTEXT emphasizes that ‘the vital point of the subjective interpretation is that <strong>Your probability</strong> for an event is a property of <strong>Your</strong> relationship to that event, and not an objective property of the event itself.’ Moreover, ‘pedantically speaking, one should always refer to probabilities <strong>for</strong> events rather than probabilities <strong>of</strong> events, and the <strong>conditioning context</strong> used in Section 2.1.1 <strong>includes the observer and all their background knowledge and assumptions.</strong>’</p>
<p>That there is ‘always a context’ goes along with what we read in Alan Turing’s recently de-classified essay The Applications of Probability to Cryptography. Under section 1.2 (‘Meaning of probability and odds’) he starts out</p>
<blockquote>
<p>I shall not attempt to give a systematic account of the theory of probability, but it may be worth while to define shortly probability and odds. The probability of an event <strong>on certain evidence</strong> is the proportion of cases in which that event may be expected to happen given that evidence. For instance if it is known the 20% of men live to the age of 70, then knowing of Hitler only Hitler is a man we can say that the probability of Hitler living to the age of 70 is 0.2. Suppose that we know that Hitler is now of age 52 the probability will be quite different, say 0.5, because 50% of men of 52 live to 70.</p>
</blockquote>
<p><strong>Not all context is subjective</strong>. We will start with a context where the initial (starting out, pre-new-data) probability is <strong>objective</strong>.</p>
<p>Just before we do, we include this passage from Clayton and Hills, in subchapter 10.2 Subjective probablity, which they denote as optional material.</p>
<blockquote>
<p>The second approach to the problem of assigning a probability to a range of values for a parameter is based on the philosophical position that probability is a subjective measure of ignorance. The investigator uses probability as a measure of subjective degree of belief in the different values which the parameter might take. With this view it is perfectly logical to say that there is a probability of 0.9 that the parameter lies within a stated range.<br />
Before observing the data, the investigator will have certain beliefs about the parameter value and these can be measured by a priori probabilities. Because they are subjective every scientist would be permitted to give different probabilities to different parameter values. However, the idea of scientific objectivity is not completely rejected. In this approach objectivity lies in the rule used to modify the a priori probabilities in the light of the data from the study. This is Bayes’ rule and statisticians who take this philosophical position call themselves Bayesians.<br />
Bayes’ rule was described in Chapter 2, where it was used to calcu- late the probabilities of exposure given outcome from the probabilities of outcome given exposure. Once we are prepared to assign probabilities to parameter values, Bayes’ rule can be used to calculate the probability of each value of a parameter (<span class="math inline">\(\theta\)</span>) given the data, from the probability of the data given the value of the parameter.<br />
The argument is illustrated by two tree diagrams. Fig. 10.2 illustrates the direction in which probabilities are specified in the statistical model — given the choice of the value of the parameter, <span class="math inline">\(\theta\)</span>, the model tells us the probability of the data. The probability of any particular combination of data and parameter value is then the product of the probability of the parameter value and the probability of data given the parameter value. In this product, the first term, Pr(<span class="math inline">\(\theta\)</span>), represents the a priori degree of belief for the value of <span class="math inline">\(\theta\)</span> and the second term, Pr(Data | <span class="math inline">\(\theta\)</span> ), is the likelihood. Fig. 10.3 reverses the conditioning argument, and expresses the joint probabilityas the product of the overall probability of the data multiplied by the probability of the parameter given the data. This latter term, Pr( <span class="math inline">\(\theta\)</span> | Data), represents the posterior degree of belief in the parameter value once the data have been observed. Since the joint probability of data and parameter value is the same no matter which way we argue, so that <span class="math display">\[Pr(\theta) \times Pr(Data | \theta) = Pr(Data) \times Pr(\theta | Data),\]</span> so that <span class="math display">\[Pr(\theta | Data) = \frac{Pr(\theta) \times Pr(Data | \theta)}{Pr(Data)}\]</span> Thus elementary probability theory tells us how prior beliefs about the value of a parameter should be modified after the observation of data.</p>
</blockquote>
<p>Their 2 figures nicely show that the difference is in the directionality or conditioning, i.e. is the object <span class="math display">\[Pr(\theta | data) \ \ \ OR \ \ \   Pr(data | \theta) \ \ ?\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="images/ClaytonHillsFig102103.png" alt="From Chapter 10.2 of Clayton and Hills" width="604" />
<p class="caption">
Figure 3.1: From Chapter 10.2 of Clayton and Hills
</p>
</div>
<p><strong>Without getting into the details of the calculations, we will apply this approach to the first example in each of the parameter genres listed above. The point is to illustrate how direct and unambigous the answer is in each case.</strong></p>
<div id="example-parameter-is-2-valued-yes-or-no" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Example: parameter is 2-valued: yes or no</h3>
<p>In the first genre, the parameter is personal or particular. In each of the examples, the true state is binary. The potential hemophilia carrier <strong>is</strong> a hemophia carrier or <strong>is not</strong>; the particular email is either malicious or is not; the person in question either committed the crime ot did not. So, there are just two possible parameter values: yes or no, <strong>is</strong> or <strong>is not</strong>.</p>
<p>From the outset, just like in Turing’s example, there is a given context. For example, suppose a woman’s brother is known to have haemophilia.</p>
<blockquote>
<p>hemophilia: a medical condition in which the ability of the blood to clot is severely reduced, causing the sufferer to bleed severely from even a slight injury. The condition is typically caused by a hereditary lack of a coagulation factor, caused by a mutation in one of the genes located on the X chromosome. - Google</p>
</blockquote>
<p>Just knowing this, the probability that the woman is a hemophilia carrier is 50% or 1/2.</p>
<p>Today, genetic testing of the carrier can help determine whether the woman is a carrier. But when JH first taught 607, the only time to learn more about her carrier status (and move her probability to 1, or towards 0) was after the births of her sons: their status was knowable virtually immedediately.</p>
<p>If it is determined that the first son has hemophilia, it establishes that she IS a carrier, thereby moving the probability up to 1. If he was not, it moves the probability down to 1/3: in other words, among ‘women like her’, i.e, other potential carriers who also have had 1 son who turned out to be Normal (NL), 1/3 of the sons are the sons of carriers, and 2/3 are the sons of non-carriers.</p>
<p>The <strong>continued updating</strong> as the women with a NL son gave birth to a second son, and so on, is shown in the diagram below, with <strong>C</strong> used as shorthand for ‘<strong>Is</strong> a <strong>C</strong>arrier.’ Technically speaking, each sequential P[C] should indicate that it is ‘conditioned on’ – and thus reflects the information in – the history up to that point. In other words, the 1/5 probability refers to P[C | both sons are NL], where “|” stands for ‘given that’, or – to use Turing’s phrase – ‘on the evidence that’.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="statbook_files/figure-html/unnamed-chunk-7-1.png" alt="At the outset, each woman had a 50:50 chance of being a haemophilia carrier. Accumulating information from the hemophilia status of the sons increasingly 'sorts' or segregates the women by moving their probabilities of being a carrier TO 1 (100%) or FURTHER TOWARDS 0 (0%). It 'updates' the probablity of  being a carrier, P[C]. For brevity,  the ' | data' in each P[C | data] is omitted." width="864" />
<p class="caption">
Figure 3.2: At the outset, each woman had a 50:50 chance of being a haemophilia carrier. Accumulating information from the hemophilia status of the sons increasingly ‘sorts’ or segregates the women by moving their probabilities of being a carrier TO 1 (100%) or FURTHER TOWARDS 0 (0%). It ‘updates’ the probablity of being a carrier, P[C]. For brevity, the ‘| data’ in each P[C | data] is omitted.
</p>
</div>
<blockquote>
<p>ASIDE: This is similar to how researchers develop strains of “transgenic” mice, by introducing an altered gene (transgene) into the genome. In order to breed true, theanimals must be made to be homozygous, i.e., to have two copies of the introduced gene (+ +). Molecular biology techniques can detect whether the transgene is present in an individual animal (without having to sacrifice the animal), but cannot distinguish a hemizygote, with one copy of the gene (+ -), from a homozygote (+ +). This difference can only be detected by breeding strategies. First generations: A copy of the transgene is injected into the pronucleus of a newly fertilized ovum, prior to fusion with the male pronucleus. Thus all animals that develop from these zygotes can have at most one copy of the gene, from the ovum. After birth, screening is performed to detect these ‘positive’ animals, called founders. After sexual maturation, all founders are bred to normal ‘wild type’ (WT) animals, to ensure that the transgene has been incorporated in such a way as to be heritable. Pairs of positive (hemizygous) animals in this F1 generation are then bred to each other. By Mendelian genetics, the distri- bution of F2 offspring should be 1:2:1, homozygous transgenic : hemizygous transgenic : homozygous normal. The homozygous normal animals are not used. The question is, how to tell the homozygous transgenic mice (the desired ones) from the hemizygous transgenic ones? Note that the mix in this reduced population is 1 homozygous transgenic to 2 hemizygous transgenic. F2 breeding: All ’positive’ F2 animals (i.e. all homozygous and hemizygous animals) are bred to wild type. Possible F3 genotypes are as follows: (by Mendelian genetics) Hemizygous (which comprise 2/3 of the F2 animals used) x wild type = 50:50, hemizygous (and therefore ‘positive’) : normal (and therefore ‘negative’), Homozygous (which comprise 1/3 of the F2 animals used) x wild type = all hemizygous (and therefore ‘positive’). That is, while only half of the offspring from a Hemi x WT pair will be ‘positive’ when screened, all of the offspring of a Homo x WT pair will be ‘positive’. The question: How many F3 offspring from a particular pairing does the researcher have to screen before declaring the positive parent as homozygous? Note: as soon as an offspring is screened as ‘negative,’ one knows the parent must have been hemizygous. A variant on the above diagram can help withe probabilities. Furthe details are avilable on the bios601 website, in the ‘probability’ chapter.</p>
</blockquote>
<p>Before moving on the the next type of parameter, a few points</p>
<ul>
<li><p>In both the hemophilia and transgenic mice examples, the ‘starting’ probability is objective and the post-data probabilities have a ‘long-run’ or ‘in large numbers of similar instances’ interpretation. One could make a diagram that shows the expected numbers ‘in every 100 women like this.’</p></li>
<li><p>There is nothing special about the ‘starting out’ probability P[<strong>C</strong>] of 0.5. Before a pregnancy test, or a pre-natal diagnostic test, for example, the probability of the target <strong>C</strong>ondition/state of interest/concern would be a function of many other factors, and could in theory take on any value between 0 and 1. The (starting out, pre-filter) proportion of malicious emails would depend on which of a person’s email accounts it was.</p></li>
<li><p>In the language of diagnostic tests, each ‘Son as a test of the mother’s carrier status’ has 50% sensitivity and 100% ‘specificity’. For sensitivity, this puts it on par with the Pap test for cervical cancer: the main problem withe latter is in the sampling. For specificity, it is better than most tests.</p></li>
<li><p>The shiny app <a href="https://jameshanley.shinyapps.io/FromPreTestToPostTestProbabilities/">From Pre-test to Post-test Probabilities</a> shows how the initial average (pre-test) probability is segregated into 2 post-test probabilities by the 2 possible test results, and the role of the 2 error-probabilities (just about all tests are fallible) in how well they push them out.</p></li>
<li><p>The ‘starting out’ probability could be subjective. For example it could be one’s impression (before getting to see up close how wrinkled their face is) as whether the person is a smoker, or one’s assessment of the probability that the accused is guilty (before getting to hear the DNA expert, or lie detection report) based on how credible the accused appears to be, and all of the other evidence to date.</p></li>
<li><p>The main point is that we are merging (adding) two sources of information.</p></li>
</ul>
</div>
<div id="example-parameter-is-a-proportion" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Example: parameter is a proportion</h3>
<p>In theory, in this genre, the true parameter value could in theory lie anywhere between 0 and 1, But again, just like in Turing’s example, we seldom start from complete ignorance, or with – in the title of pscychologist Stephen Pinker’s book – a blank slate. Even if you have never seen thumbtacks tossed onto on a surface, you can reason informally, and indicate what proportions are unlikely and likely, and where along the (0,1) scale you would ‘put your money’.</p>
<p>You could do the same when asked what proportions of your time that you are being productive, or on the phone, or sedentary, or indoors. Mind you, you might be ‘way off’ with your claims, but the nice thing is that — and this is the point of this course – you can generate data to narrow down the true proportion.</p>
<p>The other nice thing with the Bayesian approach in particular is that – no matter whether you believe the proportion is low or medium or high, we can work out what your post-data beliefs should be. It is a matter of mathematics. If, before collecting any new data, we have ‘no idea’ – a common phrase among todays’s generation, one that, if it is uttered with empahsis on the ‘no’, irks JH to no end – what the true parameter value is, that is easily handled. Moreover, enough valid data will (or should!) trump the pre-data beliefs.</p>
<blockquote>
<p>On a side note: Dick Pound, a former chancellor of McGill University, and first president of the World Anti-Doping Agency is a staunch advocate of strict drug testing for athletes. Discussing the National Hockey League in November 2005, Pound said, ‘you wouldn’t be far wrong if you said a third of hockey players are gaining some pharmaceutical assistance.’ Pound would later admit that he completely invented the figure. Both the NHL and NHLPA have denied the claims, demanding Pound provide evidence rather than make what they term unsubstantiated claims. Since his comments were made, some NHL players have tested positive for banned substances, including Bryan Berard, José Théodore, and two of 250 players involved in Olympic testing. As of June 2006, there had been 1,406 tests in the program jointly administered by the league and the union, and none has come up with banned substances under NHL rules. Pound remained skeptical, claiming the NHL rules were too lax and unclear, as they do not test for some banned substance, including certain stimulants. In an interview with hockey blogger, B. D. Gallof, of Hockeybuzz on December 19, 2007, Pound was asked to expand on the 30% comment and subsequent reaction, expounded that stimulants was ‘the NHL’s drug of choice’. He also cited that the NHL will have no credibility on a drug policy if it, and other sports, continue to run things ‘in-house’. <a href="https://en.wikipedia.org/wiki/Dick_Pound" class="uri">https://en.wikipedia.org/wiki/Dick_Pound</a> and <a href="https://www.cbc.ca/sports/hockey/dick-pound-slams-nhl-s-drug-policy-1.557993" class="uri">https://www.cbc.ca/sports/hockey/dick-pound-slams-nhl-s-drug-policy-1.557993</a></p>
</blockquote>
<p>Even before studying/asking them, investigators would have some sense of the proportions of patients whose disease would respond to a medication, or people who would volunteer for a survey or research study. These iimpressions would probably be based on previous analogous situations, and the ‘literature’, but would vary from pundit to pundit. But ultimately, they could be much improved and narrowed (and even replaced entirely) by new-data-based ones.</p>
<p>The proportion of the earth’s surface that is covered by water is easy to determine: just look up a reputable source. But what if you weren’t able to, but did have access to the database of 933 million recordings in the <a href="https://topex.ucsd.edu/cgi-bin/get_srtm30.cg">SRTM30PLUS database</a>. It has altitude/depth measurements for 43,200 x 21,600 = 933,120,000 locations. This database is so large that you would have to sample from it. From a thousand randomly chosen loactions, you would be able to ‘trust’ the first decimal in your estimate; from a million you should be able to trust the second – and maybe the third.</p>
<p>Since we already know/remember from high school ‘roughly’ what the proportion is, we will leave it for an exercsie in another chapter. In this chapter, following the advice of master-teacher Fred Mosteller, we use examples where the <strong>correct answer is not known with any precision</strong>. The proportion of these we probably know the least about is the thumbtack one. However, it has fewer personal benefits than knowing what proportion of your time you are being productive. Moreover, we have a nice written account of how you might go about learning this personal proportion.</p>
<p>In his book Elementary Bayesian Statistics, Gordon Antelman informally introduces and illustrate a Bayesian analysis of an uncertain proportion with a slightly modified version of a novel and useful application of work sampling discussed by Fuller (1985). We have changed his notation for the proportion of your time spent in productive work, and called it <span class="math inline">\(\pi\)</span>, and also modified some of his words.</p>
<blockquote>
<p>Suppose you, as a good up-to-date manager practicing continuous quality and productivity improvement, have some ideas on improving your own productivity. To see if these ideas have any merit, you would like to compare some ‘before’ measure of productivity with a comparable ‘after’ measure of productivity.<br />
For now — we shall come back to this example several times — let us focus on just a ‘before’ measure. The measure to be used is the proportion of your time spent in productive work, call it <span class="math inline">\(\pi\)</span>, as opposed to time spent doing something that would not have needed doing if things had been done right the first time. Examples of the latter might include searching for a misplaced document, recreating a deleted computer file, following up on a customer’s complaint, or waiting past a scheduled time for a meeting to start. [Today, we would add being on social media, or browsing the web for non-work-related matters] Rather than saying <span class="math inline">\(\pi\)</span> is not (precisely) ‘known’, it is better to say that ‘<span class="math inline">\(\pi\)</span> is uncertain’; from your job experience, you would really know quite a lot about p. For example, you might be almost certain that it is greater than 0.50, less than 0.90, and you might assess your odds that <span class="math inline">\(\pi\)</span> is between, say, 0.60 and 0.80 to be about 9 to 1. A precise statement of these beliefs will be your prior distribution for <span class="math inline">\(\pi\)</span>.<br />
You would probably feel uncomfortable — most people do — about assessing this prior distribution, especially since there are an infinite number of states; viz., all of the values between zero and one. But, without any real loss, you can bypass the infinite-number problem by rounding the values of <span class="math inline">\(\pi\)</span> to the nearest 5% or 10%, making the problem discrete. Then you have a contemplatable Bayes’ theorem, like those discussed in Chapter 4, with the finitely many <span class="math inline">\(\pi\)</span>-values as the possible “states”. (When we reconsider this example later in this chapter, you will see that, with a little theory, the infinite number of <span class="math inline">\(\pi\)</span>-values can almost always be handled very neatly and more easily.)</p>
</blockquote>
<p><strong>PRIOR BELIEFS</strong> For illustration, he supposes you choose just five possible values for <span class="math inline">\(\pi\)</span>, and assess your prior distribution. Since <code>R</code> does not allow Greek symbols, we will refer to it by uppercase P and assess your prior distribution. This possible prior distribution, shown below, would reflect, for example, that your judgment is that there is only about one chance in 20 that <span class="math inline">\(\pi\)</span> rounds to 0.50, about one chance in 20 that <span class="math inline">\(\pi\)</span> rounds to 0.90, about one chance in four that it rounds to 0.60, a little more than one chance in three that it rounds to 0.70, and a little less than one chance in three that it rounds to 0.80.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">P =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.60</span>, <span class="fl">0.70</span>, <span class="fl">0.80</span>, <span class="fl">0.90</span>)
PriorProbForP =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.35</span>, <span class="fl">0.30</span>, <span class="fl">0.05</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">plot</span>(P, PriorProbForP, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),
     <span class="dt">lwd=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>),  <span class="dt">lend=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;grey80&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;P: Proportion of time that I am being productive&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;My probability that P rounds to... &quot;</span>,
     <span class="dt">cex.lab =</span> <span class="fl">1.25</span>, <span class="dt">cex.axis =</span> <span class="fl">1.25</span>)
<span class="kw">text</span>(<span class="fl">0.5</span>,<span class="fl">0.25</span>, <span class="st">&quot;Prior Probabilities&quot;</span>, <span class="dt">font=</span><span class="dv">2</span>,
     <span class="dt">col=</span><span class="st">&quot;grey75&quot;</span>, <span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">cex=</span><span class="fl">1.75</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="statbook_files/figure-html/unnamed-chunk-8-1.png" alt="Prior Probabilities for the parameter P, the proportion of time that I am being productive." width="864" />
<p class="caption">
Figure 3.3: Prior Probabilities for the parameter P, the proportion of time that I am being productive.
</p>
</div>
<p><strong>DATA</strong>: Suppose you are fitted with a beeper set to beep at random times; when the beeper beeps, you classify the task being worked on as <strong>W</strong> — for ‘productive <strong>Work</strong>’, or <strong>F</strong> — for ‘<strong>Fixing</strong>’ (or today we mght say ‘<strong>F</strong>iddling’ or ‘<strong>F</strong>ooling around’ or wasting time).</p>
<p>Although we will skip the technicalities, it is important that the experiment be designed so that the trials are independent. Beeps should be unpredictable so you do not arrange, possibly subconsciously, to be doing productive work at the beep. They probably also should not be too close together to make the independence assumption more reasonable.</p>
<p><strong>Suppose the first four trials give the data <span class="math inline">\(F_1, F_2, W_3\)</span>, and <span class="math inline">\(F_4\)</span>.</strong></p>
<p>Below is a picture showing the effects of the data FFWF on the prior distribution. Three F’s in four trials increase your probabilities for the two smaller values of P and decrease your probabilities for the three larger ones.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Lik =<span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>P)<span class="op">^</span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>P
PosteriorProbForP =<span class="st"> </span>(PriorProbForP <span class="op">*</span><span class="st"> </span>Lik) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(PriorProbForP <span class="op">*</span><span class="st"> </span>Lik)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">plot</span>(P, PriorProbForP, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),
     <span class="dt">lwd=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>),  <span class="dt">lend=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;grey80&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;P: Proportion of time that I am being productive&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;My probability that P rounds to... &quot;</span>,
     <span class="dt">cex.lab =</span> <span class="fl">1.25</span>, <span class="dt">cex.axis =</span> <span class="fl">1.25</span>)
<span class="kw">text</span>(<span class="fl">0.5</span>,<span class="fl">0.25</span>, <span class="st">&quot;Prior Probabilities&quot;</span>, <span class="dt">font=</span><span class="dv">2</span>,
     <span class="dt">col=</span><span class="st">&quot;grey75&quot;</span>, <span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">cex=</span><span class="fl">1.75</span>)
<span class="kw">lines</span>(P, PosteriorProbForP, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">lwd=</span><span class="dv">6</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lend=</span><span class="dv">1</span>)
<span class="kw">text</span>(<span class="fl">0.5</span>,<span class="fl">0.4</span>, <span class="st">&quot;Posterior Probabilities</span><span class="ch">\n</span><span class="st">after observing 1W, 3F&quot;</span>, <span class="dt">font=</span><span class="dv">2</span>,
     <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">cex=</span><span class="fl">1.75</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="statbook_files/figure-html/unnamed-chunk-9-1.png" alt="Prior Probabilities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior probabilities, after observing that in n = 4 randomly sampled occasions, I was actually productive in only 1 of the 4." width="864" />
<p class="caption">
Figure 3.4: Prior Probabilities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior probabilities, after observing that in n = 4 randomly sampled occasions, I was actually productive in only 1 of the 4.
</p>
</div>
<p>The sample alone most strongly supports a value for P of 0.25 (one W in four trials); had the prior included a value of P of 0.25, the (relative) increase in going from prior to posterior would have been greatest for that value.</p>
<p>For the assumed prior, in which only p’s of 0.50, 0.60, 0.70, 0.80, or 0.90 are considered, the sample evidence FFWF in favor of a P near 0.25 can only push up the posterior probabilities for the nearest possible values - 0.50 and 0.60. (The seemingly harder consideration of <strong>all possible p’s between zero and one</strong> will handle this kind of situation more logically.)</p>
<p>Below we show the ‘<strong>continuous P </strong>’ version Antelman refers to. To make this, we calculated the mean and variance of his discrete (5-point) prior distribution, and converted them to the 2 parameters, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, of the beta distribution with the same mean and variance.</p>
<p>Conveniently, the posterior density is also a beta distribution, but with parameters <span class="math inline">\(a+1\)</span> and <span class="math inline">\(b+3\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean.P.prior =<span class="st"> </span><span class="kw">sum</span>( P<span class="op">*</span><span class="st"> </span>PriorProbForP )
var.P.prior  =<span class="st"> </span><span class="kw">sum</span>( (P<span class="op">-</span>mean.P.prior)<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>PriorProbForP )

a.plus.b =<span class="st"> </span>mean.P.prior <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>mean.P.prior) <span class="op">/</span><span class="st"> </span>var.P.prior  <span class="op">-</span><span class="st"> </span><span class="dv">1</span>
a =<span class="st"> </span>mean.P.prior <span class="op">*</span><span class="st"> </span>a.plus.b
b =<span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>mean.P.prior) <span class="op">*</span><span class="st"> </span>a.plus.b

P =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)
prior.density =<span class="st"> </span><span class="kw">dbeta</span>(P,a,b)
posterior.density =<span class="st"> </span><span class="kw">dbeta</span>(P,a<span class="op">+</span><span class="dv">1</span>,b<span class="op">+</span><span class="dv">3</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-11"></span>
<img src="statbook_files/figure-html/unnamed-chunk-11-1.png" alt="Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 4 randomly sampled occasions, I was actually productive  (W) in only 1 of the 4." width="864" />
<p class="caption">
Figure 3.5: Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 4 randomly sampled occasions, I was actually productive (W) in only 1 of the 4.
</p>
</div>
<p>Before moving on the the next type of parameter, a few points:</p>
<ul>
<li><p>The beta distribution nicely shows <strong>how the prior information/impression and the new data get combined</strong>. The <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> parameters of the prior distribution are 14.8 and 6.2. Together, they determine the mean, <span class="math inline">\(a/(a=b)\)</span>, the median, the mode, <span class="math inline">\((a-1)/(a+b-2)\)</span>, and the variance, <span class="math inline">\(a^2b^2/(a+b+1)^2\)</span> of the prior distribution. Their conterparts in the data-likelihood are 1 and 3. The ‘<span class="math inline">\(a\)</span>’ and ‘<span class="math inline">\(b\)</span>’ parameters of the posterior distribution are 15.8 and 9.2: the <span class="math inline">\(a\)</span>’s <strong>add</strong>, and the <span class="math inline">\(b\)</span>’s <strong>add</strong>. In other words, the distribution of one’s pre-data beliefs is the distribution one would have after ‘seeing’ 14.8 W’s and 6.2 F’s; the distribution of one’s post-data beliefs is the distribution one would have after ‘seeing’ 15.8 W’s and 9.2 F’s. <strong>The (synthetic) experience-equivalent of the numbers of Ws and F’s in the prior are added to the actual (observed) numbers of Ws and F’s in the data to arrive at the (new) posterior distribution.</strong></p></li>
<li><p>You are probably wondering what the posterior distribution would look like <strong>with more data</strong>. Here is what it would look like after observing 7 out of 25 or 13 out of 25. The modes of the posterior distributions are still somewhat influenced by the prior – as they are still well above P=7/25 = 0.28 and P=13/25 = 0.52. If the data were 70/250 or 13/250, the modes would be closer to P= 0.28 or P = 0.52; in other words, the data would ‘swamp’ – or ‘trump’ – the prior.</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="statbook_files/figure-html/unnamed-chunk-12-1.png" alt="Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 25 randomly sampled occasions, I was actually productive (W) in only 7 of the 25, or 13 of the 25." width="864" />
<p class="caption">
Figure 3.6: Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 25 randomly sampled occasions, I was actually productive (W) in only 7 of the 25, or 13 of the 25.
</p>
</div>
<ul>
<li><p>Be thinking about your prior for the proportion of thumbtacks that land on their back, and the proportion of the Earth’s surface that is covered by water, or [these words written on March 30, 2020, before any trial data] the proportion of patients with mild symptoms of covid-19 who would benefit from chloroquine.</p></li>
<li><p>Think about how you might elicit a prior distribution. You might want to Google ‘tools for eliciting prior distributions’ – or consult Chapter 5 of Spiegelhalter’s book.</p></li>
</ul>
<p>We now move on to last parameter genre we will consider.</p>
</div>
<div id="examples-parameter-is-a-personal-number-or-population-mean" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Examples: parameter is a personal number or population mean</h3>
<p>We will start with a discrete version of a commonly-wondered-about parameter, the age of a person whom you have just met, or seen a photo of. We will then go on to full numerical examples: your average cholesterol or blood pressure level, a baseball player’s batting average</p>
<p><strong>Example 1</strong></p>
<p><strong>How old</strong> (or <strong>what age</strong> – if you prefer to avoid speaking of ‘old’) do you think this IBC2006 attendee was when this photo was taken? This is the <em>parameter of interest</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="images/IMG_DS.png" alt="An attendee at the International Biometrics Conference, held at McGill in July 2006" width="913" />
<p class="caption">
Figure 3.7: An attendee at the International Biometrics Conference, held at McGill in July 2006
</p>
</div>
<p>He tells you he <strong>got his PhD 32 years earlier</strong>. Based on the distribution of ages at which people get their PhD (shown in grey below), that puts his current age somewhere in the blue distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="statbook_files/figure-html/unnamed-chunk-14-1.png" alt="Current ages of persons who obtained PhD 32 years earlier." width="864" />
<p class="caption">
Figure 3.8: Current ages of persons who obtained PhD 32 years earlier.
</p>
</div>
<p>This is a somewhat unusual example, as the blue distribution is very wide – partly because we could not find age-at-graduation data specific to PhD graduates in statistics in 1974. We suspect that that specific distribution is a good deal narrower than the one shown. In the next chapter, you will see that other indirect measures of age are a good bit tighter than this.</p>
<p>Nevertheless, it emphasizes that, depending on what impression you got form the photo alone, you may now wish to revise your estimate of the person’s age. We suspect that many of you would have initially though he <strong>looks like he was in mid 50s</strong>, and so would have made guesses like those shown in red in the top panel. If you are one of those, then you will want to <strong>revise the age upwards</strong>, as we do in the green in the bottom panel.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-16"></span>
<img src="statbook_files/figure-html/unnamed-chunk-16-1.png" alt="If he looked like he was in mid 50s" width="864" />
<p class="caption">
Figure 3.9: If he looked like he was in mid 50s
</p>
</div>
<p>If you initially thought he looked like he was ‘around 60’ you would have made guesses like those shown in red in the top panel. If you did, then you will want to <strong>revise upwards a little</strong>, as is shown in green, and now put hime him somewhere around 60, or a bit above.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-17"></span>
<img src="statbook_files/figure-html/unnamed-chunk-17-1.png" alt="If If he looked like he was around 60." width="864" />
<p class="caption">
Figure 3.10: If If he looked like he was around 60.
</p>
</div>
<p>If you initially thought he looked like he was ‘in his mid 60s’ your estimate is more in line with the age-at-PhD data, and so you would not revise as much. You might bet a bit more on the ‘around 60’ age-bracket.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-18"></span>
<img src="statbook_files/figure-html/unnamed-chunk-18-1.png" alt="If If he looked like he was in his mid 60s." width="864" />
<p class="caption">
Figure 3.11: If If he looked like he was in his mid 60s.
</p>
</div>
<p>As we noted, the PhD data have too much of a right tail, and so it is driving up the estimates. If you are now curious as how keen your ‘age-estimation’ skills are, here is a link to the Google Scholar page of the <a href="https://scholar.google.com/citations?user=e8DARmEAAAAJ&amp;hl=en">statistician whose age we have been trying to determine</a>.</p>
<p>Age estimation via face images (image-based age estimation) is a growing research area, with many possible applications.</p>
<p>We now describe 2 more classical examples</p>
<p><strong>Example 2</strong></p>
<p>Spiegelhalter et al. address this in their Example 3.4: ‘Suppose we are interested in the <strong>long-term systolic blood pressure (SBP) in mmHg of a particular 60-year-old female</strong>.’</p>
<blockquote>
<p>We take two independent readings 6 weeks apart, and their mean is 130. We know that SBP is measured with a standard deviation <span class="math inline">\(\sigma = 5.\)</span> What should we estimate her SBP to be?</p>
</blockquote>
<p>They then go on to give the <strong>frequentist</strong> (‘standard’, ‘orthodox’) <strong>95% confidence interval</strong>, of <strong>123.1 to 136.9</strong>, <strong>centered on the measured value of 130</strong> [we will come later to how they calculated this]. They continue, …</p>
<blockquote>
<p><strong>However</strong>, we may have <strong>considerable additional information about SBPs which we can express as a prior distribution</strong>. Suppose that a survey in the <strong>same population</strong> revealed that females aged 60 had a mean long-term SBP of 120 with standard deviation 10. <strong>This population distribution can be considered as a prior distribution for the specific individual</strong>.</p>
</blockquote>
<p>The <strong>posterior distribution</strong>, computed from the combination of the 130 measured on the woman, and the prior, is <strong>centered on 128.9</strong> and the 95% interval is <strong>122.4 to 135.4</strong>.</p>
<blockquote>
<p>This posterior distribution reveals some <strong>‘shrinkage’ towards the population mean</strong>, and a small increase in precision from not using the data alone.<br />
<strong>Intuitively</strong>, we can say that the woman has somewhat higher measurements than we would expect for someone her age, and hence we slightly adjust our estimate to allow for the possibility that her two measureshappened by chance to be on the high side. As additional measures are made, this possibility becomes less plausible and the prior knowledge will be systematically downgraded.</p>
</blockquote>
<p>Before going on to example 3, we emphasize that the 128.9 is a <strong>compromize</strong> between the persnal mean of 130, and the (prior) poulation mean of 120: it is a weighted average, with (relative) weights that are the reciprocals of the squares of the 2 standard deviations, the reciprocals of <span class="math inline">\(\frac{5^2}{2}\)</span> and <span class="math inline">\(10^2,\)</span> i.e., <span class="math inline">\(\frac{2}{25} = 0.08\)</span> and <span class="math inline">\(\frac{1}{10^2} = 0.01.\)</span> The 128.9 is <strong>8/9ths closer to the measured 130</strong> than it is to the <strong>population mean of 120</strong>. It is slightly ‘shrunk’ toawards the population. This is why physicians might not believe that the 130 is correct, and might ask for more measurements before putting this woman on blood pressure-lowering drugs.</p>
<p>[The already cited ‘Bayesian integration in sensorimotor learning’ illustrates how as humans, we automatically <strong>combine estimates of different precisions</strong> into one more precise estimate, and do so using the same mathematical laws that are used in the Bayesian approach!]</p>
<p><strong>Example 3</strong></p>
<p>One article that does go into some detail about a similar situation is the very nice medically-useful – and didactic article <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/Irwig-1991-cholesterol.pdf">Estimating an Individual’s True Cholesterol Level and Response to Intervention</a> by Les Irwig, Paul Glasziou, Andrew Wilson and Petra Macaskill.</p>
<p>It begins with a single measurement, before dealing with an average of several measurements on the same person. It also gives separate charts for persons of different ages, and deals not just with point and interval estimates, but also derives probability statements for the possibility that the person’s true cholesterol is above some threshold that should trigger intervention. The appendix is a nice tutorial for combining information.</p>
<p>Their <em>abstract</em> begins:</p>
<blockquote>
<p>An individual’s blood cholesterol measurement may differ from the true level because of short-term biological and technical measurement variability. Using data on the within-individual and population variance of serum cholesterol, we addressed the following <strong>clinical concerns: Given a cholesterol measurement, what is the individual’s likely true level?</strong> The confidence interval for the true level is wide and asymmetrical around extreme measurements because of regres-sion to the mean. Of particular concern is the misclassification of people with a screening measurement below 5.2 mmol/L who may be advised that their cholesterol level is ‘desirable’ when their true level warrants further action.</p>
</blockquote>
<p>The first half of the paper, which deals with two related topics, (a) Estimating the True Cholesterol Level, and (b) assesing the Probability of Misclassification shows the primary elements, and these notes will focus on the highlights. [after these, extensive excerpts will be included]</p>
<p>The results for (a) and (b) were presented as 2 Figures. The first gave the (posterior) credible interval for a person’s true cholesterol level based on either 1, or an average of 3, measurements, using on the horizontal axis the measured value, and on the vertical one the point and 95% credible interval. Using a graph (rather than a formula) allows the clinician to use it for all possible ‘what if’s.</p>
<p>Below, we will <strong>illustrate</strong> it using one specific example, a <strong>person whose measured value was 7.15</strong>.</p>
<p>The second uses the (posterior) credible interval to calculate the <strong>probability that someone with a specific measured value has a true level that is above a certain threshold level used in treatment guidelines.</strong> Thus, the key tool is the posterior distribution itself, and so we give the statistical basis for this.</p>
<p><em>Reasons to take a Bayesian approach</em></p>
<p>The reason this problem arises in the first place is because of short term biological variability in the quantity of interest in the person in question. If we were measuring a person’s height, we could do so carefully at just one time-point: it would not be different a week or month from now; it remains quite stable over several years. [it does vary slightly over the day, but, be keep it simple, we could speak of one’s height at mid-day]. The same is not the case for a person’s cholesterol level: even if we measured it very carefully at one time, it would be genuinely different a week or month later, even in the absence of any intervention of lifestyle change. (The same applies, more strikingly, for other blood levels such as C-reactive protein (CRP), which is a marker of inflammation).</p>
<p>Thus any single measurement, or any average of a finite number of determinations, is imprecise. <strong>So what’s new?</strong> Don’t we meet this issue all the time in statistics?</p>
<p>The <strong>point of Irwig’s article</strong> is that we should <strong>not rely solely on the estimate based on the <em>person</em>’s measurements</strong>, but rather <strong>should combine it with an estimate based on <em>outside information</em>.</strong></p>
<p>The same reasoning is at work when a physician repeats a measurement that seems extreme. In so doing, (s)he is not relying only on the point or interval estimate provided by the measurement itself: rather (s)he is <strong>also using knowledge of how this measurement behaves in other similar persons</strong>! And what we know about others, even if collectively, can help us with an individual.</p>
<p>All of the technical details are available in these notes, prepared for <a href="">bios601</a> Here we will skip to the ‘botom line’ diagrams.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-19"></span>
<img src="statbook_files/figure-html/unnamed-chunk-19-1.png" alt="A Person's estimated true cholesterol level from one measurement for a population of individuals where the mean is 5.2 mmol/L (i.e, at the time of the article, men less than 35 and women less than 45 years old). The thicker solid line indicates the estimated true level, the  thinner solid lines are 80% confidence intervals. The dotted diagonal line is an equivalence line. The arrow shows the amount by which a single measurement of 7.15 mmol/L is shrunk towards the population mean, to a rcorrected value of 6.8 mmol/L." width="864" />
<p class="caption">
Figure 3.12: A Person’s estimated true cholesterol level from one measurement for a population of individuals where the mean is 5.2 mmol/L (i.e, at the time of the article, men less than 35 and women less than 45 years old). The thicker solid line indicates the estimated true level, the thinner solid lines are 80% confidence intervals. The dotted diagonal line is an equivalence line. The arrow shows the amount by which a single measurement of 7.15 mmol/L is shrunk towards the population mean, to a rcorrected value of 6.8 mmol/L.
</p>
</div>
<p>Below, we show the detailed calculations for a person with a single measurement of 7.15 mmol/L. For now, just focus on the items shown in red (the prior distribution for <span class="math inline">\(\theta\)</span>, blue (the 7.15 for the individual, and the associated likelihood function) and green (the posterior distribution of <span class="math inline">\(\theta\)</span>). All the calculations are on the log scale.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-20"></span>
<img src="statbook_files/figure-html/unnamed-chunk-20-1.png" alt="Worked example, for a person with a measured value of 7.15" width="864" />
<p class="caption">
Figure 3.13: Worked example, for a person with a measured value of 7.15
</p>
</div>
<p><strong>combine estimates of different precisions</strong> into one more precise estimate, and do so using the same mathematical laws that are used in the Bayesian approach!]</p>
<p><strong>Example 4</strong></p>
<p>We end with a strking example of the limitations of using one-at-a-time frequentist intervals, each in isolation from the next. It is based on the example in this <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios602/MultilevelData/EfronMorrisJASA1975.pdf">classic article</a>.</p>
<p>The task is to predict each player’s batting average over the remainder of the season using only the data from the first 4 weeks of the season. Since it proved difficult to replicate their selection croteria, we used instead the first 40 of <a href="http://mlb.mlb.com/stats/sortable.jsp#elem=%5Bobject+Object%5D&amp;tab_level=child&amp;click_text=Sortable+Player+hitting&amp;game_type=&#39;R&#39;&amp;season=2019&amp;season_type=ANY&amp;league_code=&#39;MLB&#39;&amp;sectionType=sp&amp;statType=hitting&amp;page=1&amp;ts=1586307216206&amp;playerType=ALL&amp;timeframe=&amp;last_x_days=&amp;split=&amp;sortColumn=ab&amp;sortOrder=&#39;desc&#39;&amp;extended=0">these players</a>.</p>
<p>The numbers of at bats per player up to mid April ranged from 44 to 73 (median 60 ). Efron and Morris used a constant 45. The numbers from mid April to the end of September ranged from 511 to 611 (median 542 ), or about 9 times as many as in the initial sample.</p>
<p>Clearly, the <strong>extreme results in these first 4 weeks were poor estimates of the perfornmances during the rest of the season</strong>. They <strong>‘shrunk’</strong> quite a bit.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-22"></span>
<img src="statbook_files/figure-html/unnamed-chunk-22-1.png" alt="Batting averages of selected Major League baseball players, 2019" width="864" />
<p class="caption">
Figure 3.14: Batting averages of selected Major League baseball players, 2019
</p>
</div>
<p>Across the performances <strong>up to mid April</strong>, the variance is <strong>a lot more than</strong> the variances in their <strong>‘real’</strong> (longer-term, remaining) performances. Thus, only about portion of it reflects the variance of the player-specific true (long-term) success rates. Will will quantify these later, below.</p>
<p>The individual binomial-based point estimates and frequentist confidence intervals we could construct (see next section) <strong>do not use the knowledge that individual season-long averages are confined to a narror band between say 0.225 and 0.325</strong>. Thus, many of them will not come close to or ‘trap’ the eventual individual longer-term batting averages. The correct way to make better frequentist intervals is to <strong>consider the family of players as an ensemble</strong>. If we label an randomly selected player as ‘<em>rs</em>’ say, then the long-run average <span class="math inline">\(\pi_{rs}\)</span> is thought of (modelled) as drawn from the distribution (family) of long-run averages. A slightly simplified (for display and didactic purposes) version of this is shown in this figure.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-25"></span>
<img src="statbook_files/figure-html/unnamed-chunk-25-1.png" alt="Simplified Distribution, **for learning purposes**, of Long-term Batting averages of Major League baseball players. Mean = 0.290; SD = 0.040." width="864" />
<p class="caption">
Figure 3.15: Simplified Distribution, <strong>for learning purposes</strong>, of Long-term Batting averages of Major League baseball players. Mean = 0.290; SD = 0.040.
</p>
</div>
<p>How then should we think of (model) <em>rs</em>’s short-term average? To make it concrete, suppose the number of at bats was <span class="math inline">\(n_{rs}\)</span> = 60, and that we observed 21/60 hits. A natural way is to view the 21/60 as the end-result of (of having come to us in) <strong>two stages</strong>, each involving a random draw <strong>(i)</strong> <span class="math inline">\(\pi_{rs}\)</span> was randomly drawn from the above distribution and then <strong>(ii)</strong> the number of hits was drawn from a binomial(<span class="math inline">\(n\)</span>=60) distribution with the probability/proportion parameter <span class="math inline">\(\pi_{rs}\)</span>. Symbolically, we would describe the <strong>provenance</strong> of the <span class="math inline">\(y\)</span> (= 21 for example) as <span class="math display">\[(i) \ \pi_{rs} \sim distribution \ in \ previous \ figure ; \ \ (ii) \ \ y | \pi_{rs} \sim Binomial(n=60, \pi = \pi_{rs}).\]</span></p>
<blockquote>
<p><strong>provenance</strong> : The fact of coming from some particular <strong>source</strong> or quarter; <strong>origin, derivation</strong>. The history of the ownership of a work of art or an antique, used as a guide to authenticity or quality; a documented record of this. A distinction is sometimes drawn between the ‘origin’ and the ‘provenance’ of an article, as in ‘A Canterbury origin is probable, Canterbury provenance is certain.’ Forestry. The geographic source of tree seed; the place of origin of a tree. Also: seed from a specific location. <strong>Oxford English Dictionary</strong></p>
</blockquote>
<p>The bottom part of this next figure shows the frequencies of the possible end-results (possible observed short term averages). The colours show their ‘provenance’. The top part shows the provenance of the ‘21/60’ short-term averages in particular. Note the direction of the arrow, showing the reverse-engineering, <strong>from what we observe</strong> (the 21/60) <strong>back to</strong> where (who) it could have arisen from. The use of reverse probabilities is at the heart of the Bayesian approach. In his worked example, when Bayes wrote about ‘the exact probability of all conclusions based on induction’, his <strong>conclusions</strong> referred to the <strong>origin</strong> (original location, unknown) of a billiard ball, and his data referred to the locations where it came to rest in a number of trials.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-26"></span>
<img src="statbook_files/figure-html/unnamed-chunk-26-1.png" alt="The bottom frequency distribution shows the expected frequencies of Short-term Batting averages from _n_ = 60 at bats by types of players shown in previous figure. For each possible short-term average, the colours of their components  show their 'provenance', i.e., which types of player this average arises from, and the relative contributions from each type. **As an example, highlighted, within the column enclosed by a dashed line,  is the provenance of the short-term averages that equal 21/60 = 0.350**. In a real application, after n=60, IF ALL ONE KNEW WAS THE 21/60, one would not know which type of player, i.e., which colour, one was observing. Nevertheless, the theoretical calculations show that a **short-term average of 21/60 = 0.350 is more likely to have been produced by a player who bats 0.320 or 0.300 or even lower, that one who bats 0.350 or or 0.360 or higher.** Much more of the probability distribution lies to the left of the 0.350 than to the right of it! So we need to 'shrink' the observed 0.350 towards the 0.290." width="864" />
<p class="caption">
Figure 3.16: The bottom frequency distribution shows the expected frequencies of Short-term Batting averages from <em>n</em> = 60 at bats by types of players shown in previous figure. For each possible short-term average, the colours of their components show their ‘provenance’, i.e., which types of player this average arises from, and the relative contributions from each type. <strong>As an example, highlighted, within the column enclosed by a dashed line, is the provenance of the short-term averages that equal 21/60 = 0.350</strong>. In a real application, after n=60, IF ALL ONE KNEW WAS THE 21/60, one would not know which type of player, i.e., which colour, one was observing. Nevertheless, the theoretical calculations show that a <strong>short-term average of 21/60 = 0.350 is more likely to have been produced by a player who bats 0.320 or 0.300 or even lower, that one who bats 0.350 or or 0.360 or higher.</strong> Much more of the probability distribution lies to the left of the 0.350 than to the right of it! So we need to ‘shrink’ the observed 0.350 towards the 0.290.
</p>
</div>
<p>Efron and Morris give a specific formula for merging the initial information (the ‘centre’ of 0.290) with the new information (the 21/60 = 0.350). the ‘shrunken’ estimate of the long-term average is a compromise, a weighted average, with the weights determined by the precision of the 21/60 (determined mainly by the 60) and the narrowness of the ‘parent’ distribution. We don’t need to get into these at this stage.</p>
<p>But, we will address a question some of you may already have concerning the batting average example, compared with the cholesterol example. In the cholesterol example, the ‘prior’ (or population) distribution of individuals’ true levels would be reasonably <strong>well known already</strong>. But in the baseball example, even though they had easy access to all of players’ previous years of data, Efron and Morris <strong>did not use any historical data</strong>. Instead, they use a ‘<strong>contemporaneous</strong>’ prior: they inferred what the location and spread of the distribution shown in the second of our three baseball figures, just from the March to Aril 15, 2019 numbers shown along the top of the first figure. They did this by subtraction. In the simplest case, if all the short-term averages were based on the same number (<span class="math inline">\(n\)</span>) of at bats (Efron and Morris used <span class="math inline">\(n\)</span> = 45), we have the following variance (V) formula<br />
<span class="math display">\[V[short.term.averages] =  V[long.term.averages] +  \  V[binomial.proportion \ based \ on \ n.]\]</span> They got to measure the left hand quantity empirically from their data on April 26 / May 3, 1970, and they knew from mathematical statistics that the rightmost quantity is of the order of <span class="math inline">\(0.3 \times 0.7 \  / \ 45\)</span> = 0.0047. Thus they could deduce the spread of the long-term averages.</p>
<p>In <em>our</em> 2019 data, the numbers of early-season at bats vary from player to player, but we will simplify the math by taking a typical <span class="math inline">\(n\)</span>. Across the performances <strong>up to mid April</strong>, the (rounded) variance of the individual batting averages, (i.e., the left-hand <span class="math inline">\(V\)</span> in the above equation) is 0.004640. The binomial variation that you would see with <span class="math inline">\(n\)</span>’s of the order of 60 is approximately 0.003570. Player-specific true proportions vary from maybe 0.225 to 0.325. [for example, at <span class="math inline">\(\pi\)</span> =0.30 and <span class="math inline">\(n\)</span> = 60, the binomial variance is 0.30 * 0.70 / 60 = 0.003500.] Thus, by solving the equation <span class="math display">\[0.004640 =  V[long.term.averages] +  \  0.003500\]</span></p>
<p>we can back-calculate that the variance of the player-specific true (long-term) success rates is only about 0.001070. [The square root of this, sd = 0.033, broadly fits with the spead we see in the performances during the remainder of the season: the averages range from 0.224 to 0.326.]</p>
<p>To <strong>distinguish it from the typical Bayes</strong> approach, this <strong>double use of the data</strong> is sometime referred to as an ‘<strong>Empirical Bayes</strong>’ approach.</p>
<blockquote>
<p>Empirical Bayes methods are procedures for statistical inference in which the prior distribution is estimated from the data. This approach stands in contrast to <strong>standard</strong> Bayesian methods, for which the <strong>prior distribution is fixed before any data are observed</strong>. Despite this difference in perspective, empirical Bayes may be viewed as an approximation to a fully Bayesian treatment of a hierarchical model wherein the parameters at the highest level of the hierarchy are set to their most likely values, instead of being integrated out. Empirical Bayes, also known as maximum marginal likelihood, represents one approach for setting hyperparameters. Wikipedia, 2020.04.15</p>
</blockquote>
</div>
<div id="the-bayesian-bottom-line" class="section level3">
<h3><span class="header-section-number">3.1.4</span> The Bayesian Bottom Line</h3>
<ul>
<li><p>As the examples all show, the Bayesian approach allows for direct-speak about parameters, without any of the ‘legalese’ we will have to use when using the frequentist approach.</p></li>
<li>Because the end-product is a probability distribution for the parameter (<span class="math inline">\(\theta\)</span> say), we can directly talk about the (pre-) and post-data <strong>probability that</strong>, say,</li>
<li><span class="math inline">\(\theta &gt; 0,\)</span> or (for any prespecified values <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>)</li>
<li><span class="math inline">\(A &lt; \theta &lt; B\)</span>, or</li>
<li><span class="math inline">\(\theta &gt; A\)</span> or</li>
<li><span class="math inline">\(\theta &lt; B\)</span>, or, as in the case of hemophilia carriage,</li>
<li><p><span class="math inline">\(\theta = Yes\)</span> or <span class="math inline">\(\theta = No.\)</span></p></li>
<li>If <span class="math inline">\(\theta\)</span> = batting average for the remainder of season,</li>
<li><p>Prob(a 21/60 player will have <span class="math inline">\(\theta \ge\)</span> .350) = 0.11 [see Fig]</p></li>
<li>Likewise, if we fix a desired probability <span class="math inline">\(P\)</span>, we can calculate <span class="math inline">\(\theta\)</span> values <span class="math inline">\(\theta_{lower}\)</span> and <span class="math inline">\(\theta_{upper}\)</span> such that</li>
<li>Prob( <span class="math inline">\(\theta &gt; \theta_{lower}) = P\)</span>, or</li>
<li>Prob( <span class="math inline">\(\theta &lt; \theta_{upper}) = P\)</span>, or</li>
<li><p>Prob( <span class="math inline">\(\theta_{lower} &lt; \theta &lt; \theta_{upper}) = P\)</span>.</p></li>
<li>Again, if <span class="math inline">\(\theta\)</span> = batting average for the remainder of season, then, in the case of a 21/60 player,</li>
<li><p>Prob(0.245 &lt; <span class="math inline">\(\theta\)</span> &lt; 0.377) = 0.95.</p></li>
<li><strong>Contrast this with the 95% frequentist confidence interval</strong>, based <strong>only on the 21/60</strong>, and <strong>not on the knowledge that</strong> (historically, or even by back-calculation just from the early 2019 data) <strong>most players’ batting averages for a year are between 0.225 and 0.325</strong>,</li>
<li><p><span class="math inline">\(\theta_{lower}\)</span> = 0.231 ; <span class="math inline">\(\theta_{upper}\)</span> = 0.484) = 0.95.</p></li>
</ul>
<p>Can this shrinkage be done without ‘priors’?</p>
</div>
</div>
<div id="frequentist-approach" class="section level2">
<h2><span class="header-section-number">3.2</span> Frequentist approach</h2>
<p>In their chapter 11, Null hypotheses and p-values, Clayton and Hills tell us</p>
<blockquote>
<p>With most probability models there is one particular value of the parameter which corresponds to there being <em>no effect</em>. This value is called the <em>null</em> value, or <em>null hypothesis</em>. For a parameter <span class="math inline">\(\theta\)</span> we will denote this null value by <span class="math inline">\(\theta_0\)</span>. In classical statistical theory, considerable emphasis is placed on the need to disprove (or reject) the null hypothesis before claiming positive findings, and the procedures which are used to this end are called <em>statistical significance tests</em>. However, the emphasis in this theory on accepting or rejecting null hypotheses has led to widespread misunderstanding and misreporting in the medical research literature. In epidemiology, which is not an experimental science, the usefulness of the idea has been particularly questioned. Undoubtedly the idea of statistical significance testing has been overused, at the expense of the more useful procedures for <em>estimation</em> of parameters which we have discussed in previous chapters. However, it remains useful. A null hypotheses is a <em>simplifying</em> hypothesis and measuring the extent to which the data are in conflict with it remains a valuable part of scientific reasoning. In recent years there has been a trend away from a making a straight choice between accepting or rejecting the null hypothesis. Instead, the <em>degree</em> of support for the null hypothesis is measured, for example using the log likelihood ratio at the null value of the parameter. Clayton and Hills, p96. [Their illustration involved an investigation of the possible linkage of the HLA locus to nasopharyngeal cancer susceptibility]</p>
</blockquote>
<p>In Chapter 2.4, Classical hypothesis testing, Gelman and Hill tell us</p>
<blockquote>
<p>The possible outcomes of a hypothesis test are ‘reject’ or ‘not reject.’ It is never possible to ‘accept’ a statistical hypothesis, only to find that the data are not sufficient to reject it.<br />
.<br />
Comparisons of parameters to fixed values and each other: interpreting confidence intervals as hypothesis tests<br />
.<br />
The hypothesis that a parameter equals zero (or any other fixed value) is directly tested by fitting the model that includes the parameter in question and examining its 95% interval. If the interval excludes zero (or the specified fixed value), then the hypothesis is rejected at the 5% level.<br />
Testing whether two parameters are equal is equivalent to testing whether their difference equals zero. We do this by including both parameters in the model and then examining the 95% interval for their difference. As with inference for a single parameter, the confidence interval is commonly of more interest than the hypothesis test. For example, if support for the death penalty has decreased by 6% <span class="math inline">\(\pm\)</span> 2.1%, then the magnitude of this estimated difference is probably as important as that the change is statistically significantly different from zero. The hypothesis of whether a parameter is positive is directly assessed via its confidence interval. If both ends of the 95% confidence interval exceed zero, then we are at least 95% sure (under the assumptions of the model) that the parameter is positive. Testing whether one parameter is greater than the other is equivalent to examining the confidence interval for their difference and testing for whether it is entirely positive.</p>
</blockquote>
<p>In section 2.5, Problems with statistical significance, they warn that</p>
<blockquote>
<p>A common statistical error is to summarize comparisons by statistical significance and to draw a sharp distinction between significant and nonsignificant results. The approach of summarizing by statistical significance has two pitfalls, one that is obvious and one that is less well known. First, statistical significance does not equal practical significance. For example, if the estimated predictive effect of height on earnings were $10 per inch with a standard error of $2, this would be statistically but not practically significant. Conversely, an estimate of $10,000 per inch with a standard error of $10,000 would not be statistically significant, but it has the possibility of being practically significant (and also the possibility of being zero; that is what “not statistically significant” means).<br />
.<br />
The second problem is that changes in statistical significance are not themselves significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary — for example, only a small change is required to move an estimate from a 5.1% significance level to 4.9%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying variables.</p>
</blockquote>
<p>They illustrate the application of basic statistical methods with a story, in which, a couple of years before, they received a fax, entitled HELP!, from a member of a residential organization:</p>
<blockquote>
<p>Last week we had an election for the Board of Directors. Many residents believe, as I do, that the election was rigged and what was supposed to be votes being cast by 5,553 of the 15,372 voting households is instead a fixed vote with fixed percentages being assigned to each and every candidate making it impossible to participate in an honest election.</p>
</blockquote>
<p>Gelman and Hill devote 3 pages to their analysis, which led them to</p>
<blockquote>
<p>conclude that the intermediate vote tallies are consistent with [the null hypothesis of] random voting. As we explained to the writer of the fax, opinion polls of 1000 people are typically accurate to within 2%, and so, if voters really are arriving at random, it makes sense that batches of 1000 votes are highly stable. This does not rule out the possibility of fraud, but it shows that this aspect of the voting is consistent with the null hypothesis.</p>
</blockquote>
<div id="frequentist-test-of-a-null-hypothesis" class="section level3">
<h3><span class="header-section-number">3.2.1</span> (Frequentist) Test of a Null Hypothesis</h3>
<p>Use: To assess the evidence provided by sample data in favour of a pre-specified claim or ‘hypothesis’ concerning some parameter(s) or data-generating process. As with confidence intervals, tests of significance make use of the concept of a sampling distribution.</p>
<p><strong>Example 1</strong></p>
<p>The <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/c607/ch06/lady_tasting_tea.pdf">Mathematics of a Lady Tasting Tea.</a> Whether the sensory experiment was actually carried out has been the subject of subsequent <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2012.00620.x">articles</a>. In 2002, the phrase ‘Lady Tasting Tea’:’ was used as the title, and How Statistics Revolutionized Science in the Twentieth Century the subtitle of <a href="https://www.amazon.ca/Lady-Tasting-Tea-Statistics-Revolutionized/dp/0805071342">this book</a> which you can borrow from the <a href="https://mcgill.overdrive.com/media/E974A53B-3700-474A-9E11-9E8758435E70">McGill Librray</a>. Here is a link to an online copy of <a href="https://www.phil.vt.edu/dmayo/PhilStatistics/b%20Fisher%20design%20of%20experiments.pdf">Fisher’s 1935 book Design of Experiments</a></p>
<p><img src="statbook_files/figure-html/unnamed-chunk-28-1.png" width="864" style="display: block; margin: auto;" /></p>
<p><img src="statbook_files/figure-html/unnamed-chunk-29-1.png" width="864" style="display: block; margin: auto;" /></p>
<p><img src="statbook_files/figure-html/unnamed-chunk-30-1.png" width="864" style="display: block; margin: auto;" /></p>
<p><strong>Example 2</strong></p>
<blockquote>
<p>In 1949 a divorce case was heard in which the sole evidence of adultery was that a baby was born almost 50 weeks after the husband had gone abroad on military service. [Preston-Jones vs. Preston-Jones, English House of Lords]. To quote the court “The appeal judges agreed that the limit of credibility had to be drawn somewhere, but on medical evidence 349 (days) while improbable, was scientifically possible.” So the appeal failed. Altman, BMJ 1 November 1980.</p>
</blockquote>
<blockquote>
<p>In Preston-Jones v Preston Jones [1951] 1 All ER 124, the husband was away from the UK from August 17, 1945 to September 2, 1946. On August 13, 1946 the wife gave birth to a normal child and the husband brought a petition for dissolution of marriage on the ground of adultery, the allegation being based on the fact that a period of 360 days elapsed and therefore that the child must have been conceived in adultery. It was held that the court was not entitled to assume judicial knowledge that a child born 360 days after the last coitus between the husband and wife was not the child of marriage. See also Gaskill v Gaskill [1921] page 425 where 331 days could not be regarded as impossible and Hadlum v Hadlum [1948] 2 All ER 412 where in the absence of medical evidence the same conclusion was reached in respect of 348 days. If an interval of 360 days between coitus and birth could not be regarded as a remote possibility, then a fortiori a gestation period of 270-280 days suggested by the appellant could not be said to be abnormal and so the judge could not have been entitled to assume as was urged that Njogu was not the child of the appellant. <a href="http://kenyalaw.org/caselaw/cases/view/8385" class="uri">http://kenyalaw.org/caselaw/cases/view/8385</a></p>
</blockquote>
<p>The following is from this <a href="http://www.biostat.mcgill.ca/hanley/statbook/Guttmacher.pdf">‘bible’</a> for those who became parents in the 1970s</p>
<blockquote>
<p>What Are the Chances of Delivering on Time?</p>
</blockquote>
<blockquote>
<p>In over 17,000 cases of pregnancy carried beyond the twenty seventh week, 54 per cent delivered before 280 days, 4 percent had their babies either the week before or the week after the calculated date, and 74 per cent within a two-week period before or after the anticipated day of birth.<br />
On the basis of these data one can calculate the likelihood which the average woman faces when carrying a single infant, not twins, of having her baby, during each week after the twenty-seventh week from the first day of her last menstrual period.</p>
</blockquote>
<p>The author then presents a <a href="http://www.biostat.mcgill.ca/hanley/statbook/Guttmacher.pdf#page=7">frequency distribution</a> in 19 rows and 3 columns, with the headers <em>Weeks</em> (27, 28, … 45, 46+), <em>Days</em> (189-196, 196-203, … 308-315, 315+) and <em>Approximate Chance</em> (1:625, 1:625, 1:525, … 1:140, 1:140). We have converted it to a graphic.</p>
<p><img src="statbook_files/figure-html/unnamed-chunk-31-1.png" width="864" style="display: block; margin: auto;" /> The author does on to say that</p>
<blockquote>
<p>Another reliable study has shown that 40 per cent of women go into labor within a ten-day period — five days before and five days after the calculated date, and nearly two thirds within plus or minus ten days of the expected time.</p>
</blockquote>
<p>Altman, quoted above, gives a graph based on <a href="http://www.biostat.mcgill.ca/hanley/statbook/Altman1980.pdf#page=2">1970 data from England</a>.</p>
<p>In that (pre-ultrasound) era, some of the variation was dues to dating errors. But a 2013 article, based on carefully collected daily urine samples found that even with the precise measuremet of the timing of pregranct that these samples provides the <a href="https://www.sciencedaily.com/releases/2013/08/130806203327.htm">length of human pregnancies can vary naturally by as much as five weeks</a></p>
<blockquote>
<p>Normally, women are given a date for the likely delivery of their baby that is calculated as 280 days after the onset of their last menstrual period. Yet only four percent of women deliver at 280 days and only 70% deliver within 10 days of their estimated due date, even when the date is calculated with the help of ultrasound.<br />
Now, for the first time, researchers in the USA have been able to pinpoint the precise point at which a woman ovulates and a fertilised embryo implants in the womb during a naturally conceived pregnancy, and follow the pregnancy through to delivery. Using this information, they have been able to calculate the length of 125 pregnancies.<br />
“We found that the average time from ovulation to birth was 268 days – 38 weeks and two days,” said Dr Anne Marie Jukic, a postdoctoral fellow in the Epidemiology Branch at the National Institute of Environmental Health Sciences (Durham, USA), part of the National Institutes for Health. “However, even after we had excluded six pre-term births, we found that the length of the pregnancies varied by as much as 37 days.<br />
”We were a bit surprised by this finding. We know that length of gestation varies among women, but some part of that variation has always been attributed to errors in the assignment of gestational age. Our measure of length of gestation does not include these sources of error, and yet there is still five weeks of variability. It’s fascinating.&quot;<br />
The possibility that the length of pregnancies can vary naturally has been little researched, as it is impossible to tell the difference between errors in calculations and natural variability without being able to measure correctly the gestational age of a developing fetus. Previous studies conducted as long ago as the 1970s and 1980s had used the slight rise in a woman’s body temperature at waking as a way of detecting when ovulation occurred. This is an inexact measurement and cannot be used to detect when the embryo actually implants in the womb. Science Daily.</p>
</blockquote>
<p>But even if the variation is still (naturally) large, the ruling in the Preston-Jones case seems extreme. Altman asks us to look at the distribution of length of gestation in his Fig 1, ‘which the judges apparently did not do.’ Altman himself thought that</p>
<blockquote>
<p>most people would feel that the husband was hard done by. Even If this case were heard now, where would you draw the line on the basis of (Altman’s) fig 1 ?<br />
This case illustrates a failure to use statistical methods when they ought to have been used, a fairly common occurrence. Saying that an event is possible is quite different from saying that it has a probability of, say, one in 100,000. Although not an example from medical research, this case concerne essentially the same difficulty as in many more frequentl encountered problems, such as denning hypertension or obesity Everything varies; it is in trying to draw lines between good and bad, high and low, likely and unlikely, and so on, that many problems arise. Although statistics cannot answer a given question, they can often shed considerable light on the problem. Altman, 1980.</p>
</blockquote>
<p>Today, JH would add that we <strong>should <em>also</em> consider any other evidence or testimony presented in the case</strong>. (We will come back to this when discussing the prosecutor’s fallacy.)</p>
<p><strong>Other Examples</strong>:</p>
<ul>
<li>Quality Control (it has given us the terminology)</li>
<li>Taste-tests (see exercises )</li>
<li>Jumping the gun in athletics</li>
<li>Adding water to milk..seeM&amp;M2Example6.6p448</li>
<li>Water divining..seeM&amp;M2exercise6.44p471</li>
<li>Randomness of U.S. Draft Lottery of 1970.. see M&amp;M2 Example 6.6 p105-107, and 447-</li>
<li>Births in New York City after the “Great Blackout”</li>
<li>John Arbuthnot’s “argument for divine providence”</li>
<li>US Presidential elections: Taller vs. Shorter Candidate.</li>
<li>Gelman and Hill, HELP. 50,000 votes</li>
</ul>
<p>AM HERE</p>
</div>
<div id="the-ingredients-and-the-methods-of-procedure-in-a-statistical-test" class="section level3">
<h3><span class="header-section-number">3.2.2</span> The ingredients and the methods of procedure in a statistical test</h3>
<ol style="list-style-type: decimal">
<li>A claim about a parameter (or about the shape of a distribution, or the way a lottery works, etc.). Note that the null and alternative hypotheses are usually stated using Greek letters, i.e. in terms of population parameters, and in advance of (and indeed without any regard for) sample data. [ Some have been known to write hypotheses of the form H: y– = … , thereby ignoring the fact that the whole point of statistical inference is to say something about the population in general, and not about the sample one happens to study. It is worth remembering that statistical inference is about the individuals one DID NOT study, not about the ones one did. This point is brought out in the absurdity of a null hypothesis that states that in a triangle taste test, exactly p=0.333.. of the n = 10 individuals to be studied will correctly identify the one of the three test items that is different from the two others.]</li>
<li>A probability model (in its simplest form, a set of assumptions) which allows one to predict how a relevant statistic from a sample of data might be expected to behave under H0.</li>
<li><p>A probability level or threshold or dividing point below which (i.e. close to a probability of zero) one considers that an event with this low probability ‘is unlikely’ or ‘is not supposed to happen with a single trial’ or ‘just doesn’t happen’. This pre- established limit of extreme-ness is referred to as the “α (alpha) level” of the test.</p></li>
<li>A sample of data, which under H0 is expected to follow the probability laws in (2).</li>
<li>The most relevant statistic (e.g. y- if interested in inference about the parameter μ)</li>
<li>The probability of observing a value of the statistic as extreme or more extreme (relative to that hypothesized under H0) than we observed. This is used to judge whether the value obtained is either ‘close to’ i.e. ‘compatible with’ or ‘far away from’ i.e. ‘incompatible with’, H0. The ‘distance from what is expected under H0’ is usually measured as a tail area or probability and is referred to as the “P-value” of the statistic in relation to H0.</li>
<li><p>A comparison of this “extreme-ness” or “unusualness” or “amount of evidence against H0” or P-value with the agreed-on “threshold of extreme-ness”. If it is beyond the limit, H0 is said to be “rejected”. If it is not-too-small, H0 is “not rejected”. These two possible decisions about the claim are reported as “the nullhypothesisisrejectedattheP=α significancelevel”or“the null hypothesis is not rejected at a significance level of 5%”.</p></li>
</ol>
<p>nificance levels can correspond to small, nonsignificant changes in the underlying variables. 2 separate issues Ho and intervals (treated together in Bayesian)</p>
<p>• Statistical Quality Control procedures [for Decisions] • Sample survey organizations: Confidence intervals • Statistical Tests of Hypotheses Unlike Bayesian inference, there is no quantified pre-test or pre- data “impression”; the ultimate statements are about data, conditional on an assumed null or other hypothesis. Thus, an explanation of a p-value must start with the conditional “IF the parameter is … the probability that the data woul</p>
</div>
</div>
<div id="does-the-approach-matter" class="section level2">
<h2><span class="header-section-number">3.3</span> Does the approach matter?</h2>
<p>semantics</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="paras.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="CI.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02a-inference.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["statbook.pdf", "statbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
