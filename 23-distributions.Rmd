# Distributions {#distributions}


## Objectives

Distributions of individual values take their shapes and spreads from the features of the setting, and thus do not follow any general laws. The shapes and the spreads of distributions of statistical summaries and parameter estimates made from aggregates of individual observations tend to be more regular and more predictable, and thus more law-abiding.  

So, the **specific objectives** in this chapter are to truly understand

* the distinction between a natural and  investigator-made distributions, and between  observable and  conceptual ones.

* why we should  automatically associate certain distributions with certain types of random variables

* why we need to understand the pre-requisites for random variables following the distributions they do

* why we rely so much on the Normal distribution, and why it  is so 'Central' to statistical inference concerning parameters.

* why the pre-occupation with  checking 'Normality' (Gaussian-ness) is misguided

* why Normality is not even relevant when the 'variable' is not 'random', and appears on the right hand side of a regression model.

* the few contexts where shape does matter

## Named Distributions

In historical order

Bernoulli, and scaled Bernoulli

Binomial

Normal

beta

Poisson

t

Wilcoxon

=======

To get a full list of the named distributions available in `R` you can use the following command: `help(Distributions)`


### Bernoulli

The simplest random variable is one that take just 2 possible values, such as  YES/NO, MALE/FEMALE, 0/1, ON/OFF, POSITIVE/NEGATIVE, PRESENT/ABSENT, EXISTS/DOES NOT, etc.

This random variable $Y$ is governed by just one parameter, namely the probability, $\pi$, that it takes on the YES (or '1') value. Of course you can reverse the scale, and speak about the probability of a NO (or '0') result.

It is too bad that when [Wikipedia](https://en.wikipedia.org/wiki/Bernoulli_distribution), which has a unified way of showing the main features of statistical distributions, does not follow its own principles and show a graph of various versions of a Bernoulli distribution. So here is such a graph. 


```{r,eval=T, echo=F, fig.align="center", fig.height=2, fig.width=9, warning=FALSE, message=F,fig.cap="Various Bernoulli random variables/distributions. We continue our convention of using the letter Y (instead of X) as the generic name for a random variable. Moreover, in keeping with this view, all of the selected Bernoulli distributions are plotted with their 2 possible values shown on the vertical axis."}

par(mfrow=c(1,1),mar = c(0,0,1,0))

plot(-10,-10,xlim=c(1,17),ylim=c(-0.1,1.1))

Dx = 1.75
for(Pos in (1:9)){
   
   segments(Pos*Dx,0,Pos*Dx,1)
   prob = Pos/10
   arrows(Pos*Dx-Dx/6, prob, Pos*Dx, prob,
          length=0.06, angle=25)
   if(Pos==1){ 
      text(Pos*Dx, 0.5, "Y", adj=c(1.5,0.5))
      text(Pos*Dx, 1, "1", adj=c(1.25,0.5))
      text(Pos*Dx, 0, "0", adj=c(1.25,0.5))
      }
   segments(Pos*Dx,1,Pos*Dx+ prob,1,lwd=1.5)
   segments(Pos*Dx,0,Pos*Dx+ (1-prob),0)
   text(Pos*Dx + prob, 1, 
                       toString(prob), 
                       adj=c(-0.25,0.5),
                       cex=0.65,font=2)
   text(Pos*Dx + 1-prob, 0, 
                       toString(1-prob), 
                       adj=c(-0.25,0.5),
                       cex=0.65)
}

```

Please, when reading the Wikipedia entry, replace all instances of $X$ and $x$ by $Y$ and $y$. Note also that we will use $\pi$ where Wikipedia, and some textbooks, use $p$. As much as we can, we use Greek letters for parameters and Roman letters for their empirical (sample) counterparts. Also, to be  consistent, if the random variable itslef is called $Y$, then it makes sense to use $y$ as the possible relaizations of it, rather than the illogical $k$ that Wikipedia uses.]

In the sidebar, Wikipedia shows the probability mass function (_pmf_, the probabilities that go with the possible $Y$ values) in two separate rows, but in the text the _pmf_ is also shown more concisely, as (in our notation)

$$f(y) = \pi^y (1-\pi)^{1-y}, \ \ y = 0, 1.$$
If we wish to align with how the `R` software names features of distributions, we might want to switch from $f$ to $d$. `R` uses $d$ because it harmonizes with the probability $d$ensity function (_pdf_) notation that its uses for random variables on an interval scale, even though some  statistical 'purists' see that as mixing terminology: they use the term probablility mass function for discrete random variables, and probablity density function for ones on an interval scale.

$$d_{Bernoulli}(y) = \pi^y (1-\pi)^{1-y}.$$

Sadly, Bernoulli does not get its own entry in `R`'s list of named distributions, presumably because it is a special case of a binomial distribution, one where $n$ = 1. 
So we have to call `dbinom(x,size,prob)` to get the
density (probability mass) function of the binomial distribution with parameters `size` ($n$) and `prob` ($\pi$), and set $n$ to 1. 

The 3 arguments to  `dbinom(x,size,prob)` are:

* `x`: a vector of quantiles (here just 0 or/and 1),
* `size`: the number of 'trials' (our '$n$', so 1 for Bernoulli),
and 
* `prob`: the probability of 'success' on each 'trial'. We think of it as the probability that a realization of $Y4, i.e, $y$ will equal 1, or as $\pi.$)

Thus, `dbinom(x=0,size=1,prob=0.3)` yields `r dbinom(x=0,size=1,prob=0.3)`, while `dbinom(x=1,size=1,prob=0.3)` yields `r dbinom(x=1,size=1,prob=0.3)` and `dbinom(x=c(0,1),size=1,prob=0.3)` yields the vector `r dbinom(x=c(0,1),size=1,prob=0.3)`.

Incidentally, please do not adopt the convention that $x$ (or our $y$) is the number of ‘successes’ in $n$ trials. It is the number of 'positives' in a sample of $n$ independent draws from a population in which a proportion $\pi$ are positive.

**_Expectation (E) and Variance (V)_ of a Bernoulli random variable.**

Shortening $Prob(Y=y)$ to $P_y$, we have

* From first principles, 
 $$E[Y] = 0 \times P_0 + 1 \times P_1 = 0 \times (1-\pi) + 1 \times \pi = \pi,$$
while
 $$V[Y] = (0-\pi)^2 \times P_0 + (1-\pi)^2 \times P_1  = \pi^2(1-\pi) + (1-\pi)^2\pi =  \underline{\pi(1-\pi)}.$$

This functional form for the ('unit') variance is not entirely surprising: it is obvious from the selected distributions whon that the most concentrated Bernoulli distributions are the ones where the proportion $(\pi)$ of Y = 1 values is either close to 1 or to zero, and that the most spread out Bernoulli distributions are the ones where $\pi$ is close to 1/2. And, and a function of $\pi$, the Variance must be symmetric about $\pi$ = 1/2.   

The fact that the greatest uncertainty ('entropy', lack of order or predictability) is when $\pi$ = 1/2 is one of the factors that makes sports contests more engaging when teams or players  are well matched. Later, when we come to study what influences the imprecision of sample surveys, we will see that for a given sample size, the imprecision is largest when  $\pi$ is closer to 1/2.  

**Why focus on the variance of a Bernoulli random variable?** because, later, when we use the more intesting binomial distribution, we can call on first prionciples to recall what its expection and variance are. A Binomial random variable is the sum of $n$ independently distributed Bernoulli random variables, all with the same expectation $\pi$ and unit variance $\pi(1-\pi).$ Thus its expectation ($E$) and variance  ($V$) are the sums of these 'unit' versions, i.e., $E[binom.sum] = n \times \pi$ and $V[binom.sum] =  n \times \pi(1-\pi).$ Moreover, again from first principles, we can deduce that if instead of a sample _sum_, we are interested in a sample _mean_ (here the _mean_ of the 0's and 1's is the sample _proportion_), its expected value is 
$$\boxed{\boxed{E[binom.prop'n] = \frac{n \pi}{n} = \pi; \   V[.] = \frac{n  \pi(1-\pi))}{n^2} = \frac{\pi(1-\pi)}{n}; \ SD[.] = \frac{\sqrt{\pi(1-\pi)}}{ \sqrt{n}} = \frac{\sigma_{0,1}}{\sqrt{n}} } }  $$  

Note here the generic way we write the SD of the sampling distribution of a sample proportion, in the same way that we write the SD of the sampling distribution of a sample mean, as $\sigma_u/\sqrt{n},$ where $\sigma_u$ is the 'unit' SD, the standard deviation of the  values of _individuals_. The individual values in the case of a Bernoulli randomn variable are just 0s and 1s, and their SD is $\sqrt{\pi(1-\pi)}.$ We call this SD the SD of the 0'1 and 1's, or $\sigma_{0,1}$ for short.

Notice how, even though it might look nicer and simpler to compute, and involves just 1 square root calculation, we did not write the SD of a binomial proportion as  
$$SD[binom.proportion] = \sqrt{\frac{\pi(1-\pi)}{ n} }.$$ 
We choose instead to use the $\sigma/\sqrt{n}$ version, to show that it has the same _form_ as the SD for the sampling distribution of a sample mean. Now that we no longer need to  savw keystrokes on a hand caloculator, we should move away from  computational forms and focus instead on the intuitive form.  Sadly, many textbooks re-use the same concept in disjoint chapters without telling readers they are cases of the same SD formula. 

There is a lot to be gained by thinking of proportions as means, but where the $Y$ values are just 0's and 1's. You can use the  `R code` below to simulate a very large number of 0's and 1's, and calculate their variance. The  `sd` function in `R` doesn't know or care that the values you supply it are limited to just 0s and 1s, or spread along an interval. Better still don't use the `rbinom` function; instead use the `sample` function, with replacement.



```{r,eval=T, echo=T, fig.align="center", fig.height=2, fig.width=9, warning=FALSE, message=F,fig.cap="Various Bernoulli random variables/distributions. We continue our convention of using the letter Y (instead of X) as the generic name for a random variable. Moreover, in keeping with this view, all of the selected Bernoulli distributions are plotted with their 2 possible values shown on the vertical axis."}

n = 750

zeros.and.ones = sample(0:1, n , 
   prob=c(0.2, 0.8),replace=TRUE )

m = matrix(zeros.and.ones,n/75,75)
noquote(apply(m,1,paste,collapse=""))

sum(zeros.and.ones)/n
round( sd(zeros.and.ones),4)

```

Try the above code with a larger $n$ and a different $\pi$ and convince yourself that the variance (and thus the SD) of the individual 0 and 1 values (a) have nothing to do with how many there are and everything to do with what proportion of them are of each type and (b)  are larger when the proportions are close to each other, and smaller when they are not. 

**Scaled-Bernoulli random variables**

**Could we get by without studying the Binomial Distribution?** The answer is 'for most applications, yes.' The reason is that in in most cases, we are able to use a Gaussian (Normal) approximation to the binomial distribution. Thus, all we need are its expectation and variance (standard deviation):  we don't need the `dbinom()` probability mass function, or the `pbinom()` that gives the cumulative distribution function and thus the tail areas, or the `qbinom()` function that gives the quantiles. But sometimes we deal with situations where the binomial distributions are not symmetric and close-enough-to-Gaussian.

Below we recount how, in 1738, almost 4 decades before Gauss was born, when summing the probabilities of a binomial distribution with a large $n$, [deMoivre](https://en.wikipedia.org/wiki/De_Moivre–Laplace_theorem) effectively used the as-yet unrecognized 'Gaussian' distribution as a very accurate approximation. Without calling it this, he relied on the standard deviation of the binomial distribution. 


===

### Binomial

**The Binomial Distribution is a  model for the (sampling) variability of a proportion or count in a randomly selected sample**

**The Binomial Distribution: what it is**

* The $n+1$ probabilities $p_{0}, p_{1}, ..., p_{y}, ..., p_{n}$ of observing $y$ = $0, 1, 2, \dots , n$ 'positives'  in $n$ independent realizations of a Bernoulli random variable $Y$with probability, $\pi,$ that Y=1, and (1-$\pi$) that it is 0. The number is the sum of $n$ independen Bernoulli random variables with the same probability, such as in s.r.s of $n$ individuals.
    
* Each of the $n$ observed elements is binary (0 or 1)

* There are $2^{n}$ possible _sequences_ ... but only $n+1$ possible _values_, i.e. $0/n,\;1/n,\;\dots ,\;n/n$  can think of $y$ as sum of $n$ Bernoulli r. v.'s. [Later on, in ptractive, we will work in the same scale as  parameter. i.e., (0,1). not the  (0,n) 'count' scale.]
        
* Apart from  $n$, the probabilities $p_{0}$ to $p_{n}$ depend on only 1 parameter:
   +  the probability that a selected individual will be 'positive'  i.e., have the trait of interest
   + the proportion of 'positive' individuals in the sampled population

*  Usually we denote this (un-knowable) proportion by $\pi$ (or sometimes by the more generic $\theta$) 
   + Textbooks are not consistent (see below); we try to use Greek letters for parameters,
   + Note Miettinen's  use of UPPER-CASE letters, [e.g. $P$, $M$] for _PARAMETERS_ and lower-case letters [e.g., $p$, $m$] for _statistics_ (_estimates} of parameters_).


|   Author(s)          | PARAMETER     | Statistic    | 
|:------------------|   :----:      |   :----:     |  
|Clayton and Hills  |   $\pi$       |  $p = D/n$    |  
|Moore and McCabe, Baldi and Moore  |   $p$    |  $\hat{p} = y/n$    |
|Miettinen          |   $P$         |  $p = y/n$    |
| This book         |    $\pi$      | $p = D/n$  |

* Shorthand: $Y \sim Binomial(n, \pi)$ or $y \sim Binomial(n, \pi)$

**How it arises**

* Sample Surveys 
* Clinical Trials 
* Pilot studies
* Genetics
* Epidemiology 

**Uses**

* to make inferences about $\pi$ from observed proportion $p = y/n$.
* to make inferences in more complex situations, e.g. ...

   + Prevalence Difference: $\pi_{index.category}$ - $\pi_{reference.category}$
   
   + Risk Difference (RD): $\pi_{index.category}$ - $\pi_{reference.category}$
   
   + Risk Ratio, or its synonym Relative Risk (RR): $\frac{\pi_{index.category}}{\pi_{reference.category}}$ 
   
   + Odds Ratio (OR): $\frac{\pi_{index.category}/(1-\pi_{index.category})}{
   \pi_{reference.category}/(1-\pi_{reference.category}) }$
   
   + Trend in several $\pi$'s


**Requirements for $Y$ to have a Binomial$(n, \pi)$ distribution**

* Each element in the 'population' is 0 or 1, but we are only interested in estimating the proportion ($\pi$) of 1’s; we are not interested in individuals.

* Fixed sample size $n$.

* Elements selected at random and independent of each other; each element in population has the same probability of being sampled: i.e., we have $n$ independent Bernoulli random variables with the same expectation (statisticians say '_i.i.d_' or '_independent and identically distributed_').

*  It helps to distinguish the population values, say $Y_1$ to $Y_N$, from the $n$ sampled values $y_1$ to $y_n$.
Denote by $y_i$ the value of the $i$-th sampled element. Prob[$y_i$ = 1] is constant (it is $\pi$) across $i$.
In the [What proportion of our time do we spend indoors?](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Mean-Quantile/inside_outside.pdf) example, it is the **random/blind sampling** of the temporal and spatial patterns of 0s and 1s that **makes $y_1$ to $y_n$ independent of each other**. The $Y$'s, the elements in the population can be related to each other [e.g. there can be a peculiar spatial distribution of persons] but if elements are chosen at random, the chance that the value of the $i$-th element chosen is a 1 cannot depend on the value of $y_{i−1}$ or any other $y$: the sampling is 'blind' to the spatial location of the 1’s and 0s.


**Binomial probabilities, illustrated using a Binomial Tree**


```{r,eval=T, echo=F, fig.align="center", fig.height=8, fig.width=9, warning=FALSE, message=F,fig.cap="From 5 (independent and identically distributed) Bernoulli observations to Binomial(n=5), with the Bernoulli probability left unspecified. There are 2 to the power n possible (distinct) sequences of 0's and 1's, each with its probability. We are not interested in these 2 to the power n probabilities, but in the probability that the sample  contains y 1's and (n-y) 0's. There are only (n+1) possibilities for y, namely 0 to n. Fortunately, each of the n.choose.y sequences that lead to the same sum or count y, has the same probability. So we group the 2.to.power.n sequences into (n+1) sets, according to the sum or count. Each sequence in the set with  y 1's and (n-y) 0's has the same probability, namely  the prob.to.the.power.y times (1-prob).to.the.power.(n-y). Thus, in lieu of adding all such probabilities, we simply multiply this  probability by the number, n.choose-y -- shown in black -- of unique sequences in the set. Check: the frequencies in black add to 2.to.power.n. Nowadays, the (n+1) probabilities are easily obtained by supplying a value for the 'prob' argument in the R function dbinom(), instead of  computing the binomial coefficient n.choose-y by hand."}


par(mfrow=c(2,1),mar = rep(0.001,4) )


s =c(-1,1)
COL=c("red","blue")
p=0.4

plot(c(0,8.5),c(-1,40),col="white",xaxt="n")

y5=1:32
y4=apply(matrix(y5,2,16),2,mean)
arrows(4,rep(y4,each=2), 5, y5,
 col=rep(COL,16),
  length=0.06, angle=25)
text(4.5,0,"5th")

y3=apply(matrix(y4,2,8),2,mean)
arrows(3,rep(y3,each=2), 4, y4,
 col=rep(COL,16),
  length=0.06, angle=25)
text(3.5,0,"4th")
 
y2=apply(matrix(y3,2,4),2,mean)
arrows(2,rep(y2,each=2), 3, y3,
 col=rep(COL,8),
  length=0.06, angle=25)
text(2.5,0,"3rd")

y1=apply(matrix(y2,2,2),2,mean)
arrows(1,rep(y1,each=2), 2, y2,
 col=rep(COL,4),
  length=0.06, angle=25)
text(1.5,0,"2nd")

arrows(0,mean(y5), 1, y1,
 col=COL,
  length=0.06, angle=25)
text(0.5,0,"1st")


C=c("red","blue")
for(i in 31:0){
	r = as.integer(intToBits(y5[i])[5:1] ) 
	points(5+(1:5)/10,rep(i+1,5),col=C[r+1],
	pch=19,cex=0.5)	
}

text(0.8,38,"",adj=c(0,0))
	
	
	text(0,38,expression(
	      paste(
	      "The ", 2^n,
" possible sequences of n independent Bernoulli observations")),
	      adj=c(0,0),cex=1.25)
	
	text(5.7,36,
"With n=5, 32 possible sequences.

In the next panel, sequences leading
to the same positive:negative
(RED/blue) 'split' are grouped.

The number of sequences leading
to the same split is shown in black.

With n=5,
there are 6 possible splits

The probability of a given split
is the probability of any one 
of the sequences leading to it,
multiplied by the number of such 
sequences.",
cex=1.0,adj=c(0,1))
	
    
     text(0,35,
      expression(paste(
        "Prob[ i-th observation is BLUE, i.e. = 1 ] = ",
         pi)
      ),col="blue",adj=c(0,0.5)
    )

#######
     
plot(c(0,7.0),c(-6.7,6.5),col="white",
     xaxt="n",yaxt="n")
 for (y1 in s){
	arrows(0, 0, p, p*y1,
	  col=COL[(y1+3)/2],length=0.07,
	  angle=20, lwd = 1+ (y1==1) )
	if(y1>0) text(p/2, y1/2,
		  expression(pi),
		  adj=c(0.5,0.0),col="blue")
	if(y1<0) text(p/2, y1/2,
		  expression(1 - pi),
		  adj=c(0.5,1),col="red")
	
	
	for (y2 in s){
	  
	  	  arrows(1,y1,1+p,y1+y2*p,
	      col=COL[(y2+3)/2],
	      length=0.07,angle=20,
	      lwd = 1+ (y2==1) )
	  for (y3 in s){
	  	arrows(2,y1+y2,2+p,y1+y2+y3*p,
	      col=COL[(y3+3)/2],
	      length=0.07,angle=20,
	      lwd = 1+ (y3==1) )
	    for (y4 in s){
	  	  arrows(3,y1+y2+y3,3+p,y1+y2+y3+y4*p,
	      col=COL[(y4+3)/2],
	      length=0.07,angle=20,
	      lwd = 1+ (y4==1) )
	      for (y5 in s){
	  	   
	  	   arrows(4,y1+y2+y3+y4,4+p,y1+y2+y3+y4+y5*p,
	         col=COL[(y5+3)/2],
	         length=0.07,angle=20,
	         lwd = 1+ (y5==1) )
          } # y5
        } # y4
      } # y3
    } #y2
 } #y1

	    
    N=c(1,2,1)	  
	for(ss in 2:1){
		y = ss*1 - (2-ss)*1
		C = N[ss+1]
		text(1+(p)/2, 
		     y-0.2,expression(pi),
		     col="blue", adj=c(0.5,1))
	}
	
    N=c(1,3,3,1)	  
	for(ss in 3:1){
		y = ss*1 - (3-ss)*1
		C = N[ss+1]
		text(2+(p)/2, 
		     y-0.2,expression(pi),
		     col="blue", adj=c(0.5,1))
	}
	N=c(1,4,6,4,1)	  
	for(ss in 4:1){
		y = ss*1 - (4-ss)*1
		C = N[ss+1]
		text(3+(p)/2, 
		     y-0.2,expression(pi),
		     col="blue", adj=c(0.5,1))
	}
	N=c(1,5,10,10,5,1)	  
	for(ss in 5:1){
		y = ss*1 - (5-ss)*1
		C = N[ss+1]
		text(4+(p)/2, 
		     y-0.2,expression(pi),
		     col="blue", adj=c(0.5,1))
	}
	

	   text(0.0, 5.5,
"1,2,3, ... 10: Number of sequences that yield the
indicated split (can obtain from nCy or Pascal's Triangle).
All sequences leading to the split are equiprobable.",
	 adj=c(0,0.5),cex=1.05)
	 
	 text(6,6,"Binomial Probabilities*")
  text(6.9,-6.25,"* in R: dbinom(0:5,size=5,prob=0.xx)",
  family="mono", adj=c(1,1),,cex=1.25)
  
  text(0,0,"1", adj=c(1,0.5))

 	N=c(1,1)	  
	for(ss in 1:0){
		y = ss*1 - (1-ss)*1
		C = N[ss+1]
		text(1-(1-p)/1.2, 
		     y*(p+1)/2,toString(C),adj=c(0,0.5))
	}		
	points( (p+1)/2, (p+1)/2,
		  cex=1,pch=19,col="blue")
    points( (p+1)/2, -(p+1)/2,
		  cex=1,pch=19,col="red")
		  
		  
	N=c(1,2,1)	  
	for(s in 2:0){
		y = s*1 - (2-s)*1
		C = N[s+1]
		text(2-(1-p)/1.5, 
		     y*(p+1)/2,toString(C),adj=c(1,0.5))
	}	
		  
    points( 1+(p+1)/2, 1+(p+1)/2+p/2,
		  cex=1,pch=19,col="blue")
	points( 1+(p+1)/2, 1+(p+1)/2-p/2,
		  cex=1,pch=19,col="blue")
		  
    points( 1+(p+1)/2, p/2,
		  cex=1,pch=19,col="blue")
	points( 1+(p+1)/2, -p/2,
		  cex=1,pch=19,col="red")
		  
	points( 1+(p+1)/2, -1-(p+1)/2+p/2,
		  cex=1,pch=19,col="red")
	points( 1+(p+1)/2, -1-(p+1)/2-p/2,
		  cex=1,pch=19,col="red")
		  
	N=c(1,3,3,1)	  
	for(ss in 3:0){
		if(ss==3) COL=rep("blue",4)
		if(ss==2) COL[3]="red"
		if(ss==1) COL[2]="red"
		if(ss==0) COL[1]="red"
		y = ss*1 - (3-ss)*1
		points( 2+(p+1.1)/2      , y+0.1,
		  cex=1,pch=19,col=COL[1])
		points( 2+(p+1.1)/2 -0.08,  y-0.2,
		  cex=1,pch=19,col=COL[2])
		points( 2+(p+1.1)/2 +0.08,  y-0.2,
		  cex=1,pch=19,col=COL[3])
		
	}
	
	N=c(1,3,3,1)	  
	for(ss in 3:0){
		y = ss*1 - (3-ss)*1
		C = N[ss+1]
		text(3-(1-p)/1.5, 
		     y-0.2,toString(C),adj=c(1,0.5))
	}
	
	N=c(1,4,6,4,1)	  
	for(ss in 4:0){
		if(ss==4) COL=rep("blue",4)
		if(ss==3) COL[4]="red"
		if(ss==2) COL[3]="red"
		if(ss==1) COL[2]="red"
		if(ss==0) COL[1]="red"
		y = ss*1 - (4-ss)*1
		points( 3+(p+1.1)/2 - 0.06, y-0.2,
		  cex=1,pch=19,col=COL[1])
		points( 3+(p+1.1)/2 +0.06,  y-0.2,
		  cex=1,pch=19,col=COL[2])
		points( 3+(p+1.1)/2 -0.06,  y+0.2,
		  cex=1,pch=19,col=COL[3])
		points( 3+(p+1.1)/2 +0.06,  y+0.2,
		  cex=1,pch=19,col=COL[4])
		    
		text(4-(1-p)/1.5, 
		     y-0.2,toString(N[ss+1]),adj=c(1,0.5))
	}
	
		  
	N=c(1,5,10,10,5,1)	  
	for(ss in 5:0){
		if(ss==5) COL=rep("blue",5)
		if(ss==4) COL[5]="red"
		if(ss==3) COL[4]="red"
		if(ss==2) COL[3]="red"
		if(ss==1) COL[2]="red"
		if(ss==0) COL[1]="red"
		y = ss*1 - (5-ss)*1
		
		text(5-(1-p)/1.24, 
		     y,toString(N[ss+1]),adj=c(1,0.5))
		
		points( 4+(p+1.1)/2 - 0.12, y,
		  cex=1,pch=19,col=COL[1])
		points( 4+(p+1.1)/2 +0.12,  y,
		  cex=1,pch=19,col=COL[2])
		points( 4+(p+1.1)/2 ,  y+0.40,
		  cex=1,pch=19,col=COL[3])
		points( 4+(p+1.1)/2 ,  y-0.40,
		  cex=1,pch=19,col=COL[4])
		points( 4+(p+1.1)/2 ,  y,
		  cex=1,pch=19,col=COL[5])

		C = N[ss+1]
		text(4.75+(p+1.6)/2, y,
		  substitute(
		    paste(coef, 
		     phantom(0),
		     scriptstyle(x),
		     phantom(0)),
		    list(coef=C,y=s,rest=3-ss)
		  ),
		  adj=c(1,0.8)
		)
		
		text(4.7+(p+1.6)/2+0.4, y,
		  substitute(
		    paste( pi^y, phantom(0)),
		    list(coef=C,y=ss,rest=5-ss)
		  ),col="blue",
		  adj=c(1,0.5)
		)
		
		text(4.7+(p+1.6)/2+0.4, y,
		  substitute(
		    paste( (1-pi)^rest),
		    list(coef=C,y=ss,rest=5-ss)
		  ),col="red",
		  adj=c(0,0.5)
		)
		
	}
	
	
```

If you rotate the binomial tree to the right by 90 degrees, and use your imagination, you can see how it resembles the [quincunx](https://en.wikipedia.org/wiki/Quincunx) constructed by [Francis Galton](https://en.wikipedia.org/wiki/Bean_machine). He used it to show how the Central Linit Theorem, applied to the sum of several 'Bernoulli deflections to the right and left',  makes a Binomial distribution approach a Gaussian one. Several [games](https://en.wikipedia.org/wiki/Bean_machine#Games) and game shows are built on this pinball machine, for example, [Plinko](https://fivethirtyeight.com/features/what-if-god-were-a-giant-game-of-plinko/) and, more recently, [The Wall](https://www.nbc.com/the-wall?nbc=1).
Galton's quincunx has its own cottage industry, and versions of it often displayed in Science Museums. The present authors inherited a low tech version of the [Galton Board](http://www.galtonboard.com), where the 'shot' are turnip seeds, from former  McGill [Professor -- and early teacher of course 607 -- FDK Liddell](https://www.mcgill.ca/medicine/staff-resources/inmemoriam/2003). 


**Does the Binomial Distribution apply if... ?**


|             |      |      | 
|:------------------|   ----:      |   :----     |  
|Interested in:      |   $\pi$    |  the proportion of 16 year old girls in Quebec  protected against rubella  |
|Choose:  | $n$ = 100    | girls: 20 at random from each of 5 randomly selected schools ['cluster' sample] |
|Count | $y$ | how many of the $n$ = 100 are protected|
| | | $\bullet$ Is $y ~ \sim Binomial(n=100, \ \pi)$ ? | 
| ............. | ........... | ............................................................... |
|'SMAC':      |   $\pi$    | Chemistry Auto-analyzer with n = 18 channels. Critertion for 'positivity' set so that Prob['abnormal' result in Healthy person] = 0.03 for each of 18 chemistries tested |  
| | |  |
|Count | $y$ | (In 1 patient) how many of $n$ = 18 give abnormal result.
|
| | | $\bullet$ Is $y ~ \sim Binomial(n=18, \ \pi=0.03)$ ? [credit:  Ingelfinger textbook](https://www.amazon.ca/Biostatistics-Clinical-Medicine-Joseph-Ingelfinger/dp/0023597216) | 
| ............. | ........... | ............................................................... |
| ............. | ........... | ............................................................... |
|Sex Ratio:      |   $n=4$    | children in each family |  
| | $y$ | number of girls in family |
| | | $\bullet$ Is $y ~ \sim Binomial(n=4, \ \pi=0.49)$ ? | 
| ............. | ........... | ............................................................... |
|Interested in:      |    $\pi_u$   | proportion in 'usual' exercise classes and in |
|   |    $\pi_e$   | expt'l. exercise classes who 'stay the course' |
| Randomly| 4 | classes of |
| Allocate|25 | students each to usual course |
| | $n_u = 4 \times 25 = 100$ | |
| | | |
|  | $n_e$ = 4 | classes of |
|  |25 | students each to experimental course |
| | $n_e = 4 \times 25 =100$ | |
| | | |
|Count | $y_u$ | how many of the $n_u$ = 100 complete course|
|      | $y_e$ | how many of the $n_e$ = 100 complete course|
| | | $\bullet$ Is $y_u ~ \sim Binomial(n = 100, \ \pi_u)$ ? |
| | | $\bullet$ Is $y_e ~ \sim Binomial(n = 100, \ \pi_e)$ ? |
| ............. | ........... | ............................................................... |
|Pilot Study:      |       | To estimate proportion $\pi$ of population that is eligible and willing to participate in long-term research study, keep recruiting until obtain |
| | $y$ = 5 | who are. Have to approach $n$ to get $y$.
| | | $\bullet$ Is $y ~ \sim Binomial(n, \ \pi)$ ? | 
| ............. | ........... | ............................................................... |

**Calculating Binomial probabilities**

_Exactly_

* probability mass function (p.m.f.) :

   + formula: $Prob[y] = \  ^n C _y \ \pi^y \  (1 − \pi)^{n−y}$.
   
   + recursively: $Prob[0] = (1−\pi)^n$; \ \  $Prob[y] = \frac{n−y+1}{y} \times \frac{\pi}{1-\pi} \times Prob[y−1]$.
   
* Statistical Packages:
   + R functions `dbinom()`, `pbinom()`, `qbinom()`  
   probability mass, distribution/cdf, and quantile functions.

   + Stata function `Binomial(n,k,p)`
   
   + SAS `PROBBNML(p, n, y)` function
   
* Spreadsheet — Excel function `BINOMDIST(y, n, π, cumulative)`

* Tables: CRC; Fisher and Yates; Biometrika Tables; Documenta Geigy

_Using an approximation_

* Poisson Distribution ($n$ large; small $\pi$)

* **Normal (Gaussian) Distribution** ($n$ large or midrange $\pi$, so that the expected value, $n \times \pi$, is sufficiently far 'in from' the 'edges' of the scale, i.e., sufficiently  far in from 0 and from $n$, so that a Gaussian distribution doesn't flow past one of the edges. The Normal approximation is good for when you don't have access to software or Tables, e.g, on a plane, or when the internet is down, or the battery on your phone or laptop had run out, or it takes too long to boot up Windows!).  
To use the Normal approximatiom, be aware of the **scale you are working in**, .e.g., if say $n = 10$, whether the summary is a **count** or a **proportion** or a **percentage**.


|   | r.v.  | e.g.  | E  | S.D. |
|----: |  :----      |   ---:  | :--: | :---: |  
| **count**: |  $y$   |  2 | $n \times \pi$ | $\{n \times \pi \times (1-\pi) \}^{1/2}$ |
| | | | | i.e., $n^{1/2} \times \sigma_{Bernoulli}$ |
| | | | | |                                  
| **proportion**:   | $p=y/n$  | 0.2 | $\pi$ | $\{\pi \times (1-\pi) / n \}^{1/2}$ |
| | | | | i.e., $\sigma_{Bernoulli} / n^{1/2}$ |
| | | | | |
| **percentage**: | $100p\%$ | 20\% | $100 \times \pi$ | $100 \times SD[p]$ |

The first person to suggest an approximation, using what we now call the 'Normal' or 'Gaussian' of 'Laplace-Gaussian' distribution, was 
[deMoivre, in 1738](http://www.biostat.mcgill.ca/hanley/statbook/TheDoctrineOfChancesAnnotated.pdf). There is a debate among historians as to whether this marks the first description of the Normal distribution: the piece does not explicitly point to the probability density function  $\frac{1}{\sigma \sqrt{2 \pi}} \times exp[-z^2/2\sigma^2],$  but it does highlight the role of the quantity $(1/2) \times \sqrt{n}$, the standard deviation of the sum of $n$ independent Bernoulli random variables, each with expectation 1/2 and thus  a 'unit' standard deviation of 1/2, and also the SD quantity $\sqrt{\pi(1-\pi)}$ $\times$ $\sqrt{n}$ in the more general case. DeMoivre arrived at the familiar '68-95-99.7 rule' : the percentages of a normal distribution that lie within 1, 2 and 3 SD's of its mean.


**Factors that modulate the shapes of  Binomial distributions**

* size of $n$: the larger the n, the more symmetric

* value of $\pi$: the closer to 1/2, the more symmetric

In these small-$n$ contexts, only those distribtions where $\pi$ is close to 0.5 are reasonably symmetric. 


In larger-$n$ contexts (see below), as long as there is 'room' for them to be,  binomial distribtions where the expected value $E = n \times \pi$ is at least 5-10 'in from the edges' (i.e. to the right of 0, or the left of $n$, are reasonably symmetric. 



```{r,eval=T, echo=F, fig.align="center", fig.height=4, fig.width=9, warning=FALSE, message=F}

showBinomials = function(n=5,d.n=1){
  
    par(mfrow=c(1,1),mar = c(0,0,1,0))

    plot(-10,-10,xlim=c(-0.1,1.1),ylim=c(-0.12,1.12)*n,,frame=FALSE)

    Dx = 0.1

    yy = 0:n 
    
    yyy = seq(0,n,d.n)
    
    

    text(-0.04,  1.11*n, expression(paste(pi,":")), adj=c(0,0),cex=1.5,font=2)
    text(-0.04, -0.11*n, expression(paste(pi,":")), adj=c(0,1),cex=1.5,font=2)

    if(n > 5 & (n-5)> 5){
        segments(Dx/2,   5, 1-Dx, 5, col="lightblue",   
                 lty="dotted", lwd=1.5)
        segments(Dx/2, n-5, 1-Dx, n-5, col="lightblue",
                 lty="dotted", lwd=1.5)
    } 
    if(n > 10){
        segments(Dx/2,   10, 1-Dx,   10, col="lightblue",
                 lty="dotted", lwd=2)
        segments(Dx/2, n-10, 1-Dx, n-10, col="lightblue",
                 lty="dotted", lwd=2)
    } 
    for(P in seq(Dx,1-Dx,Dx)){
        if(P %in% seq(Dx,1-Dx,Dx)) {
            text(P-Dx/2,  1.11*n, toString(P), adj=c(0,0))
            text(P-Dx/2, -0.11*n, toString(P), adj=c(0,1))
        }
        rect(P-Dx/2, -1/2, P+Dx/2, n+1/2, border="grey85")
        if(P==Dx){ 
           text(P -Dx, 0.5*n, "Y", adj=c(1.5,0.5))
           text(1-P+Dx,   0.5*n, "Y", adj=c(0,0.5))
           for(y in yy) {
              if( y %in% yyy ){
              text(P-Dx/1.5, y, toString(y), 
                   adj=c(1.25,0.5))
              text(1-Dx + Dx/1.5   , y, toString(y), 
                   adj=c(0,   0.5) )
              }
           }
        }
    xx = dbinom(yy,n,P ) * Dx * sqrt(n/5)
    E = n*P ; COL=col="grey55"
    if( ( E >= 5 & E <=  (n-5) )  ) {
        COL = "blue"
    }
    rect(rep(P,n+1)-Dx/2, yy-0.1, rep(P,n+1)+xx - Dx/2, yy+0.1, 
              border=NA, col=COL )
    
    if( COL=="blue") {
       points(P-Dx/1.75,E, pch=19, cex=0.5, col=COL)
       text(P-Dx/1.75,E, expression(mu), 
            cex=1,adj=c(1.5,0.5),font=2, col=COL)
      
    } 
    }
  
}


```


```{r,eval=T, echo=F, fig.align="center", fig.height=4, fig.width=9, warning=FALSE, message=F,fig.cap="Binomial random variables/distributions, where n = 5, and the Bernoulli expectation (probability) is smaller (left panels) or larger (right panels)."}

showBinomials(n=5,d.n=1)

```

```{r,eval=T, echo=F, fig.align="center", fig.height=4, fig.width=9, warning=FALSE, message=F,fig.cap="Various Binomial random variables/distributions, where n = 20. The dotted horizontal lines in light blue are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units from the (0,n) boundaries are shown in blue."}

showBinomials(n=20,d.n=2)

```

```{r,eval=T, echo=F, fig.align="center", fig.height=4, fig.width=9, warning=FALSE, message=F,fig.cap="Various Binomial random variables/distributions, where n = 50. The blue dotted lines are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units in from the (0,n) boundaries are shown in blue"}

showBinomials(n=50,d.n=5)

```


https://en.wikipedia.org/wiki/Normal_distribution#CITEREFStigler1982


dark https://physics.stackexchange.com/questions/28563/hours-of-light-per-day-based-on-latitude-longitude-formula

https://environhealthprevmed.biomedcentral.com/articles/10.1186/s12199-017-0637-4

car sppeds https://purr.purdue.edu/publications/2671/1

Hypergeometric

Non-Central Hypergeometric

Chi-square

===

EXERCISES

ADD YOUR OWN SEARCH RESULTS to table above

Hydroxychloroquine uniformity table.

few for differences

woolf

normal

transformations


## Exercises

### Clusters of Miscarriages [based on article by L Abenhaim] 

Assume that:

* 15\% of all pregnancies end in a recognized spontaneous abortion (miscarriage) -- this is probably a conservative estimate.
* Across North America, there are 1,000 large companies. In each of them, 10 females who work all day with computer terminals become pregnant within the course of a year [the number who get pregnant would vary, but assume for the sake of this exercise that it is exactly 10 in each company].
*There is no relationship between working with computers and the risk of miscarriage.
* a ``cluster'' of miscarriages is defined as ``at least 5 of 10 females in the same company suffering a miscarriage within a year''

Exercise: Calculate the number of 'clusters' of miscarriages one would expect in the 1,000 companies. _Hint_: begin with the probability  of a cluster. 

### 'Prone-ness' to Miscarriages ? 

Some studies suggest that the chance of a pregnancy ending in a spontaneous abortion is approximately 30%.

1. On this basis, if a woman becomes pregnant 4 times, what does the binomial distribution give as her chance of having 0,1,2,3 or 4 spontaneous abortions?
2. On this basis, if 70 women each become pregnant 4 times, what number of them would you expect to suffer 0,1,2,3 or 4 spontaneous abortions?  (Think of the answers in (i) as proportions of women).
3. Compare these theoretically expected numbers out of 70 with the following observed data on 70 women, each of whom had 4 pregnancies:

| | | | | | |
|--: | :--: | :--: | :--: | :--: | :--: |
| No. of spontaneous abortions: | 0 | 1 | 2 | 3 | 4 |
| No. of women with this many abortions: |  23 | 28 | 7 | 6 | 6 |

4. Why don't the expected numbers agree very well with the observed numbers?  i.e. which assumption(s) of the Binomial Distribution are possibly being violated?  (Note that the overall rate of spontaneous abortions in the observed data is in fact 84 out of 280 pregnancies or 30%)

### Automated Chemistries (from Ingelfinger et al)

At the Beth Israel Hospital in Boston, an automated clinical chemistry analyzer is used to give 18 routinely ordered chemical determinations on one order (glucose, BUN, creatinine, ..., iron). The ``normal'' values for these 18 tests were established by the concentrations of these chemicals in the sera of a large sample of healthy volunteers. The normal range was defined so that an average of 3% of the values found in these healthy subjects fell outside.

1. Using the binomial formula [even if it is na\"{\i}ve to do so here], compute the probability that a healthy subject will have normal values on all 18 tests. Also calculate the probability of 2 or more abnormal values.
2. Which of the requirements for the binomial distribution are definitely satisfied, and which ones may not be?
3. Among 82 normal employees at the hospital, 52/82 (64%) had all normal tests, 19/82 (23%) had 1 abnormal test and 11/82 (13%) had 2 or more abnormal tests. Compare these observed percentages with the theoretical distribution obtained from calculations using the binomial distribution. Comment on the closeness of the fit.

### Binomial or Opportunistic? (Capitalization on chance... multiple looks at data) 

[Question from Ingelfinger et al. textbook] Mrs A has mild diabetes controlled by diet.  Blood values vary rapidly, so think of each day as a new (independent) situation.
Her morning urine sugar test is negative 80% of the time and positive (+) 20% of the time. [It is never graded higher than +]. 

1. At her regular visits to her physician, the physician always asks about last 5 days. At this particular visit, she tells the physician that the test has been + on each of the last 5 days.  
What is the probability that this would occur if her condition has remained unchanged? Does this observation give reason to think that her condition has changed?

2. Is the situation different if she observes, _between_ visits, that the test is positive on 5 successive days and phones to express her concern?

### Can one influence the sex of a baby?

This question was prompted by this  [article](http://www.biostat.mcgill.ca/hanley/statbook/SexRatioOvulationNEJM1979.pdf)  in an 1979 issue of the New England Journal of Medicine.

1. Consider a binomial variable with $n = 145$ and $\pi = 0.528$. Calculate the SD of, and therefore a measure of the variation in, the proportions that one would observe in different samples of 145 if $\pi$ = 0.528. [In other words, the SD of the sampling distribution of the sample proportion.]  

The following is abstracted from that NEJM article:

> The baby's sex was studied in births to Jewish women who observed the orthodox ritual of sexual separation each month and who resumed intercourse within two days of ovulation. The proportion of male babies was 95/145 or 65.5% (!!) in the offspring of those women who resumed intercourse two days after ovulation (the  overall percentage of male babies born to the 3658 women who had resumed intercourse within two days of ovulation [i.e. days -2, -1, 0, 1 and 2]  was 52.8%).  

2. How does the SD you calculated above help you judge the findings?  And why did you _not_ have to substitute the $p=0.655$ into the SD formula, and call it an SE of $p$?

### It's the 3rd week of the course: it must be Binomial 

In which of the following would $Y$ _**not**_ have a Binomial distribution?  Why?

1. The pool of potential jurors for a murder case contains 100 persons chosen at random from the adult residents of a large city.  Each person in the pool is asked whether he or she opposes the death penalty; $Y$ is the number who say 'Yes.'
2. $Y$ = number of women listed in different random samples of size 20 from the 1990 directory of statisticians.
3. $Y$ = number of occasions, out of a randomly selected sample of 100 occasions during the year, in which you were indoors.  (One might use this design to estimate what proportion of time you spend indoors)
4. $Y$ = number of months of the year in which it snows in Montreal.
5. $Y$ = Number, out of 60 occupants of 30 randomly chosen cars, wearing seatbelts.
6. $Y$ = Number, out of 60 occupants of 60 randomly chosen cars, wearing seatbelts.
7. $Y$ = Number, out of a department's 10 microcomputers and 4 printers, that are going to fail in their first year.
8. $Y$ = Number, out of simple random sample of 100 individuals, that are left-handed.
9. $Y$ = Number, out of 5000 randomly selected from mothers giving birth each month in Quebec, who will test HIV positive.    
10. You observe the sex of the next 50 children born at a local hospital; $Y$ is the number of girls among them.
11. A couple decides to continue to have children until their first girl is born; $Y$ is the total number of children the couple has.
12. You want to know what percent of married people believe that mothers of young children should not be employed outside the home.  You plan to interview 50 people, and for the sake of convenience you decide to interview both the husband and the wife in 25 married couples.  The random variable $Y$ is the number among the 50 persons interviewed who think mothers should not be employed.


### Tests of intuition

* A coin will be tossed either 2 times or 20 times.  You will win $2.00 if the number of heads is equal to the number of tails, no more and no less.  Which is correct? (i) 2 tosses is better. (ii) 100 tosses is better. (iii) Both offer the same chance of winning.

* Hospital A has 100 births a year, hospital B has 2500. In which hospital is it more that at least 55\% of births in one year will be boys.


### CI for proportion when observe 0/n or n/n

Oncologists often first try out new experimental agents on patients with metastatic disease who have failed all standard therapies.
One (old) rule for deciding to abandon an experimental agent was: if it was tried on 14 successive patients, but did not show anti-tumour in any of them, i.e. if it `struck out' in all 14, yielding a response rate of 0/14.

Their reasoning is that this poor result ``rules out'' (with 95\% confidence) the possibility that it would be active in more than 20\% of future patients.
In other words, the data are incompatible with any $\pi > 20\%.$

Check out their rule, by computing/obtaining a 95\% 1-sided CI for $\pi.$ 

[Use a 1-sided CI if one is interested in putting just an \textit{\underline{upper}} bound on the probability of benefit or risk: e.g., what is upper bound on $\pi = $ probability of getting HIV from HIV-infected dentist? See JAMA article 
``If Nothing Goes Wrong, Is Everything All Right? Interpreting Zero Numerators''  by Hanley and Lippman-Hand

http://www.epi.mcgill.ca/hanley/Reprints/If\_Nothing\_Goes\_1983.pdf

and the `rule of $3/n$' for a 1-sided 95\% CI.
See also the second example (with sample size 4 !) in the section `in defense of (some) small studies'
in this article
http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/Place\_of\_statistical\_1989.pdf





