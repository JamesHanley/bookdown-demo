# Distributions {#distributions}


## Objectives

Distributions of individual values take their shapes and spreads from the features of the setting, and thus do not follow any general laws. The shapes and the spreads of distributions of statistical summaries and parameter estimates made from aggregates of individual observations tend to be more regular and more predictable, and thus more law-abiding.  

So, the **specific objectives** in this chapter are to truly understand

* the distinction between a natural and  investigator-made distributions, and between  observable and  conceptual ones.

* why we should  automatically associate certain distributions with certain types of random variables

* why we need to understand the pre-requisites for random variables following the distributions they do

* why we rely so much on the Normal distribution, and why it  is so 'Central' to statistical inference concerning parameters.

* why the pre-occupation with  checking 'Normality' (Gaussian-ness) is misguided

* why Normality is not even relevant when the 'variable' is not 'random', and appears on the right hand side of a regression model.

* the few contexts where shape does matter

## Named Distributions

In historical order

Bernoulli, and scaled Bernoulli

Binomial

Normal

beta

Poisson

t

Wilcoxon

=======

To get a full list of the named distributions available in `R` you can use the following command: `help(Distributions)`


### Bernoulli

The simplest random variable is one that take just 2 possible values, such as  YES/NO, MALE/FEMALE, 0/1, ON/OFF, POSITIVE/NEGATIVE, PRESENT/ABSENT, EXISTS/DOES NOT, etc.

This random variable $Y$ is governed by just one parameter, namely the probability, $\pi$, that it takes on the YES (or '1') value. Of course you can reverse the scale, and speak about the probability of a NO (or '0') result.

It is too bad that when [Wikipedia](https://en.wikipedia.org/wiki/Bernoulli_distribution), which has a unified way of showing the main features of statistical distributions, does not follow its own principles and show a graph of various versions of a Bernoulli distribution. So here is such a graph. 


```{r,eval=T, echo=F, fig.align="center", fig.height=2, fig.width=9, warning=FALSE, message=F,fig.cap="Various Bernoulli random variables/distributions. We continue our convention of using the letter Y (instead of X) as the generic name for a random variable. Moreover, in keeping with this view, all of the selected Bernoulli distributions are plotted with their 2 possible values shown on the vertical axis."}

par(mfrow=c(1,1),mar = c(0,0,1,0))

plot(-10,-10,xlim=c(1,17),ylim=c(-0.1,1.1))

Dx = 1.75
for(Pos in (1:9)){
   
   segments(Pos*Dx,0,Pos*Dx,1)
   prob = Pos/10
   arrows(Pos*Dx-Dx/6, prob, Pos*Dx, prob,
          length=0.06, angle=25)
   if(Pos==1){ 
      text(Pos*Dx, 0.5, "Y", adj=c(1.5,0.5))
      text(Pos*Dx, 1, "1", adj=c(1.25,0.5))
      text(Pos*Dx, 0, "0", adj=c(1.25,0.5))
      }
   segments(Pos*Dx,1,Pos*Dx+ prob,1,lwd=1.5)
   segments(Pos*Dx,0,Pos*Dx+ (1-prob),0)
   text(Pos*Dx + prob, 1, 
                       toString(prob), 
                       adj=c(-0.25,0.5),
                       cex=0.65,font=2)
   text(Pos*Dx + 1-prob, 0, 
                       toString(1-prob), 
                       adj=c(-0.25,0.5),
                       cex=0.65)
}

```

Please, when reading the Wikipedia entry, replace all instances of $X$ and $x$ by $Y$ and $y$. Note also that we will use $\pi$ where Wikipedia, and some textbooks, use $p$. As much as we can, we use Greek letters for parameters and Roman letters for their empirical (sample) counterparts. Also, to be  consistent, if the random variable itslef is called $Y$, then it makes sense to use $y$ as the possible relaizations of it, rather than the illogical $k$ that Wikipedia uses.]

In the sidebar, Wikipedia shows the probability mass function (_pmf_, the probabilities that go with the possible $Y$ values) in two separate rows, but in the text the _pmf_ is also shown more concisely, as (in our notation)

$$f(y) = \pi^y (1-\pi)^{1-y}, \ \ y = 0, 1.$$
If we wish to align with how the `R` software names features of distributions, we might want to switch from $f$ to $d$. `R` uses $d$ because it harmonizes with the probability $d$ensity function (_pdf_) notation that its uses for random variables on an interval scale, even though some  statistical 'purists' see that as mixing terminology: they use the term probablility mass function for discrete random variables, and probablity density function for ones on an interval scale.

$$d_{Bernoulli}(y) = \pi^y (1-\pi)^{1-y}.$$

Sadly, Bernoulli does not get its own entry in `R`'s list of named distributions, presumably because it is a special case of a binomial distribution, one where $n$ = 1. 
So we have to call `dbinom(x,size,prob)` to get the
density (probability mass) function of the binomial distribution with parameters `size` ($n$) and `prob` ($\pi$), and set $n$ to 1. 

The 3 arguments to  `dbinom(x,size,prob)` are:

* `x`: a vector of quantiles (here just 0 or/and 1),
* `size`: the number of 'trials' (our '$n$', so 1 for Bernoulli),
and 
* `prob`: the probability of 'success' on each 'trial'. We think of it as the probability that a realization of $Y4, i.e, $y$ will equal 1, or as $\pi.$)

Thus, `dbinom(x=0,size=1,prob=0.3)` yields `r dbinom(x=0,size=1,prob=0.3)`, while `dbinom(x=1,size=1,prob=0.3)` yields `r dbinom(x=1,size=1,prob=0.3)` and `dbinom(x=c(0,1),size=1,prob=0.3)` yields the vector `r dbinom(x=c(0,1),size=1,prob=0.3)`.

Incidentally, please do not adopt the convention that $x$ (or our $y$) is the number of ‘successes’ in $n$ trials. It is the number of 'positives' in a sample of $n$ independent draws from a population in which a proportion $\pi$ are positive.

**_Expectation (E) and Variance (V)_ of a Bernoulli random variable.**

Shortening $Prob(Y=y)$ to $P_y$, we have

* From first principles, 
 $$E[Y] = 0 \times P_0 + 1 \times P_1 = 0 \times (1-\pi) + 1 \times \pi = \pi,$$
while
 $$V[Y] = (0-\pi)^2 \times P_0 + (1-\pi)^2 \times P_1  = \pi^2(1-\pi) + (1-\pi)^2\pi =  \underline{\pi(1-\pi)}.$$

This functional form for the ('unit') variance is not entirely surprising: it is obvious from the selected distributions whon that the most concentrated Bernoulli distributions are the ones where the proportion $(\pi)$ of Y = 1 values is either close to 1 or to zero, and that the most spread out Bernoulli distributions are the ones where $\pi$ is close to 1/2. And, and a function of $\pi$, the Variance must be symmetric about $\pi$ = 1/2.   

The fact that the greatest uncertainty ('entropy', lack of order or predictability) is when $\pi$ = 1/2 is one of the factors that makes sports contests more engaging when teams or players  are well matched. Later, when we come to study what influences the imprecision of sample surveys, we will see that for a given sample size, the imprecision is largest when  $\pi$ is closer to 1/2.  

**Why focus on the variance of a Bernoulli random variable?** because, later, when we use the more intesting binomial distribution, we can call on first prionciples to recall what its expection and variance are. A Binomial random variable is the sum of $n$ independently distributed Bernoulli random variables, all with the same expectation $\pi$ and unit variance $\pi(1-\pi).$ Thus its expectation ($E$) and variance  ($V$) are the sums of these 'unit' versions, i.e., $E[binom.sum] = n \times \pi$ and $V[binom.sum] =  n \times \pi(1-\pi).$ Moreover, again from first principles, we can deduce that if instead of a sample _sum_, we are interested in a sample _mean_ (here the _mean_ of the 0's and 1's is the sample _proportion_), its expected value is 
$$\boxed{\boxed{E[binom.prop'n] = \frac{n \pi}{n} = \pi; \   V[.] = \frac{n  \pi(1-\pi))}{n^2} = \frac{\pi(1-\pi)}{n}; \ SD[.] = \frac{\sqrt{\pi(1-\pi)}}{ \sqrt{n}} = \frac{\sigma_{0,1}}{\sqrt{n}} } }  $$  

Note here the generic way we write the SD of the sampling distribution of a sample proportion, in the same way that we write the SD of the sampling distribution of a sample mean, as $\sigma_u/\sqrt{n},$ where $\sigma_u$ is the 'unit' SD, the standard deviation of the  values of _individuals_. The individual values in the case of a Bernoulli randomn variable are just 0s and 1s, and their SD is $\sqrt{\pi(1-\pi)}.$ We call this SD the SD of the 0'1 and 1's, or $\sigma_{0,1}$ for short.

Notice how, even though it might look nicer and simpler to compute, and involves just 1 square root calculation, we did not write the SD of a binomial proportion as  
$$SD[binom.proportion] = \sqrt{\frac{\pi(1-\pi)}{ n} }.$$ 
We choose instead to use the $\sigma/\sqrt{n}$ version, to show that it has the same _form_ as the SD for the sampling distribution of a sample mean. Now that we no longer need to  savw keystrokes on a hand caloculator, we should move away from  computational forms and focus instead on the intuitive form.  Sadly, many textbooks re-use the same concept in disjoint chapters without telling readers they are cases of the same SD formula. 

There is a lot to be gained by thinking of proportions as means, but where the $Y$ values are just 0's and 1's. You can use the  `R code` below to simulate a very large number of 0's and 1's, and calculate their variance. The  `sd` function in `R` doesn't know or care that the values you supply it are limited to just 0s and 1s, or spread along an interval. Better still don't use the `rbinom` function; instead use the `sample` function, with replacement.



```{r,eval=T, echo=T, fig.align="center", fig.height=2, fig.width=9, warning=FALSE, message=F,fig.cap="Various Bernoulli random variables/distributions. We continue our convention of using the letter Y (instead of X) as the generic name for a random variable. Moreover, in keeping with this view, all of the selected Bernoulli distributions are plotted with their 2 possible values shown on the vertical axis."}

n = 750

zeros.and.ones = sample(0:1, n , 
   prob=c(0.2, 0.8),replace=TRUE )

m = matrix(zeros.and.ones,n/75,75)
noquote(apply(m,1,paste,collapse=""))

sum(zeros.and.ones)/n
round( sd(zeros.and.ones),4)

```

Try the above code with a larger $n$ and a different $\pi$ and convince yourself that the variance (and thus the SD) of the individual 0 and 1 values (a) have nothing to do with how many there are and everything to do with what proportion of them are of each type and (b)  are larger when the proportions are close to each other, and smaller when they are not. 

**Scaled-Bernoulli random variables**

**Could we get by without studying the Binomial Distribution?** The answer is 'for most applications, yes.' The reason is that in in most cases, we are able to use a Gaussian (Normal) approximation to the binomial distribution. Thus, all we need are its expectation and variance (standard deviation):  we don't need the `dbinom()` probability mass function, or the `pbinom()` that gives the cumulative distribution function and thus the tail areas, or the `qbinom()` function that gives the quantiles. But sometimes we deal with situations where the binomial distributions are not symmetric and close-enough-to-Gaussian.

we will come back to how the binomial distribution led to the [deMoivre's derivation](https://en.wikipedia.org/wiki/De_Moivre–Laplace_theorem) of a 'Gaussian distribution -- in 1738, almost 4 decades before Gauss was born.


https://fivethirtyeight.com/features/what-if-god-were-a-giant-game-of-plinko/


===

### Binomial

**The Binomial Distribution is a  model for the (sampling) variability of a proportion or count in a randomly selected sample**

**The Binomial Distribution: what it is**

* The $n+1$ probabilities $p_{0}, p_{1}, ..., p_{y}, ..., p_{n}$ of observing $y$ = $0, 1, 2, \dots , n$ 'positives'  in $n$ independent realizations of a Bernoulli random variable $Y$with probability, $\pi,$ that Y=1, and (1-$\pi$) that it is 0. The number is the sum of $n$ independen Bernoulli random variables with the same probability, such as in s.r.s of $n$ individuals.
    
* Each of the $n$ observed elements is binary (0 or 1)

* There are $2^{n}$ possible _sequences_ ... but only $n+1$ possible _values_, i.e. $0/n,\;1/n,\;\dots ,\;n/n$  can think of $y$ as sum of $n$ Bernoulli r. v.'s. [Later on, in ptractive, we will work in the same scale as  parameter. i.e., (0,1). not the  (0,n) 'count' scale.]
        
* Apart from  $n$, the probabilities $p_{0}$ to $p_{n}$ depend on only 1 parameter:
   +  the probability that a selected individual will be 'positive'  i.e., have the trait of interest
   + the proportion of 'positive' individuals in the sampled population

*  Usually we denote this (un-knowable) proportion by $\pi$ (or sometimes by the more generic $\theta$) 
   + Textbooks are not consistent (see below); we try to use Greek letters for parameters,
   + Note Miettinen's  use of UPPER-CASE letters, [e.g. $P$, $M$] for _PARAMETERS_ and lower-case letters [e.g., $p$, $m$] for _statistics_ (_estimates} of parameters_).


|   Author(s)          | PARAMETER     | Statistic    | 
|:------------------|   :----:      |   :----:     |  
|Clayton and Hills  |   $\pi$       |  $p = D/n$    |  
|Moore and McCabe, Baldi and Moore  |   $p$    |  $\hat{p} = y/n$    |
|Miettinen          |   $P$         |  $p = y/n$    |
| This book         |    $\pi$      | $p = D/n$  |

* Shorthand: $Y \sim Binomial(n, \pi)$ or $y \sim Binomial(n, \pi)$

**How it arises**

* Sample Surveys 
* Clinical Trials 
* Pilot studies
* Genetics
* Epidemiology 

**Uses**

* to make inferences about $\pi$ from observed proportion $p = y/n$.
* to make inferences in more complex situations, e.g. ...

   + Prevalence Difference: $\pi_{index.category}$ - $\pi_{reference.category}$
   
   + Risk Difference (RD): $\pi_{index.category}$ - $\pi_{reference.category}$
   
   + Risk Ratio, or its synonym Relative Risk (RR): $\frac{\pi_{index.category}}{\pi_{reference.category}}$ 
   
   + Odds Ratio (OR): $\frac{\pi_{index.category}/(1-\pi_{index.category})}{
   \pi_{reference.category}/(1-\pi_{reference.category}) }$
   
   + Trend in several $\pi$'s


**Requirements for $Y$ to have a Binomial$(n, \pi)$ distribution**

* Each element in the 'population' is 0 or 1, but we are only interested in estimating the proportion ($\pi$) of 1’s; we are not interested in individuals.

* Fixed sample size $n$.

* Elements selected at random and independent of each other; each element in population has the same probability of being sampled: i.e., we have $n$ independent Bernoulli random variables with the same expectation (statisticians say '_i.i.d_' or '_independent and identically distributed_').

*  It helps to distinguish the population values, say $Y_1$ to $Y_N$, from the $n$ sampled values $y_1$ to $y_n$.
Denote by $y_i$ the value of the $i$-th sampled element. Prob[$y_i$ = 1] is constant (it is $\pi$) across $i$.
In the [What proportion of our time do we spend indoors?](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Mean-Quantile/inside_outside.pdf) example, it is the **random/blind sampling** of the temporal and spatial patterns of 0s and 1s that **makes $y_1$ to $y_n$ independent of each other**. The $Y$'s, the elements in the population can be related to each other [e.g. there can be a peculiar spatial distribution of persons] but if elements are chosen at random, the chance that the value of the $i$-th element chosen is a 1 cannot depend on the value of $y_{i−1}$ or any other $y$: the sampling is 'blind' to the spatial location of the 1’s and 0s.


**Binomial probabilities, illustrated using a Binomial Tree**


```{r,eval=T, echo=F, fig.align="center", fig.height=8, fig.width=9, warning=FALSE, message=F,fig.cap="From 5 (independent and identically distributed) Bernoulli observations to Binomial(n=5), with the Bernoulli probability left unspecified. There are 2 to the power n possible (distinct) sequences of 0's and 1's, each with its probability. We are not interested in these 2 to the power n probabilities, but in the probability that the sample  contains y 1's and (n-y) 0's. There are only (n+1) possibilities for y, namely 0 to n. Fortunately, each of the n.choose.y sequences that lead to the same sum or count y, has the same probability. So we group the 2.to.power.n sequences into (n+1) sets, according to the sum or count. Each sequence in the set with  y 1's and (n-y) 0's has the same probability, namely  the prob.to.the.power.y times (1-prob).to.the.power.(n-y). Thus, in lieu of adding all such probabilities, we simply multiply this  probability by the number, n.choose-y -- shown in black -- of unique sequences in the set. Check: the frequencies in black add to 2.to.power.n. Nowadays, the (n+1) probabilities are easily obtained by supplying a value for the 'prob' argument in the R function dbinom(), instead of  computing the binomial coefficient n.choose-y by hand."}


par(mfrow=c(2,1),mar = rep(0.001,4) )


s =c(-1,1)
COL=c("red","blue")
p=0.4

plot(c(0,8.5),c(-1,40),col="white",xaxt="n")

y5=1:32
y4=apply(matrix(y5,2,16),2,mean)
arrows(4,rep(y4,each=2), 5, y5,
 col=rep(COL,16),
  length=0.06, angle=25)
text(4.5,0,"5th")

y3=apply(matrix(y4,2,8),2,mean)
arrows(3,rep(y3,each=2), 4, y4,
 col=rep(COL,16),
  length=0.06, angle=25)
text(3.5,0,"4th")
 
y2=apply(matrix(y3,2,4),2,mean)
arrows(2,rep(y2,each=2), 3, y3,
 col=rep(COL,8),
  length=0.06, angle=25)
text(2.5,0,"3rd")

y1=apply(matrix(y2,2,2),2,mean)
arrows(1,rep(y1,each=2), 2, y2,
 col=rep(COL,4),
  length=0.06, angle=25)
text(1.5,0,"2nd")

arrows(0,mean(y5), 1, y1,
 col=COL,
  length=0.06, angle=25)
text(0.5,0,"1st")


C=c("red","blue")
for(i in 31:0){
	r = as.integer(intToBits(y5[i])[5:1] ) 
	points(5+(1:5)/10,rep(i+1,5),col=C[r+1],
	pch=19,cex=0.5)	
}

text(0.8,38,"",adj=c(0,0))
	
	
	text(0,38,expression(
	      paste(
	      "The ", 2^n,
" possible sequences of n independent Bernoulli observations")),
	      adj=c(0,0),cex=1.25)
	
	text(5.7,36,
"With n=5, 32 possible sequences.

In the next panel, sequences leading
to the same positive:negative
(RED/blue) 'split' are grouped.

The number of sequences leading
to the same split is shown in black.

With n=5,
there are 6 possible splits

The probability of a given split
is the probability of any one 
of the sequences leading to it,
multiplied by the number of such 
sequences.",
cex=1.0,adj=c(0,1))
	
    
     text(0,35,
      expression(paste(
        "Prob[ i-th observation is BLUE, i.e. = 1 ] = ",
         pi)
      ),col="blue",adj=c(0,0.5)
    )

#######
     
plot(c(0,7.0),c(-6.7,6.5),col="white",
     xaxt="n",yaxt="n")
 for (y1 in s){
	arrows(0, 0, p, p*y1,
	  col=COL[(y1+3)/2],length=0.07,
	  angle=20, lwd = 1+ (y1==1) )
	if(y1>0) text(p/2, y1/2,
		  expression(pi),
		  adj=c(0.5,0.0),col="blue")
	if(y1<0) text(p/2, y1/2,
		  expression(1 - pi),
		  adj=c(0.5,1),col="red")
	
	
	for (y2 in s){
	  
	  	  arrows(1,y1,1+p,y1+y2*p,
	      col=COL[(y2+3)/2],
	      length=0.07,angle=20,
	      lwd = 1+ (y2==1) )
	  for (y3 in s){
	  	arrows(2,y1+y2,2+p,y1+y2+y3*p,
	      col=COL[(y3+3)/2],
	      length=0.07,angle=20,
	      lwd = 1+ (y3==1) )
	    for (y4 in s){
	  	  arrows(3,y1+y2+y3,3+p,y1+y2+y3+y4*p,
	      col=COL[(y4+3)/2],
	      length=0.07,angle=20,
	      lwd = 1+ (y4==1) )
	      for (y5 in s){
	  	   
	  	   arrows(4,y1+y2+y3+y4,4+p,y1+y2+y3+y4+y5*p,
	         col=COL[(y5+3)/2],
	         length=0.07,angle=20,
	         lwd = 1+ (y5==1) )
          } # y5
        } # y4
      } # y3
    } #y2
 } #y1

	    
    N=c(1,2,1)	  
	for(ss in 2:1){
		y = ss*1 - (2-ss)*1
		C = N[ss+1]
		text(1+(p)/2, 
		     y-0.2,expression(pi),
		     col="blue", adj=c(0.5,1))
	}
	
    N=c(1,3,3,1)	  
	for(ss in 3:1){
		y = ss*1 - (3-ss)*1
		C = N[ss+1]
		text(2+(p)/2, 
		     y-0.2,expression(pi),
		     col="blue", adj=c(0.5,1))
	}
	N=c(1,4,6,4,1)	  
	for(ss in 4:1){
		y = ss*1 - (4-ss)*1
		C = N[ss+1]
		text(3+(p)/2, 
		     y-0.2,expression(pi),
		     col="blue", adj=c(0.5,1))
	}
	N=c(1,5,10,10,5,1)	  
	for(ss in 5:1){
		y = ss*1 - (5-ss)*1
		C = N[ss+1]
		text(4+(p)/2, 
		     y-0.2,expression(pi),
		     col="blue", adj=c(0.5,1))
	}
	

	   text(0.0, 5.5,
"1,2,3, ... 10: Number of sequences that yield the
indicated split (can obtain from nCy or Pascal's Triangle).
All sequences leading to the split are equiprobable.",
	 adj=c(0,0.5),cex=1.05)
	 
	 text(6,6,"Binomial Probabilities*")
  text(6.9,-6.25,"* in R: dbinom(0:5,size=5,prob=0.xx)",
  family="mono", adj=c(1,1),,cex=1.25)
  
  text(0,0,"1", adj=c(1,0.5))

 	N=c(1,1)	  
	for(ss in 1:0){
		y = ss*1 - (1-ss)*1
		C = N[ss+1]
		text(1-(1-p)/1.2, 
		     y*(p+1)/2,toString(C),adj=c(0,0.5))
	}		
	points( (p+1)/2, (p+1)/2,
		  cex=1,pch=19,col="blue")
    points( (p+1)/2, -(p+1)/2,
		  cex=1,pch=19,col="red")
		  
		  
	N=c(1,2,1)	  
	for(s in 2:0){
		y = s*1 - (2-s)*1
		C = N[s+1]
		text(2-(1-p)/1.5, 
		     y*(p+1)/2,toString(C),adj=c(1,0.5))
	}	
		  
    points( 1+(p+1)/2, 1+(p+1)/2+p/2,
		  cex=1,pch=19,col="blue")
	points( 1+(p+1)/2, 1+(p+1)/2-p/2,
		  cex=1,pch=19,col="blue")
		  
    points( 1+(p+1)/2, p/2,
		  cex=1,pch=19,col="blue")
	points( 1+(p+1)/2, -p/2,
		  cex=1,pch=19,col="red")
		  
	points( 1+(p+1)/2, -1-(p+1)/2+p/2,
		  cex=1,pch=19,col="red")
	points( 1+(p+1)/2, -1-(p+1)/2-p/2,
		  cex=1,pch=19,col="red")
		  
	N=c(1,3,3,1)	  
	for(ss in 3:0){
		if(ss==3) COL=rep("blue",4)
		if(ss==2) COL[3]="red"
		if(ss==1) COL[2]="red"
		if(ss==0) COL[1]="red"
		y = ss*1 - (3-ss)*1
		points( 2+(p+1.1)/2      , y+0.1,
		  cex=1,pch=19,col=COL[1])
		points( 2+(p+1.1)/2 -0.08,  y-0.2,
		  cex=1,pch=19,col=COL[2])
		points( 2+(p+1.1)/2 +0.08,  y-0.2,
		  cex=1,pch=19,col=COL[3])
		
	}
	
	N=c(1,3,3,1)	  
	for(ss in 3:0){
		y = ss*1 - (3-ss)*1
		C = N[ss+1]
		text(3-(1-p)/1.5, 
		     y-0.2,toString(C),adj=c(1,0.5))
	}
	
	N=c(1,4,6,4,1)	  
	for(ss in 4:0){
		if(ss==4) COL=rep("blue",4)
		if(ss==3) COL[4]="red"
		if(ss==2) COL[3]="red"
		if(ss==1) COL[2]="red"
		if(ss==0) COL[1]="red"
		y = ss*1 - (4-ss)*1
		points( 3+(p+1.1)/2 - 0.06, y-0.2,
		  cex=1,pch=19,col=COL[1])
		points( 3+(p+1.1)/2 +0.06,  y-0.2,
		  cex=1,pch=19,col=COL[2])
		points( 3+(p+1.1)/2 -0.06,  y+0.2,
		  cex=1,pch=19,col=COL[3])
		points( 3+(p+1.1)/2 +0.06,  y+0.2,
		  cex=1,pch=19,col=COL[4])
		    
		text(4-(1-p)/1.5, 
		     y-0.2,toString(N[ss+1]),adj=c(1,0.5))
	}
	
		  
	N=c(1,5,10,10,5,1)	  
	for(ss in 5:0){
		if(ss==5) COL=rep("blue",5)
		if(ss==4) COL[5]="red"
		if(ss==3) COL[4]="red"
		if(ss==2) COL[3]="red"
		if(ss==1) COL[2]="red"
		if(ss==0) COL[1]="red"
		y = ss*1 - (5-ss)*1
		
		text(5-(1-p)/1.24, 
		     y,toString(N[ss+1]),adj=c(1,0.5))
		
		points( 4+(p+1.1)/2 - 0.12, y,
		  cex=1,pch=19,col=COL[1])
		points( 4+(p+1.1)/2 +0.12,  y,
		  cex=1,pch=19,col=COL[2])
		points( 4+(p+1.1)/2 ,  y+0.40,
		  cex=1,pch=19,col=COL[3])
		points( 4+(p+1.1)/2 ,  y-0.40,
		  cex=1,pch=19,col=COL[4])
		points( 4+(p+1.1)/2 ,  y,
		  cex=1,pch=19,col=COL[5])

		C = N[ss+1]
		text(4.75+(p+1.6)/2, y,
		  substitute(
		    paste(coef, 
		     phantom(0),
		     scriptstyle(x),
		     phantom(0)),
		    list(coef=C,y=s,rest=3-ss)
		  ),
		  adj=c(1,0.8)
		)
		
		text(4.7+(p+1.6)/2+0.4, y,
		  substitute(
		    paste( pi^y, phantom(0)),
		    list(coef=C,y=ss,rest=5-ss)
		  ),col="blue",
		  adj=c(1,0.5)
		)
		
		text(4.7+(p+1.6)/2+0.4, y,
		  substitute(
		    paste( (1-pi)^rest),
		    list(coef=C,y=ss,rest=5-ss)
		  ),col="red",
		  adj=c(0,0.5)
		)
		
	}
	
	
```

**Does the Binomial Distribution apply if... ?**


|             |      |      | 
|:------------------|   ----:      |   :----     |  
|Interested in:      |   $\pi$    |  the proportion of 16 year old girls in Quebec  protected against rubella  |
|Choose:  | $n$ = 100    | girls: 20 at random from each of 5 randomly selected schools ['cluster' sample] |
|Count | $y$ | how many of the $n$ = 100 are protected|
| | | $\bullet$ Is $y ~ \sim Binomial(n=100, \ \pi)$ ? | 
| ............. | ........... | ............................................................... |
|'SMAC':      |   $\pi$    | Chemistry Auto-analyzer with n = 18 channels. Critertion for 'positivity' set so that Prob['abnormal' result in Healthy person] = 0.03 for each of 18 chemistries tested |  
| | |  |
|Count | $y$ | (In 1 patient) how many of $n$ = 18 give abnormal result.
|
| | | $\bullet$ Is $y ~ \sim Binomial(n=18, \ \pi=0.03)$ ? [credit:  Ingelfinger textbook](https://www.amazon.ca/Biostatistics-Clinical-Medicine-Joseph-Ingelfinger/dp/0023597216) | 
| ............. | ........... | ............................................................... |
| ............. | ........... | ............................................................... |
|Sex Ratio:      |   $n=4$    | children in each family |  
| | $y$ | number of girls in family |
| | | $\bullet$ Is $y ~ \sim Binomial(n=4, \ \pi=0.49)$ ? | 
| ............. | ........... | ............................................................... |
|Interested in:      |    $\pi_u$   | proportion in 'usual' exercise classes and in |
|   |    $\pi_e$   | expt'l. exercise classes who 'stay the course' |
| Randomly| 4 | classes of |
| Allocate|25 | students each to usual course |
| | $n_u = 4 \times 25 = 100$ | |
| | | |
|  | $n_e$ = 4 | classes of |
|  |25 | students each to experimental course |
| | $n_e = 4 \times 25 =100$ | |
| | | |
|Count | $y_u$ | how many of the $n_u$ = 100 complete course|
|      | $y_e$ | how many of the $n_e$ = 100 complete course|
| | | $\bullet$ Is $y_u ~ \sim Binomial(n = 100, \ \pi_u)$ ? |
| | | $\bullet$ Is $y_e ~ \sim Binomial(n = 100, \ \pi_e)$ ? |
| ............. | ........... | ............................................................... |
|Pilot Study:      |       | To estimate proportion $\pi$ of population that is eligible and willing to participate in long-term research study, keep recruiting until obtain |
| | $y$ = 5 | who are. Have to approach $n$ to get $y$.
| | | $\bullet$ Is $y ~ \sim Binomial(n, \ \pi)$ ? | 
| ............. | ........... | ............................................................... |

**Calculating Binomial probabilities**

_Exactly_

* probability mass function (p.m.f.) :

   + formula: $Prob[y] = \  ^n C _y \ \pi^y \  (1 − \pi)^{n−y}$.
   
   + recursively: $Prob[0] = (1−\pi)^n$; \ \  $Prob[y] = \frac{n−y+1}{y} \times \frac{\pi}{1-\pi} \times Prob[y−1]$.
   
* Statistical Packages:
   + R functions `dbinom()`, `pbinom()`, `qbinom()`  
   probability mass, distribution/cdf, and quantile functions.

   + Stata function `Binomial(n,k,p)`
   
   + SAS `PROBBNML(p, n, y)` function
   
* Spreadsheet — Excel function `BINOMDIST(y, n, π, cumulative)`

* Tables: CRC; Fisher and Yates; Biometrika Tables; Documenta Geigy

_Using an approximation_

* Poisson Distribution ($n$ large; small $\pi$)

* **Normal (Gaussian) Distribution** ($n$ large or midrange $\pi$, so that the expecetd value, $n \times \pi$, is sufficiently far 'in from' the 'edges' of the scale, i.e., sufficiently  far in from 0 and from $n$, so that a Gaussian distribution doesn't flow past one of the edges. The Normal approximation is good for when you don't have access to software or Tables, e.g, on a plane, or when the internet is down, or the battery on your phone or laptop had run out, or it takes too long to boot up Windows!).  
To use the Normal approximatiom, be aware of the **scale you are working in**, .e.g., if say $n = 10$, whether the summary is a **count** or a **proportion** or a **percentage**.


|   | r.v.  | e.g.  | E  | S.D. |
|----: |  :----      |   ---:  | :--: | :---: |  
| **count**: |  $y$   |  2 | $n \times \pi$ | $\{n \times \pi \times (1-\pi) \}^{1/2}$ |
| | | | | i.e., $n^{1/2} \times \sigma_{Bernoulli}$ |
| | | | | |                                  
| **proportion**:   | $p=y/n$  | 0.2 | $\pi$ | $\{\pi \times (1-\pi) / n \}^{1/2}$ |
| | | | | i.e., $\sigma_{Bernoulli} / n^{1/2}$ |
| | | | | |
| **percentage**: | $100p\%$ | 20\% | $100 \times \pi$ | $100 \times SD[p]$ |


Exercise: deMoivre 1738


dark https://physics.stackexchange.com/questions/28563/hours-of-light-per-day-based-on-latitude-longitude-formula

https://environhealthprevmed.biomedcentral.com/articles/10.1186/s12199-017-0637-4

car sppeds https://purr.purdue.edu/publications/2671/1

Hypergeometric

Non-Central Hypergeometric

Chi-square

===

EXERCISES

ADD YOUR OWN SEARCH RESULTS to table above

Hydroxychloroquine uniformity table.

few for differences

woolf

normal

transformations


## Exercises

3. Above you saw the possible outcomes of the **6/49** game. You could also put the possibilities into this 2 x 2 table

Frequencies of numbers selected/not selected by you (rows) and drawn/not by Loto Machine (columns)

|        |       |      |      |      |      |      |  |
|-----:|-------:|-------:|-------:|-----:|-----:|:----:|:----:|:----:|
|        |       |       |     | LOTO |      |      |      |   |
|        |       |       |   Yes |      | No   |      | ALL     |    |
|        |       |       |      |      |      |      |    |
|        |       | .      | ........     | ........     |  ........    |   .   |    |
|        |  Yes  | : |   y   |      | 6-y  |  !   |  6  |    |
|  YOU   |       |       |      |      |      |      |    |
|        |  No   | : | 6 - y |      | 37+y |   :  |  43    |    |
|        |    |  .  |  ........    | ........  |  ........    |  .    |    |
|        | ALL   |  |  6   |      | 43   |      |  49  |    |

3. continued

   + Use  your searching skills to find the formal statistical name for the distribution of $Y$, which can take on the values $y$ = 0, 1, $\dots$, 6. 
   + Likewise, find the name of the `R` functions (`d??p?r` , `p??p?r`, `q??p?r` and `r??p?r`) that allow you to obtain the 7 probabilities, the 7 cumulative provabilities, the quantiles, and the `r??p?r` function that draws random values from this distribution.
   + Even with knowing what the full (numerical) probability distribution of $Y$ is, it is possible, just from the marginal totals for the 2 x 2 table, to calculate the expected value of $Y.$ Do so. (_Hint_: look up 'expected values for r x c tables')
   + Just by imagining what the full probablility distibution on the integers 0 to 6 must look like [or by looking at the material earlier on this page], come up with an approximate value of the variance of Y. [If you look up this distribution in Wikipedia, you wiull find the exact formula for it.]  
   
4. The chapter on inference possible relates  the '**Lady Tasting Tea**' story. You could put the possible outcomes of the trial into this 2 x 2 table involving 8 cups of tea, into which the milk had been poured first in 4 instances, and second in the other 4. The lady was told there were 4 of each, so she indicated the 4 into which she jusged the milk was poured 1st, and the 4 into which she judged the milk was poured 2nd.  

Frequencies of the correctness/incorrectness of the replies by the lady (rows) in relation to the truth arranged by experimenter (columns)


|        |       |      |      |      |      |      |  |
|-----:|-------:|-------:|-------:|-----:|-----:|:----:|:----:|:----:|
|        |       |       |     | TRUTH |      |      |      |   |
|        |       |       |   Milk 1st |      | Milk 2nd   |      | ALL     |    |
|        |       |       |      |      |      |      |    |
|        |       | .      | ........     | ........     |  ........    |   .   |    |
|        |  Milk 1st  | : |   y   |      | 4-y  |  :   |  4 cups  |    |
|  LADY   |       |       |      |      |      |      |    |
|        |  Milk 2nd   | : | 4 - y |      | 4+y |   :  |  4 cups    |    |
|        |    |  .  |  ........    | ........  |  ........    |  .    |    |
|        | ALL   |  |  4 cups   |      | 4 cups   |      |  8 cups  |    |

4. continued

   + Use the `R` function `d??p?r` to obtain, under the null hypothesis, the 5 probabilities for $y$.
   + Under this null, what is the probability of getting **at least 3** correct?
   + What if 12 cups (6 and 6) were used? What then would be the probability of getting **at least 5** correct? **all 6**?

5. **Saving** on the numbers of binary yests **by pooling**. When a binary blood test [one that yields a positive ('+ve') or negative ('-ve') result] gives +ve results in only a small proportion, $\pi,$ of blood samples, it may be possible to economize on the costs of testing by pooling m blood samples, according to the following procedure: (i) each blood sample is divided into two portions; one portion is kept in reserve while the other is pooled with the corresponding portions from $m$ - 1 other blood samples (ii) if the result of a single test on the pooled bloods is -ve, each of the $m$ individual blood samples are considered -ve; if the result is +ve, then the $m$ reserve bloods are individually tested.
   
   + With $m$ = 20 and $\pi$ = 0.1, calculate the expected number of tests required to determine the status of eack of the blood samples. (_Hint_: a tree diagram may help)




12. No. copies of wild type if frequency is 1/5. Mean/Var.? 



https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/phasing-penny.html

simulation

http://www2.ku.edu/~kuwpaper/2013Papers/201309.pdf

https://www-jstor-org.proxy3.library.mcgill.ca/stable/pdf/3552184.pdf?refreqid=excelsior%3A86597fff2392f0b0d698235434d88c3a


Galton's way of showing that the heights of the married couples in his dataset were virtually uncorrelated -- for computing exercise

