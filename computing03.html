<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 16 Computing Week3 | Introduction to Statistical Analysis: a regression-from-the-outset approach</title>
  <meta name="description" content="A regression-from-the-outset based approach" />
  <meta name="generator" content="bookdown 0.18.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 16 Computing Week3 | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A regression-from-the-outset based approach" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Computing Week3 | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  
  <meta name="twitter:description" content="A regression-from-the-outset based approach" />
  

<meta name="author" content="Sahir, Shirin and Jim" />


<meta name="date" content="2020-04-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="computing02.html"/>
<link rel="next" href="dalite.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">In Planning Stage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#target"><i class="fa fa-check"></i><b>0.1</b> Target</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#topicstextbooks"><i class="fa fa-check"></i><b>0.2</b> Topics/textbooks</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#regression-from-the-outset"><i class="fa fa-check"></i><b>0.3</b> Regression from the outset</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#parameters-first-data-later"><i class="fa fa-check"></i><b>0.4</b> Parameters first, data later</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#lets-switch-to-y-bar-and-drop-x-bar."><i class="fa fa-check"></i><b>0.5</b> Let’s switch to “y-bar”, and drop “x-bar”.</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#computing-from-the-outset"><i class="fa fa-check"></i><b>0.6</b> Computing from the outset</a></li>
<li class="chapter" data-level="0.7" data-path="index.html"><a href="index.html#appendix"><i class="fa fa-check"></i><b>0.7</b> Appendix:</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#goals"><i class="fa fa-check"></i><b>1.1</b> Goals</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#structure"><i class="fa fa-check"></i><b>1.2</b> Structure</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#attitudes-etc."><i class="fa fa-check"></i><b>1.3</b> Attitudes, etc….</a></li>
</ul></li>
<li class="part"><span><b>I Part I</b></span></li>
<li class="chapter" data-level="2" data-path="paras.html"><a href="paras.html"><i class="fa fa-check"></i><b>2</b> Statistical Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="paras.html"><a href="paras.html#parameters"><i class="fa fa-check"></i><b>2.1</b> Parameters</a></li>
<li class="chapter" data-level="2.2" data-path="paras.html"><a href="paras.html#parameter-contrasts"><i class="fa fa-check"></i><b>2.2</b> Parameter Contrasts</a><ul>
<li class="chapter" data-level="2.2.1" data-path="paras.html"><a href="paras.html#parameter-relations-in-numbers-and-words"><i class="fa fa-check"></i><b>2.2.1</b> Parameter relations in numbers and words</a></li>
<li class="chapter" data-level="2.2.2" data-path="paras.html"><a href="paras.html#parameter-relations-in-symbols-and-with-the-help-of-an-index-category-indicator"><i class="fa fa-check"></i><b>2.2.2</b> Parameter relations in symbols, and with the help of an index-category indicator</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="paras.html"><a href="paras.html#parameter-functions"><i class="fa fa-check"></i><b>2.3</b> Parameter functions</a></li>
<li class="chapter" data-level="2.4" data-path="paras.html"><a href="paras.html#phraseology-to-avoid"><i class="fa fa-check"></i><b>2.4</b> Phraseology to avoid</a></li>
<li class="chapter" data-level="2.5" data-path="paras.html"><a href="paras.html#summary"><i class="fa fa-check"></i><b>2.5</b> SUMMARY</a></li>
<li class="chapter" data-level="2.6" data-path="paras.html"><a href="paras.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="paras.html"><a href="paras.html#references"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html#the-bayesian-approach"><i class="fa fa-check"></i><b>3.1</b> The Bayesian Approach</a><ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#example-parameter-is-2-valued-yes-or-no"><i class="fa fa-check"></i><b>3.1.1</b> Example: parameter is 2-valued: yes or no</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference.html"><a href="inference.html#example-parameter-is-a-proportion"><i class="fa fa-check"></i><b>3.1.2</b> Example: parameter is a proportion</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference.html"><a href="inference.html#example-parameter-is-a-mean"><i class="fa fa-check"></i><b>3.1.3</b> Example: parameter is a mean</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inference.html"><a href="inference.html#frequentist-approach"><i class="fa fa-check"></i><b>3.2</b> Frequentist approach</a></li>
<li class="chapter" data-level="3.3" data-path="inference.html"><a href="inference.html#does-it-matter"><i class="fa fa-check"></i><b>3.3</b> Does it matter?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Parameter Intervals</a><ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> ‘100% confidence’ intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#more-nuanced-intervals"><i class="fa fa-check"></i><b>4.2</b> More-nuanced intervals</a></li>
<li class="chapter" data-level="4.3" data-path="CI.html"><a href="CI.html#summary-1"><i class="fa fa-check"></i><b>4.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="paraMu.html"><a href="paraMu.html"><i class="fa fa-check"></i><b>5</b> The ‘mean’ parameter <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="paraMu.html"><a href="paraMu.html#two-genres"><i class="fa fa-check"></i><b>5.1</b> Two genres</a></li>
<li class="chapter" data-level="5.2" data-path="paraMu.html"><a href="paraMu.html#fitting-these-to-data-estimating-them-from-data"><i class="fa fa-check"></i><b>5.2</b> Fitting these to data / Estimating them from data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="paraPi.html"><a href="paraPi.html"><i class="fa fa-check"></i><b>6</b> The (proportion) parameter</a><ul>
<li class="chapter" data-level="6.1" data-path="paraPi.html"><a href="paraPi.html#example-one"><i class="fa fa-check"></i><b>6.1</b> Example one</a></li>
<li class="chapter" data-level="6.2" data-path="paraPi.html"><a href="paraPi.html#example-two"><i class="fa fa-check"></i><b>6.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="paraLambda.html"><a href="paraLambda.html"><i class="fa fa-check"></i><b>7</b> The (event rate) parameter</a><ul>
<li class="chapter" data-level="7.1" data-path="paraLambda.html"><a href="paraLambda.html#etc"><i class="fa fa-check"></i><b>7.1</b> Etc</a></li>
<li class="chapter" data-level="7.2" data-path="paraLambda.html"><a href="paraLambda.html#etc-1"><i class="fa fa-check"></i><b>7.2</b> ETC</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html"><i class="fa fa-check"></i><b>8</b> Contrast: 2 mean parameters</a><ul>
<li class="chapter" data-level="8.1" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html#estimand-estimator-estimate"><i class="fa fa-check"></i><b>8.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html"><i class="fa fa-check"></i><b>9</b> Contrast: 2 proportion parameters</a><ul>
<li class="chapter" data-level="9.1" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html#estimand-estimator-estimate-1"><i class="fa fa-check"></i><b>9.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html"><i class="fa fa-check"></i><b>10</b> Contrast: 2 speed parameters</a><ul>
<li class="chapter" data-level="10.1" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html#estimand-estimator-estimate-2"><i class="fa fa-check"></i><b>10.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="part"><span><b>II Part II</b></span></li>
<li class="chapter" data-level="11" data-path="Probability.html"><a href="Probability.html"><i class="fa fa-check"></i><b>11</b> Probability</a><ul>
<li class="chapter" data-level="11.1" data-path="Probability.html"><a href="Probability.html#conditional-forwards"><i class="fa fa-check"></i><b>11.1</b> Conditional – forwards</a></li>
<li class="chapter" data-level="11.2" data-path="Probability.html"><a href="Probability.html#conditional-reverse"><i class="fa fa-check"></i><b>11.2</b> Conditional – reverse</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Distributions.html"><a href="Distributions.html"><i class="fa fa-check"></i><b>12</b> Distributions /Random Variables</a><ul>
<li class="chapter" data-level="12.1" data-path="Distributions.html"><a href="Distributions.html#gaussian-bernoulli-binomial-poisson"><i class="fa fa-check"></i><b>12.1</b> Gaussian Bernoulli-Binomial Poisson</a></li>
<li class="chapter" data-level="12.2" data-path="Distributions.html"><a href="Distributions.html#expectation-and-variance"><i class="fa fa-check"></i><b>12.2</b> Expectation and Variance</a></li>
<li class="chapter" data-level="12.3" data-path="Distributions.html"><a href="Distributions.html#functionscombinations-of-random-variables"><i class="fa fa-check"></i><b>12.3</b> Functions/combinations of random variables</a></li>
</ul></li>
<li class="part"><span><b>III Part III</b></span></li>
<li class="chapter" data-level="13" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>13</b> Mathematics</a><ul>
<li class="chapter" data-level="13.1" data-path="math.html"><a href="math.html#notation"><i class="fa fa-check"></i><b>13.1</b> Notation</a></li>
<li class="chapter" data-level="13.2" data-path="math.html"><a href="math.html#powers-logarithms-and-antilogarithms"><i class="fa fa-check"></i><b>13.2</b> Powers, Logarithms and Anti–logarithms</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="computing01.html"><a href="computing01.html"><i class="fa fa-check"></i><b>14</b> Computing Session 1</a><ul>
<li class="chapter" data-level="14.1" data-path="computing01.html"><a href="computing01.html#biological-background"><i class="fa fa-check"></i><b>14.1</b> Biological background</a></li>
<li class="chapter" data-level="14.2" data-path="computing01.html"><a href="computing01.html#statistical-task"><i class="fa fa-check"></i><b>14.2</b> Statistical Task</a><ul>
<li class="chapter" data-level="14.2.1" data-path="computing01.html"><a href="computing01.html#the-p-and-q-functions-an-orientation"><i class="fa fa-check"></i><b>14.2.1</b> The p and q functions: an orientation</a></li>
<li class="chapter" data-level="14.2.2" data-path="computing01.html"><a href="computing01.html#exercises-1"><i class="fa fa-check"></i><b>14.2.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="computing01.html"><a href="computing01.html#summary-2"><i class="fa fa-check"></i><b>14.3</b> SUMMARY</a><ul>
<li class="chapter" data-level="14.3.1" data-path="computing01.html"><a href="computing01.html#computing"><i class="fa fa-check"></i><b>14.3.1</b> Computing</a></li>
<li class="chapter" data-level="14.3.2" data-path="computing01.html"><a href="computing01.html#statistical-concepts-and-principles"><i class="fa fa-check"></i><b>14.3.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="computing02.html"><a href="computing02.html"><i class="fa fa-check"></i><b>15</b> Computing: Session No. 2</a><ul>
<li class="chapter" data-level="15.1" data-path="computing02.html"><a href="computing02.html#scientific-background"><i class="fa fa-check"></i><b>15.1</b> Scientific background</a></li>
<li class="chapter" data-level="15.2" data-path="computing02.html"><a href="computing02.html#random-variation"><i class="fa fa-check"></i><b>15.2</b> Random Variation</a><ul>
<li class="chapter" data-level="15.2.1" data-path="computing02.html"><a href="computing02.html#measurement-errors"><i class="fa fa-check"></i><b>15.2.1</b> Measurement errors</a></li>
<li class="chapter" data-level="15.2.2" data-path="computing02.html"><a href="computing02.html#biological-variation"><i class="fa fa-check"></i><b>15.2.2</b> Biological variation</a></li>
<li class="chapter" data-level="15.2.3" data-path="computing02.html"><a href="computing02.html#example-2"><i class="fa fa-check"></i><b>15.2.3</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="computing02.html"><a href="computing02.html#when-these-laws-dont-apply"><i class="fa fa-check"></i><b>15.3</b> When these Laws don’t apply</a></li>
<li class="chapter" data-level="15.4" data-path="computing02.html"><a href="computing02.html#summary-3"><i class="fa fa-check"></i><b>15.4</b> SUMMARY</a><ul>
<li class="chapter" data-level="15.4.1" data-path="computing02.html"><a href="computing02.html#computing-1"><i class="fa fa-check"></i><b>15.4.1</b> Computing</a></li>
<li class="chapter" data-level="15.4.2" data-path="computing02.html"><a href="computing02.html#statistical-concepts-and-principles-1"><i class="fa fa-check"></i><b>15.4.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="computing03.html"><a href="computing03.html"><i class="fa fa-check"></i><b>16</b> Computing Week3</a><ul>
<li class="chapter" data-level="16.1" data-path="computing03.html"><a href="computing03.html#ages-of-books"><i class="fa fa-check"></i><b>16.1</b> Ages of books</a></li>
<li class="chapter" data-level="16.2" data-path="computing03.html"><a href="computing03.html#ngrams"><i class="fa fa-check"></i><b>16.2</b> ngrams</a></li>
<li class="chapter" data-level="16.3" data-path="computing03.html"><a href="computing03.html#ice-breakup-dates"><i class="fa fa-check"></i><b>16.3</b> Ice Breakup Dates</a><ul>
<li class="chapter" data-level="16.3.1" data-path="computing03.html"><a href="computing03.html#the-2018-book-of-guesses"><i class="fa fa-check"></i><b>16.3.1</b> The 2018 Book of Guesses</a></li>
<li class="chapter" data-level="16.3.2" data-path="computing03.html"><a href="computing03.html#trends-over-the-last-100-years"><i class="fa fa-check"></i><b>16.3.2</b> Trends over the last 100 years</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="computing03.html"><a href="computing03.html#galtons-data-on-family-heights"><i class="fa fa-check"></i><b>16.4</b> Galton’s data on family heights</a></li>
<li class="chapter" data-level="16.5" data-path="computing03.html"><a href="computing03.html#temperature-perceptions"><i class="fa fa-check"></i><b>16.5</b> Temperature perceptions</a></li>
<li class="chapter" data-level="16.6" data-path="computing03.html"><a href="computing03.html#natural-history-of-prostate-cancer"><i class="fa fa-check"></i><b>16.6</b> Natural history of prostate cancer</a></li>
<li class="chapter" data-level="16.7" data-path="computing03.html"><a href="computing03.html#serial-psa-values"><i class="fa fa-check"></i><b>16.7</b> Serial PSA values</a></li>
<li class="chapter" data-level="16.8" data-path="computing03.html"><a href="computing03.html#graphics"><i class="fa fa-check"></i><b>16.8</b> Graphics</a></li>
<li class="chapter" data-level="16.9" data-path="computing03.html"><a href="computing03.html#possible-body-mass-indices"><i class="fa fa-check"></i><b>16.9</b> Possible Body Mass Indices</a></li>
<li class="chapter" data-level="16.10" data-path="computing03.html"><a href="computing03.html#galton"><i class="fa fa-check"></i><b>16.10</b> Galton</a></li>
<li class="chapter" data-level="16.11" data-path="computing03.html"><a href="computing03.html#epidemics"><i class="fa fa-check"></i><b>16.11</b> Epidemics</a></li>
<li class="chapter" data-level="16.12" data-path="computing03.html"><a href="computing03.html#duplicate-birthdays"><i class="fa fa-check"></i><b>16.12</b> Duplicate Birthdays</a></li>
<li class="chapter" data-level="16.13" data-path="computing03.html"><a href="computing03.html#lottery-payoffs"><i class="fa fa-check"></i><b>16.13</b> Lottery payoffs</a></li>
<li class="chapter" data-level="16.14" data-path="computing03.html"><a href="computing03.html#chevalier-de-méré"><i class="fa fa-check"></i><b>16.14</b> Chevalier de Méré</a></li>
<li class="chapter" data-level="16.15" data-path="computing03.html"><a href="computing03.html#detecting-a-fake-bernoulli-sequenece"><i class="fa fa-check"></i><b>16.15</b> Detecting a fake Bernoulli sequenece</a></li>
<li class="chapter" data-level="16.16" data-path="computing03.html"><a href="computing03.html#cell-occupancy"><i class="fa fa-check"></i><b>16.16</b> Cell occupancy</a></li>
<li class="chapter" data-level="16.17" data-path="computing03.html"><a href="computing03.html#life-tables"><i class="fa fa-check"></i><b>16.17</b> Life Tables</a></li>
<li class="chapter" data-level="16.18" data-path="computing03.html"><a href="computing03.html#carrier-status-genetics"><i class="fa fa-check"></i><b>16.18</b> Carrier Status (genetics)</a></li>
<li class="chapter" data-level="16.19" data-path="computing03.html"><a href="computing03.html#diagnostic-and-statistical-tests"><i class="fa fa-check"></i><b>16.19</b> Diagnostic and statistical tests</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="dalite.html"><a href="dalite.html"><i class="fa fa-check"></i><b>17</b> DALITE</a><ul>
<li class="chapter" data-level="17.1" data-path="dalite.html"><a href="dalite.html#aim"><i class="fa fa-check"></i><b>17.1</b> Aim</a></li>
<li class="chapter" data-level="17.2" data-path="dalite.html"><a href="dalite.html#how-it-works"><i class="fa fa-check"></i><b>17.2</b> How it works</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Analysis: a regression-from-the-outset approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="computing03" class="section level1">
<h1><span class="header-section-number">Chapter 16</span> Computing Week3</h1>
<p>(Probabilities, evaluated by simulation)</p>
<div id="ages-of-books" class="section level2">
<h2><span class="header-section-number">16.1</span> Ages of books</h2>
</div>
<div id="ngrams" class="section level2">
<h2><span class="header-section-number">16.2</span> ngrams</h2>
</div>
<div id="ice-breakup-dates" class="section level2">
<h2><span class="header-section-number">16.3</span> Ice Breakup Dates</h2>
<p>Here are some details on the <a href="http://www.nenanaakiceclassic.com">Nenana Ice Classic</a> More <a href="http://www.john-daly.com/nenana.htm">here</a></p>
<div id="the-2018-book-of-guesses" class="section level3">
<h3><span class="header-section-number">16.3.1</span> The 2018 Book of Guesses</h3>
<p>We are keen to establish the distribution of guesses, with the guessed times measured from midnight on December 31, 2017. Thus a guess of April 06, at 5:15 p.m. would be measured as 31 + 28 + 31 + 5 + 12/24 + (5+ 15/60)/24 = 95.71875 days into the year 2018.</p>
<p>It would be tedious to apply optical character recognition (OCR) to each of the 1210 pages in order to be able to computerize all 240,000 guesses. Instead, you are asked to reconstruct the distribution of the guesses in two more economical ways:</p>
<ul>
<li><p>By determining, for (the beginning of) each day, from April 01 to June 01 inclusive, the proportion, p, of guesses that predede that date. [ In <code>R', if p = 39.6% of the guesses were below 110 days, we would write this as pGuessDistribution(110) = 0.396. Thus, if we were dealing with the location  of a  value in a Gaussian ('normal') distribution, we would write</code>pnorm(q=110, mean = , sd = )` ] Once you have determined them, plot these 62 p’s (on the vertical axis) against the numbers of elapsed days (90-152) on the horizontal axis.</p></li>
<li><p>By determining the 1st, 2nd, … , 98th, 99th percentiles. These are specific examples of ‘quantiles’, or q’s. The q-th quantile is the value (here the elapsed number of days since the beginning of 2018) such that a proportion q of all values are below this value, and 1-q are above it. [ In <code>R', if 40% of the guesses were below 110.2 days, we would write this as qGuessDistribution(p=0.4) = 110.2 days. Thus, if we were dealing with the 40th percentile of a  Gaussian distribution with mean 130 and standard deviation 15, we would write</code>qnorm(p=0.4, mean = 130, sd = 15)`. ] Once you have determined them, plot the 99 p’s (on the vertical axis) against the 99 (elapsed) times on the horizontal axis.</p></li>
<li><p>Compare the Q<span class="math inline">\(_{25}\)</span>, Q<span class="math inline">\(_{50}\)</span>, and Q<span class="math inline">\(_{75}\)</span> obtained directly with the ones obtained by interpolation of the curve showing the results of the other method.</p></li>
<li><p>Compare the directly-obtained proportions of guesses that are before April 15, April 30, and May 15 with the ones obtained by interpolation of the curve showing the results of the other method.</p></li>
<li><p>By successive subtractions, calculate the numbers of guesses in each 1-day bin, and make a histogram of them. From them, calculate the mean, the mode, and the standard deviation.</p></li>
</ul>
<p>To measure the spread of guesses, Galton, in his vox populi (wisdom of crowds) article, began with the interquartile range (IQR), i.e. the distance between Q75 and Q25, the 3rd and 1st quartiles. In any distribution, 1/2 the values are within what was called the ’probable error (PE) of the mean; i.e., it is equally probable that a randomly selected value would be inside or outside this middle-50 interval. Today, we use standard deviation (SD) instead of probable error. In a <em>Gaussian</em> distribution, some 68% of values are within 1 SD of the mean, whereas 50% of values are within 1 PE of the mean. We can use <code>R</code> to figure out how big a PE is in a Gaussian distribution compared with a SD. By setting the SD to 1, and the eman to 0, we have</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Q75 =<span class="st"> </span><span class="kw">qnorm</span>(<span class="dt">p =</span> <span class="fl">0.75</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)
<span class="kw">round</span>(Q75,<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.67</code></pre>
<p>i.e, a PE is roughtly 2/3rds of a SD.</p>
<ul>
<li><p>Galton – convert to SD.</p></li>
<li><p>Geometric mean Amsterdam study</p></li>
<li><p>How far off was the median guess in 2018 from the actual time? Answer in days, and (with reservations stated) as a percentage?</p></li>
<li><p>Why did the experts at the country fair do so much better?</p></li>
<li><p>Where were the punters in 2019 wrt the actual ?</p></li>
</ul>
<p><a href="https://www.technologyreview.com/s/528941/forget-the-wisdom-of-crowds-neurobiologists-reveal-the-wisdom-of-the-confident/" class="uri">https://www.technologyreview.com/s/528941/forget-the-wisdom-of-crowds-neurobiologists-reveal-the-wisdom-of-the-confident/</a></p>
<p><a href="https://www.all-about-psychology.com/the-wisdom-of-crowds.html" class="uri">https://www.all-about-psychology.com/the-wisdom-of-crowds.html</a></p>
<p><a href="http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf" class="uri">http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf</a></p>
<p>[Nenana Ice Classic]</p>
<p>Tanana River</p>
</div>
<div id="trends-over-the-last-100-years" class="section level3">
<h3><span class="header-section-number">16.3.2</span> Trends over the last 100 years</h3>
<p>fill in the data since 200x.</p>
<p>cbind with …</p>
<p><a href="https://www.adn.com/alaska-news/2019/04/14/nenana-ice-classic-tripod-goes-down-setting-record-for-earliest-river-break-up/" class="uri">https://www.adn.com/alaska-news/2019/04/14/nenana-ice-classic-tripod-goes-down-setting-record-for-earliest-river-break-up/</a></p>
<p>time trends</p>
<p>morning vs afternoon ?</p>
<p>2019 extreme… how many SD’s from the line?</p>
<ul>
<li>Where were the punters in 2019 wrt the actual ?</li>
</ul>
<p>Sources:</p>
<p>1917-2003 data in textfile on course website;</p>
<p><a href="http://www.nenanaakiceclassic.com/" class="uri">http://www.nenanaakiceclassic.com/</a> for data past 2003</p>
<p>ascii (txt) and excel files with data to 2003</p>
<p>Working in teams of two …</p>
<p>*Create a dataframe containing the breakup data for the years 1917-2007. Possible ways to do so include: directly from the ascii (txt) file; from the Excel file</p>
<p>*From the decimal portion of the Julian time, use <code>R</code> to create a frequency table of the hour of the day at which the breakup occurred.</p>
<p>*From the month and day, use <code>R</code> to calculate your own version of the Julian day (and the decimal portion if you want to go further and use the hour and minute)</p>
<p>*Is there visual evidence that over the last 91 years, the breakup is occurring at an earlier date?</p>
<p>*Extract the date and ice thickness measurements for the years 1989-2007 from the website and use <code>your software of choice</code>R` to create a single dataset with the 3 variable, year, day and thickness. From this, fit a separate trendline for each year, and calculate the variability of these within-year slopes.</p>
</div>
</div>
<div id="galtons-data-on-family-heights" class="section level2">
<h2><span class="header-section-number">16.4</span> Galton’s data on family heights</h2>
<p>These data were gathered to examine the relation between heights of parents and heights of their (adult) children. They have been recently ‘uncovered’ from the Galton archives. As a first issue, for this exercise, you are also asked to see whether the parent data suggest that stature plays “a sensible part in marriage selection”.</p>
<p>For the purposes of this exercise, the parent data [see <a href="http://www.epi.mcgill.ca/hanley/galton" class="uri">http://www.epi.mcgill.ca/hanley/galton</a> ] are in a file called parents.txt , with families numbered 1-135, 136A, 136-204 ( {the heights of the adult offspring will be used in a future exercise)</p>
<p>Do the following tasks using <code>R</code></p>
<ol style="list-style-type: decimal">
<li><p>Categorize each father’s height into one of 3 bins (shortest 1/4, middle 1/2, tallest 1/4). Do likewise for mothers. Then, as Galton did [ Table III ], obtain the 2-way frequency distribution and assess whether “we may regard the married folk as picked out of the general population at haphazard”.</p></li>
<li><p>Calculate the variance Var[F] and Var[M] of the fathers’ [F] and mothers’ [M] heights respectively. Then create a new variable consisting of the sum of F and M, and calculate Var[F+M]. Comment. Galton called this a “shrewder” test than the “ruder” one he used in 1.</p></li>
<li><p>When Galton first anayzed these data in 1885-1886, Galton and Pearson hadn’t yet invented the correlation coefficient. Calculate this coefficient and see how it compares with your impressions in 1 and 2.</p></li>
</ol>
</div>
<div id="temperature-perceptions" class="section level2">
<h2><span class="header-section-number">16.5</span> Temperature perceptions</h2>
<p>Create 5 datasets from the questionnaire data on temperature perceptions etc.</p>
<ol style="list-style-type: lower-roman">
<li><p>by importing directly from the Excel file applied to .csv version of Excel file);</p></li>
<li><p>by first removing the first row (of variable names) and exporting the Excel file into a ’comma-separated-values&quot; (.csv) text file, then …</p></li>
</ol>
<p>reading the data in this .csv file via the INFILE and INPUT statements in a SAS DATA step,</p>
<p>[SAS] INFILE ‘path’ DELIMITER =“,”; INPUT ID MALE $ MD $ EXAM TEMPOUTC TEMPINC TEMPOUTF TEMPINF TEMPFEEL TIME PLACE $ ;</p>
<ol start="3" style="list-style-type: lower-roman">
<li>by reading the data in the text file temps_1.txt into</li>
</ol>
<p>the SAS dataset via the INFILE and INPUT statements. Notice that the ‘missing’ values use the SAS representation (.) for missing values.</p>
<p>or the Stata dataset using the ‘infile’ command</p>
<ol start="4" style="list-style-type: lower-roman">
<li>by reading the data in the text file temps_2.txt via [in SAS] the INFILE and INPUT statements in a DATA step or [in Stata] the ‘infix’ command.</li>
</ol>
<p>Here you will need to be careful, since ‘free-format’ will not work correctly (it is worth trying free format with this file, just to see what goes wrong!). When using the INFILE method, you can control some of the damage by using the ‘MISSOVER’ option in the INFILE statement: this keeps the INPUT statement from continuing on into the next data line in order to find the (in our example) 11 values implied by the variable list. JH uses this ‘defensive’ option in ALL of his INFILE statements.</p>
<ol start="22" style="list-style-type: lower-alpha">
<li>by cutting and pasting the contents of the text file temps_2.txt directly into the SAS or Stata program - in SASthe lines of data go immediately after the DATALINES statement, and there needs to be a line containing a semicolon to indicate the end of the data stream. In Stata, the lines of data go immediately after the infile or infix statement, and there needs to be a line containing the word ‘end’ to indicate the end of the data stream</li>
</ol>
<p>This Cut and Paste Method is NOT RECOMMENDED when the number of observations is large, as it is too all too easy to inadvertently alter the data, and the SAS/Stata porogram becomes quite long and unwieldy. It is Good Data Management Practice to separate the program statements from the data.</p>
<p>[Run [in SAS] PROC MEANS [in Stata] the ‘describe’ command, on the numerical variables, and [in SAS] PROC FREQ or [in Stata] the ‘tabulate’ command, on the non-numerical variables, to check that the 5 datasets you created contain the same information. Also, get in the habit of viewing or printing several observations and checking the entries against the ‘source’.</p>
<p>When using (i), have SAS show you the SAS statements generated by the wizard. Store these, and the DATA steps for (ii) to (v) in a single SAS program file (with suffix .sas).</p>
<p>Annotate liberally using comments:</p>
<p>in SAS, either begin with * ; or enclose with /* … */</p>
<p>in Stata ..begin the line with * or place the comment between /* and */ delimiters or begin the comment with // or begin the comment with ///</p>
<p>Q2</p>
<pre><code>Use one of these 5 datasets, and the appropriate [in SAS, PROCs (see Exploring Data under UCLA SAS Class Notes 2.0)], or [in Stata, the list comamnd, and the analyses from the Statistics menu] to</code></pre>
<ol style="list-style-type: lower-roman">
<li><p>list the names and characteristics of the variables</p></li>
<li><p>list the first 5 observations in the dataset</p></li>
<li><p>list the id # and the responses just to q3, w5 and q6, for all respondents, with respondents in the order: female MDs, male MDs, female non-MDs, male non-MDs. Indicate the [sub-]statement that is required to reverse this order.</p></li>
<li><p>create a 2-way frequency table, showing the frequencies of respondents in each of the 2 (MD nonMD) x 2 (male female) = 4 ‘cells’ (one defintion of an epidemiologist is ‘an MD broken down by age and sex’). Turn off all the extra printed output, so that the table just has the cell frequencies and the row and column totals.</p></li>
<li><p>compare the mean and median attitude to exams in MDs vs. non-MDs (hint: in SAS, the CLASS statement may help). Get SAS/Stata to limit the output to just the ‘n’, the min, the max, the mean and the median for each subgroup. And try to also get it to limit the number of decimal places of output (in SAS the MAXDEC option is implememnted in some procedures, but as far as JH can determine not in all)</p></li>
<li><p>compare the mean temperature perceptions (q6) of male and female respondents</p></li>
<li><p>[in SAS] create a low-res (‘typewriter’ resolution) scatterplot of the responses to q5 (vertical axis) vs. q4 (horizonatal axis), using a plotting symbol that shows whether the responsdent is a male or a female. If we have not covered how to show this ‘3rd dimension’, look at the ONLINE Documentation file {the guide for most of the procedures covered in this set of exercises is in the Base SAS Procedures Guide; other procedures are in sthe more advanced ‘STAT’ module}. You can specify the variable whose values are to mark each point on the plot. See PLOT statement in PROC PLOT, and the example with variables height weight and gender. [in Stata] use the (automatically hi-res) graphics capabilities available from the ‘Graphics’ menu</p></li>
</ol>
<p>[if SAS] Put all of the programs for Q1, and all of these program steps and output for Q2 in a single .txt file (JH will use a mono-spaced font such as Courier to view it – that way the alignment should be OK), with PROC statements interleaved with output, and a helpful 2-line title (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line (that way, lines won’t wrap-around when JH views the material).</p>
<p>[if Stata] paste the results and graphics into Word.</p>
<p>NOTE: To be fair to SAS, it CAN produce decent (and even some publication-quality) graphics. See <a href="http://www.ats.ucla.edu/stat/sas/topics/graphics.htm" class="uri">http://www.ats.ucla.edu/stat/sas/topics/graphics.htm</a></p>
<p>Then submit the text file electronically (i.e., by email) to JH by 9 am on Monday October 2.</p>
</div>
<div id="natural-history-of-prostate-cancer" class="section level2">
<h2><span class="header-section-number">16.6</span> Natural history of prostate cancer</h2>
<p>Q1</p>
<p>The following data items are from an investigation into the natural history of (untreated) prostate cancer [ report (.pdf) by Albertsen Hanley Gleason and Barry in JAMA in September 1998 ].</p>
<p>id, dates of birth and diagnosis, Gleason score, date of last contact, status (1=dead, 0=alive), and – if dead – cause of death (see 2b below). data file (.txt) for a random 1/2 of the 767 patients</p>
<ol style="list-style-type: decimal">
<li><p>Compute the distribution of age at diagnosis (5-year intervals) and year of diagnosis (5 year intervals). Also compute the mean and median ages at diagnosis.</p></li>
<li><p>For each of the 20 cells in Table 2 (5 Gleason score categories x 4 age-at-dx categories), compute the</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>number of man-years (M-Y) of observation</p></li>
<li><p>number of deaths from prostate cancer(1), other causes(2), unknown causes(3)</p></li>
<li><p>prostate cancer(1) death rate [ deaths per 100 M-Y ]</p></li>
<li><p>proportion who survived at least 15 years.</p></li>
</ol>
<p>For a and b you can use the ‘sum’ option in PROC means; ie PROC MEANS data = … SUM; VAR vars you want to sum; BY the 2 variables that form the cross-classification. Also think of a count as a sum of 0s and 1s. For c (to avoid having to compute 20 rates by hand), you can ‘pipe’ i.e. re-direct the sums to a new sas datafile, where you can then divide one by other to get (20) rates. Use OUTPUT OUT = …. SUM= …names for two sums;</p>
<ol start="3" style="list-style-type: decimal">
<li><p>On a single graph, plot the 5 Kaplan-Meier survival curves, one for each of the 5 Gleason score categories (PROC LIFETEST .. Online help is under the SAS STAT module, or see <a href="http://www.ats.ucla.edu/stat/sas/seminars/sas_survival/default.htm" class="uri">http://www.ats.ucla.edu/stat/sas/seminars/sas_survival/default.htm</a>. For Stata, see <a href="http://www.ats.ucla.edu/stat/stata/seminars/stata_survival/default.htm" class="uri">http://www.ats.ucla.edu/stat/stata/seminars/stata_survival/default.htm</a>.</p></li>
<li><p>[OPTIONAL] In order to compare the death rates with those of U.S. men of the same age, for each combination of calendar year period (1970-1974, 1975-1979, …, 1994-1999) and 5 year age-interval (55-59, 60-64, … obtain</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>the number of man-years of follow-up and</p></li>
<li><p>the number of deaths.</p></li>
</ol>
<p>Do so by creating, from the record for each man, as many separate observations as the number of 5yr x 5yr “squares” that the man traverses diagonally through the Lexis diagram [ use the OUTPUT statement within the DATA step]. Then use PROC MEANS to aggregate the M-Y and deaths in each square. If you get stuck, here is some SAS code that does this, or see the algorithm given in Breslow and Day, Volume II, page ___</p>
<p>Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday Nov 7.</p>
</div>
<div id="serial-psa-values" class="section level2">
<h2><span class="header-section-number">16.7</span> Serial PSA values</h2>
<p>Q1</p>
<p>These two files contain PSA values [pre-] and [post-] treatment of prostate cancer *.</p>
<ol style="list-style-type: lower-alpha">
<li>Create a ‘wide’ PSA file of 25 log-base-2 PSA values per man (some will be missing, if PSA not measured 25 times). Print some excerpts.</li>
</ol>
<p>(b)From the dataset created in (a), create a long file, with just the observations containing the non-missing log-base-2 PSA values [OUTPUT statement in DATA step]. Print and plot some excerpts.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>From the dataset created in (b), create a wide file [ RETAIN, first. and last. helpful here; or use PROC TRANSPOSE ]. Print some excerpts.</li>
</ol>
<ul>
<li>The order of the variables is given in this sas program . Some of the code in the program may also be of help.</li>
</ul>
<p>Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday Nov 14.</p>
</div>
<div id="graphics" class="section level2">
<h2><span class="header-section-number">16.8</span> Graphics</h2>
<p>1a</p>
<p>Re-produce (or if you think you can, improve on) three of the graphs shown in “Examples of graphs from Medical Journals.” These examples are in a pdf file on the main page. Use Excel for at least one of them, and R/Stata/SAS for at least one other. Do not go to extraordinary lengths to make them exactly like those shown – the authors, or the journals themselves, may have used more specialized graphics software. You may wish to annotate them by making (and sharing with us) notes on those steps/options that were not immediately obvious and that took you some effort to figure out. Insert all three into a single electronic document.</p>
<p>1b</p>
<p>Browse some medical and epidemiologic journals and some magazines and newspapers published in the last 12 months, Identify the statistical graph you think is the worst, and the one you think is the best. Tell us how many graphs you looked at, and why you chose the two you did. If you find a helpful online guide or textbook on how to make good statistical graphs, please share the reference with us. [The bios601 site <a href="http://www.epi.mcgill.ca/hanley/bios601/DescriptiveStatistics/" class="uri">http://www.epi.mcgill.ca/hanley/bios601/DescriptiveStatistics/</a> has a link to the Textbook by Cleveland and the book “R Graphics” by Paul Murrell.</p>
<p>If possible, electronically paste the graphs into the same electronic file you are using for 1a.</p>
<p>2</p>
<p>[OPTIONAL] The main page has a link to a lifetable workbook containing three sheets. Note that the ‘lifetable’ sheet in this workbook is used to calculate an abridged current life table based on the 1960 U.S. data. Use this sheet as a guideline, and create a current life-table (‘complete’, i.e., with 1-year age-intervals) for Canadian males, using the male population sizes, and numbers of deaths, by age, Canada 2001. [The calculations in columns O to W of the lifetable sheet are not relevant for this exercise]. Details on the elements of, and the construction of current lifetables can be found in the chapters (on website) from the textbooks by Bradford Hill and Selvin, and in the technical notes provided by the US National Center for Health Statistics in connection with US Lifetable 2000. See also the FAQ for 613 from 2005. The fact that the template is for an abridged life table, with mostly 5-year intervals, whereas the task is to construct a full lifetable with 1 year intervals, caused some people problems last year.. they realized something was wrong when the life expectancy values were way off!</p>
<p>Since this is an exercise, and not a calculation for an insurance company that wants to have 4 sig. decimal places, don’t overly fuss about what values of ‘a’ you use for the early years.. they don’t influence the calculations THAT much: If you try different sets of values (such as 0.1 in first year and 0.5 thereafter) you will not find a big impact. But don’t take my word for it .. the beauty of a spreadsheet is that you can quickly see the consequences of different assumptions or ‘what ifs’.</p>
<p>[In practice, in order not to be unduly influenced by mortality rates in a single calendar year (e.g. one that had a very bad influenza season), current lifetables are usually based on several years of mortality data. Otherwise, or if they are based on a small population, the quantities derived from them will exhibit considerable random fluctuations from year to year ]</p>
<p>Once you have completed the table, use the charting facilities in Excel to plot the survival curve for the hypothetical (fictitious) male ‘cohort’ represented by the current lifetable.</p>
<p>On a separate graph, use two histograms to show the distributions of the ages at death (i) for this hypothetical male ‘cohort’ and (ii) those males who died in 2001. To make it easy to compare them, superimpose the histograms or put them ‘side by side’ or ‘back to back’ within the same graph. Explain why the two differ in shape and location. Calculate/derive (and include them somewhere on the spreadsheet) the median and mean age at death in the hypothetical cohort and the corresponding statistics for the actual deaths in 2001.</p>
</div>
<div id="possible-body-mass-indices" class="section level2">
<h2><span class="header-section-number">16.9</span> Possible Body Mass Indices</h2>
<p>This exercise investigates different definitions of Body Mass Index (BMI).</p>
<p>BACKGROUND: With weight measured in Kilograms, and height in metres, BMI is usually defined as weight divided by the SQUARE of height, i.e., BMI = Wt / (Height*Height), or BMI = Wt/(height<strong>2) using, as SAS and several other programming languages do, the symbol </strong> for ‘raised to the power of’. [ NB: Excel uses ^ to denote this ]</p>
<p>What’s special about the power of 2? Why not a power of 1 i.e., Weight/height?</p>
<p>Why not 3, i.e., Weight/*(height<strong>3) ? Why not 2.5 i.e. Weight/(height</strong>2.5)?</p>
<p>One of the statistical aims of a transformation of weight and height to BMI is that BMI be statistically less correlated with height, thereby separating height and height into two more useful components height and BMI. For example in predicting lung function (e.g. FEV1), it makes more sense to use height and BMI than height and weight, since weight has 2 components in it – it is partly height and partly BMI. Presumably, one would choose the power which minimizes the correlation.</p>
<p>The task in this project is to investigate the influence of the power of height used in the ratio, and to see if the pattern of correlations with power is stable over different settings (datasets).</p>
<p>DATA: To do this, use 2 of the 6 datasets on the 678 webpage: [usernane is c678 and p w is H**<em>J</em>44 ]</p>
<ul>
<li>Children aged 11-16 Alberta 1985 (under ‘Datasets’)</li>
<li>18 year olds in Berkeley longitudinal study, born 1928/29 (under ‘Datasets’)</li>
<li>Dataset on bodyfat – 252 men (see documentation) (under ‘Datasets’)</li>
<li>Pulse Rates before and after Exercise – Australian undergraduates in 1990’s (under ‘Projects’)</li>
<li>Miss America dataset 1921-2000 (under ‘Resources’)</li>
<li>Playboy dataset 1929-2000 (under ‘Resources’)</li>
</ul>
<p>METHODS: First create each of the two SAS datasets, and if height and weight are not already in metres and Kg, convert them to these units. Drop any irrelevant variables. Inside each dataset, create a variable giving the source of the data (we will merge the two – and eventually all six– datasets, so we need to be able to tell which one each observation came from).</p>
<p>Combine the two datasets, i.e. ‘stack’ them one above the other in a single dataset. Print out some excerpts.</p>
<p>For each subject in the combined dataset, create 5 versions of &lt;<BMI> using the powers 1, 1.5, 2, 2.5 and 3.</p>
<p>Calculate the correlation between the ‘BMI’ obtained with each of these powers, and height. Do this separately for the observations from the two different sources (the BY statement should help here).</p>
<p>Report your CONCLUSIONS.</p>
</div>
<div id="galton" class="section level2">
<h2><span class="header-section-number">16.10</span> Galton</h2>
<p>The objective of this exercise is to examine the relation between heights of parents and heights of their (adult) children, using recently ‘uncovered’ data from the Galton archives, You are asked to assess if Galton’s way of dealing with the fact that heights of males and females are quite different produces sharper correlations than we would obtain using ‘modern’ methods of dealing with this fact. As side issues, you are also asked to see whether the data suggest that stature plays “a sensible part in marriage selection” and to comment on the correlations of the heights in the 4 {father,son}, {father,daughter}, {mother,son} and {mother,daughter} pairings.</p>
<p>BACKGROUND: Galton ‘transmuted’ female heights into their ‘male-equivalents’ by multiplying them by 1.08, and then using a single combined ‘uni-sex’ dataset of 900-something offspring and their parents. While some modern-day anayysts would simply calculate separate correlations for the male and female offspring (and then average the two correlations, as in a meta-analysis), most would use the combined dataset but ‘partial out’ the male-females differences using a multivariable analysis procedure. The various multivariable procedures in effect create a unisex dataset by adding a fixed number of inches to each female’s height (or, equivalently, in the words of one of our female PhD students, by ‘cutting the men down to size’). JH was impressed by the more elegant ‘proportional scaling’ in the ‘multiplicative model’ used by Galton, compared with the ‘just use the additive models most readiliy available in the software’ attitude that is common today. In 2001, he located the raw (untransmuted) data that allows us to compare the two approaches.</p>
<p>DATA: For the purposes of this exercise, the data [see <a href="http://www.epi.mcgill.ca/hanley/galton" class="uri">http://www.epi.mcgill.ca/hanley/galton</a> ] are in two separate files:</p>
<ul>
<li>the heights# of 205 sets of parents ( parents.txt ) with families numbered 1-135, 136A, 136-204</li>
<li><p>the heights# of their 900-something* children ( offspring.txt ) with families numbered as above</p></li>
<li><p>The data on eight families are deliberately omitted, to entice the scholar in you to get into the habit of looking at (and even double checking) the original data. Since here we are more interested in the computing part in this course, and because time is short, ignore this invitation to inspect the data – we already had a look at them in class. In practice, we often add in ‘missing data’ later, as there are always some problem cases, or lab tests that have to be repeated, or values that need to be checked, or subjects who didn’t get measured at the same time as others etc.. JH’s habit is to make the additions in the ‘source’ file (.txt or .xls or whatever) and re-run the entire SAS DATA step(s) to create the updated SAS dataset (temporary or permanent). If the existing SAS datset is already large, and took a lot of time to create, you might consider creating a small dataset with the new observations, and then stacking (using SE) the new one under the existing one – in a new file. SAS has fancier ways too, and others may do things differently!</p></li>
</ul>
<p>If your connection is too slow to view the photo of the first page of the Notebook, the title reads</p>
<p>FAMILY HEIGHTS (add 60 inches to every entry in the Table)</p>
<p>METHODS/RESULTS/COMMENTS:</p>
<ol style="list-style-type: decimal">
<li><p>Categorize each father’s height into one of 3 subgroups (shortest 1/4, middle 1/2, tallest 1/4). Do likewise for mothers. Then, as Galton did [ Table III ], obtain the 2-way frequency distribution and assess whether “we may regard the married fold as picked out of the general population at haphazard”.</p></li>
<li><p>Calculate the variance Var[F] and Var[M] of the fathers’ [F] and mothers’ [M] heights respectively. Then create a new variable consisting of the sum of F and M, and calculate Var[F+M]. Comment. Galton called this a “shrewder” test than the “ruder” one he used in 1. ( statistic-keyword VAR in PROC MEANS)</p></li>
<li><p>When Galton first anayzed these data in 1885-1886, Galton and Pearson hadn’t yet invented the CORRelation coefficient. Calculate this coefficient and see how it compares with your impressions in 1 and 2.</p></li>
<li><p>Create two versions of the transmuted mother’s heights, one using Galton’s and one using the modern-day (lazy-person’s, blackbox?) additive scaling [for the latter, use the observed difference in the average heights of fathers and mothers, which you can get by e.g., running PROC MEANS on the offspring dataset, either BY gender, or using gender as a CLASS variable]. In which version of the transmuted mothers’ heights is their SD more simlar to the SD of the fathers? ( statistic-keyword STD in PROC MEANS)</p></li>
<li><p>Create the two corresponding versions of what Galton called the ‘mid-parent’ (ie the average of the height of the father and the height of the transmuted mother). Take mid-point to mean the half-way point (so in this case the average of the two)</p></li>
<li><p>Create the corresponding two versions (additive and multiplicative scaling) of the offspring heights (note than sons’ heights remain ‘as is’). Address again, but now for daughters vs sons, the question raised at the end of 4.</p></li>
<li><p>Merge the parental and offspring datasets created in steps 4 and 6, taking care to have the correct parents matched with each offspring (this is called a 1:many merge).</p></li>
<li><p>Using the versions based on 1.08, round the offspring and mid-parent heighs to the nearest inch (or use the FLOOR function to just keep the integer part of the mid-parent height –you need not be as fussy as Galton was about the groupings of the mid-parent heights), and obtain a 2-way frequency distribution similar to that obtained by Galton [ Table I ]. Note that, opposite to we might do today, Galton put the parents on the vertical, and the offspring on the horizontal axis. ( The MOD INT FLOOR CEIL and ROUND functions can help you map observations into ‘bins’ ; we will later see a way to do so using loops)</p></li>
<li><p>Galton called the offspring in the same row of his table a ‘filial array’. Find the median height for each filial array, and plot it, as Galton did, against the midpoint of the interval containing their midparent – you should have one datapoint for each array<em>. Put the mid-parent values on the vertical, and the offspring on the horizontal axis. By eye, estimate the slope of the line of best fit to the datapoints. Mark your fitted line by ‘manually’ inserting two markers at the opposite corners of the plot. Does the slope of your fitted line agree with Galton’s summary of the degree of “regression to mediocrity”? [ Plate IX ] </em>Note that Galton used datapoints for just 9 filial arrays, choosing to omit those in the bottom and top rows (those with the very shortest and the very tallest parents) because the data in these arrays were sparse. ( By using the binned parental height in the CLASS statement in PROC MEANS or PROC UNIVARIATE, directing the output to a new SAS dataset, and applying PROC PLOT to this new dataset, you can avoid having to do the plotting manually See more on this in the FAQ)</p></li>
<li><p>Plot the individual unisex offspring heights (daughters additively transmuted) versus the mid-parent height (mothers transmuted). OVERLAY on it, with a different plotting symbol, the corresponding plot involving the multiplicatively transmuted offspring values (on the parent-axis, stay with Galton’s definition of a midparent). (see FAQ) Compare the two, and have a look at Galton’s fitted ellipse, corresponding to a bivariate normal distribution [ Plate X ]) {here, again, we would be more likely to plot the parents’ heights on the horizontal, and the offspring heights on the vertical axis}.</p></li>
<li><p>For each of the following ‘offspring vs. mid-parent’ correlations, use the ‘mid-parent’ obtained using Galton’s multiplicative method. Calculate (a) the 2 correlations for the 2 unisex versions of the offspring data (b) the sex-specific correlations (i.e., daughters and sons separately) and (c) the single parent-offspring correlation, based on all offspring combined, and their untransmuted heights, ignoring the sex of the offspring. Comment on the correlations obtained, and on the instances where there are big disparities between them. [ a PLOT, with separate plotting symbols for sons and daughters, might help in the case of (c) ]</p></li>
<li><p>Calculate the 4 correlations (i) father,son (ii) father,daughter, (iii) mother,son and (iv) mother,daughter. Comment on the pattern, and on why you think it turned out this way.</p></li>
</ol>
<p>Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday October 30.</p>
</div>
<div id="epidemics" class="section level2">
<h2><span class="header-section-number">16.11</span> Epidemics</h2>
</div>
<div id="duplicate-birthdays" class="section level2">
<h2><span class="header-section-number">16.12</span> Duplicate Birthdays</h2>
</div>
<div id="lottery-payoffs" class="section level2">
<h2><span class="header-section-number">16.13</span> Lottery payoffs</h2>
</div>
<div id="chevalier-de-méré" class="section level2">
<h2><span class="header-section-number">16.14</span> Chevalier de Méré</h2>
</div>
<div id="detecting-a-fake-bernoulli-sequenece" class="section level2">
<h2><span class="header-section-number">16.15</span> Detecting a fake Bernoulli sequenece</h2>
</div>
<div id="cell-occupancy" class="section level2">
<h2><span class="header-section-number">16.16</span> Cell occupancy</h2>
</div>
<div id="life-tables" class="section level2">
<h2><span class="header-section-number">16.17</span> Life Tables</h2>
</div>
<div id="carrier-status-genetics" class="section level2">
<h2><span class="header-section-number">16.18</span> Carrier Status (genetics)</h2>
</div>
<div id="diagnostic-and-statistical-tests" class="section level2">
<h2><span class="header-section-number">16.19</span> Diagnostic and statistical tests</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="computing02.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dalite.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/33-computing03.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["statbook.pdf", "statbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
