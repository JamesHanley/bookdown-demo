# Statistical Inference {#inference}

Google gives the following defintion

>The theory, methods, and practice of forming judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling.

The Oxford English Dictionary defines it as

>The drawing of inferences about a population based on data taken from a sample of that population; an inference drawn in this way; the branch of statistics concerned with this procedure.

We would add to these numerical statements about unknown (and unknowable) constants, as well as the mechanism or process that generated the limited data you got/get to observe.

Some example parameters -- some scientific, some more personal or particularistic -- include

* Whether
   + a potential hemophilia carrier is in fact a carrier
   + a particular email is malicious
   + a person committed the crime they are accused of
   + a person has been infected with a certain virus
   
* The proportion of 
   + thumbtacks that land on their back when tossed 
   + your time that you are being productive
   + the earth's surface that is covered by water
   + your driving time that you are on the phone
   + your time (over the entire year) that you spend inside
   + patients whose disease would respond to a medication
   + people who would volunteer for a demanding survey or long-term research study

* The numerical value for
   + the density of the Earth,relative to water
   + the age of a person whom you have just met
   + your cholesterol level
   + the mean depth of the ocean
   + the 20th percentile of the depths of the ocean
   + the median age of a population

To address the uncertainties involved in the judgements/inferences, some use of probabilities is required.



In their preamble to their chapter on inferemce, Clayton and Hills tell us that

>There are **two radically different approaches** to associating a probability with a range of parameter values, reflecting a deep philosophical division amongst mathematicians and scientists about the nature of probability. We shall start with the more orthodox view within biomedical science.

Clayton and Hills completed  their book in 1993.
Since then, propelled by greater  computer power, and by people like
Clayton's Cambridge colleague David Spiegelhalter, whose book we will start with, the Bayesian approach to 'associating a  probability with a range of parameter values' has become more common. It has not yet reached the status of 'customary or conventional, as a means or method; established.' that the dictionaries give as the meaning of orthodox. In any case, we should not take Clayton and Hills'  use of the phrase 'more orthodox' to describe the _frequentist_ approach to mean 
that the Bayesian approach `does not  conform to the approved form of analysis' or is in some sense `wrong.' 


The first-established of the two  'schools' (or 'churches') of statistical inference** makes **direct probabilistic statements about the possible parameter values**. This approach goes back at least as far as the mid-1700's essay 'A method of calculating the exact probability of all conclusions based on induction'; ironically the author was a  Presbyterian minister.

The developments since then are  nicely told in the very readable book [_The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy_ ](https://www.amazon.com/Theory-That-Would-Not-Die-ebook/dp/B0050QB3EQ) by Sharon Bertsch McGrayne, and in her Microsoft lecture

<iframe width="720" height="405" src="https://www.youtube.com/embed/2o-_BGqYM5U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
and her Google lecture
<iframe width="720" height="405" src="https://www.youtube.com/embed/8oD6eBkjF9o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


Maybe, by calling it the more 'orthodox',  all that Clayton and Hills mean is that the frequentist approach is more popular method today. It got a slow start, and dates from the early 20th century. (In one of our sampling exercises, we will try to determine the relative frequencies of the two approaches in the epidemiology and medical literature).

Interestingly, if you use 
[Google Books	Ngram Viewer](
https://books.google.com/ngrams/graph?content=frequentist+approach%2C+bayesian+approach&case_insensitive=on&year_start=1800&year_end=2008&corpus=15&smoothing=3&share=&direct_url=t4%3B%2Cfrequentist%20approach%3B%2Cc0%3B%2Cs0%3B%3Bfrequentist%20approach%3B%2Cc0%3B%3BFrequentist%20approach%3B%2Cc0%3B%3BFrequentist%20Approach%3B%2Cc0%3B.t4%3B%2Cbayesian%20approach%3B%2Cc0%3B%2Cs0%3B%3BBayesian%20approach%3B%2Cc0%3B%3BBayesian%20Approach%3B%2Cc0%3B%3BBAYESIAN%20APPROACH%3B%2Cc0%3B%3Bbayesian%20approach%3B%2Cc0) you get a different sense. Maybe this is because the majority don't need to justify the methods they use!

Frequentist statements are  **indirect**. They are (conditional) probabilistic statements about the **data** and about the performance of the **procedure** used to bracket the parameter values. A variant on it ranks the various  possible parameter values according to how probable the observed data would be under each of these, but does not make direct probabilistic statements about the parameter values themselves. Because it is indirect, conditional, the results are often interpreted incorrectly. 

We begin with the direct method, one that studies tell us we are born with, and use throughout our lives, both consciously and subconsciously, to continue to learn/update.

> When we learn a new motor skill, such as playing an approaching
tennis ball, both our sensors and the task possess variability. 
[...] We show that subjects internally represent both the statistical
distribution of the task and their sensory uncertainty,
combining them in a manner consistent with a performance-optimizing bayesian process. The central nervous system therefore employs probabilistic models during sensorimotor learning. [Bayesian integration in sensorimotor learning](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/nature02169.pdf)

leading to this New York Times headline

> [Subconsciously, Athletes May Play Like Statisticians](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/AthletesStatisticians.pdf)
  

## The Bayesian Approach

to probability statements concerning parameter values.

This paragraph is taken from this chapter
[An Overview of the Bayesian Approach](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/an%20overview%20of%20the%20Bayesian%20approach.pdf)
in the book Bayesian Approaches to Clinical Trials and Health-Care Evaluation by David Speigelhalter et al, describes it well:

> The standard interpretation of probability describes long-run properties of repeated random events (Section 2.1.1). This is known as the frequency interpretation of probability, and standard statistical methods are sometimes referred to as 'frequentist'. In contrast, the **Bayesian approach** rests on an essentially 'subjective' interpretation of probability, which is allowed to express generic uncertainty or 'degree of belief' about any unknown but potentially observable quantity, whether or not it is one of a number of repeatable experiments. For example, it is quite reasonable from a subjective perspective to think of a probability of the event 'Earth will be openly visited by aliens in the next ten years', whereas it may be difficult to interpret this potential event as part of a 'long-run' series. Methods of assessing subjective probabilities and probability distributions will be discussed in Section 5.2.

Section 3.1 SUBJECTIVITY AND CONTEXT emphasizes that
'the vital point of the subjective interpretation is that **Your probability** for an event is a property of **Your** relationship to that event, and not an objective property of the event itself.' Moreover, 'pedantically speaking, one should always refer to probabilities **for** events rather than probabilities **of** events, and the **conditioning context**  used in Section 2.1.1 **includes the observer and all their background knowledge and assumptions.**'

That there is 'always a context' goes along with what we read in Alan Turing's recently de-classified essay The Applications of Probability to Cryptography. Under section 1.2 ('Meaning of probability and odds') he starts out

>I shall not attempt to give a systematic account of the theory of probability, but it may be worth while to define shortly probability and odds. The probability of an event **on certain evidence** is the proportion of cases in which that event may be expected to happen given that evidence. For instance if it is known the 20% of men live to the age of 70, then knowing of Hitler only Hitler is a man we can say that the probability of Hitler living to the age of 70 is 0.2. Suppose that we know that Hitler is now of age 52 the probability will be quite different, say 0.5, because 50% of men of 52 live to 70.

**Not all context is subjective**. We will start with a context where the initial (starting out, pre-new-data) probability is **objective**.

Just before we do, we include this passage from Clayton and Hills, in subchapter 10.2 Subjective probablity, which they denote as optional material.

> The second approach to the problem of assigning a probability to a range of
values for a parameter is based on the philosophical position that probability is a subjective measure of ignorance. The investigator uses probability
as a measure of subjective degree of belief in the different values which the
parameter might take. With this view it is perfectly logical to say that
there is a probability of 0.9 that the parameter lies within a stated range.  
Before observing the data, the investigator will have certain beliefs
about the parameter value and these can be measured by a priori probabilities. Because they are subjective every scientist would be permitted
to give different probabilities to different parameter values. However, the
idea of scientific objectivity is not completely rejected. In this approach
objectivity lies in the rule used to modify the a priori probabilities in the
light of the data from the study. This is Bayes' rule and statisticians who
take this philosophical position call themselves Bayesians.  
Bayes' rule was described in Chapter 2, where it was used to calcu-
late the probabilities of exposure given outcome from the probabilities of
outcome given exposure. Once we are prepared to assign probabilities to
parameter values, Bayes' rule can be used to calculate the probability of
each value of a parameter ($\theta$) given the data, from the probability of the data given the value of the parameter.  
The argument is illustrated by two tree diagrams. Fig. 10.2 illustrates
the direction in which probabilities are specified in the statistical model
— given the choice of the value of the parameter, $\theta$, the model tells us
the probability of the data. The probability of any particular combination
of data and parameter value is then the product of the probability of the
parameter value and the probability of data given the parameter value. In
this product, the first term, Pr($\theta$), represents the a priori degree of belief for the value of $\theta$ and the second term, Pr(Data | $\theta$ ), is the likelihood. Fig. 10.3 reverses the conditioning argument, and expresses the joint probabilityas the product of the overall probability of the data multiplied by the probability of the parameter given the data. This latter term, Pr( $\theta$ | Data), represents the posterior degree of belief in the parameter value once the data have been observed. Since the joint probability of data and parameter value is the same no matter which way we argue,
so that
$$Pr(\theta) \times Pr(Data | \theta) = Pr(Data) \times Pr(\theta | Data),$$
so that
$$Pr(\theta | Data) = \frac{Pr(\theta) \times Pr(Data | \theta)}{Pr(Data)}$$
Thus elementary probability theory tells us how prior beliefs about the
value of a parameter should be modified after the observation of data.

Their 2 figures nicely show that the difference is in the directionality or conditioning, i.e. is the object $$Pr(\theta | data) \ \ \ OR \ \ \   Pr(data | \theta) \ \ ?$$  


```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F, fig.cap="From Chapter 10.2 of Clayton and Hills"}

knitr::include_graphics("images/ClaytonHillsFig102103.png",dpi=96)

```


**Without getting into the details of the calculations, we will apply this approach to the  first example in each of the parameter genres listed above. The point is to illustrate how direct and unambigous the answer is in each case.**

### Example: parameter is 2-valued: yes  or no

In the first genre, the parameter is personal or particular. In each of the examples, the true state is binary. The potential hemophilia carrier **is**  a hemophia carrier or **is not**; the particular email is either malicious or is not; the person in question either committed the crime ot did not. So, there are just two possible  parameter values: yes or no, **is** or **is not**.

From the outset, just like in Turing's example, there is a given context. For example, suppose a woman's brother is known to have haemophilia.

> hemophilia: a medical condition in which the ability of the blood to clot is severely reduced, causing the sufferer to bleed severely from even a slight injury. The condition is typically caused by a hereditary lack of a coagulation factor, caused by a mutation in one of the genes located on the X chromosome. - Google

Just knowing this, the  probability that the woman is a hemophilia carrier is 50% or 1/2.

Today, genetic testing of the carrier can help determine whether the woman is a carrier. But when JH first taught 607, the only time to learn more about her carrier status (and move her probability to  1, or towards 0) was after the births of her sons: their status was knowable virtually immedediately. 

If it is determined that the first son has hemophilia, it establishes that she IS a carrier, thereby moving the probability up to 1. If he was not, it moves the probability down  to 1/3: in other words, among 'women like her', i.e, other potential carriers who also have had 1 son who turned out to be Normal (NL), 1/3 of the sons are  the sons of carriers, and 2/3 are the sons of non-carriers.   

The **continued updating** as the women with a NL son gave birth to a second son, and so on, is shown in the diagram below, with **C**  used as shorthand for '**Is** a **C**arrier.'  Technically speaking, each sequential P[C] should indicate that it is 'conditioned on' -- and thus reflects the information in -- the history up to that point. In other words, the 1/5 probability refers to P[C | both sons are NL], where "|" stands for 'given that', or -- to use Turing's phrase -- 'on the evidence that'.



```{r,eval=T, echo=F, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F, fig.cap="At the outset, each woman had a 50:50 chance of being a haemophilia carrier. Accumulating information from the hemophilia status of the sons increasingly 'sorts' or segregates the women by moving their probabilities of being a carrier TO 1 (100%) or FURTHER TOWARDS 0 (0%). It 'updates' the probablity of  being a carrier, P[C]. For brevity,  the ' | data' in each P[C | data] is omitted."}

par(mfrow=c(1,1),mar = c(0.1,0.1,0.1,.1))
 
plot(c(-1.5,7),c(-0.1,1.1),col="white")

dx=c(0.05)
CEX = 1.25

text(0.5+dx, 1.02, "Women", adj=c(1,0), cex=CEX)

text(3.5, 1.075, "Their Son(s)",
     adj=c(0.5,0),col="grey50",cex=CEX)

rect(0.5-dx, 0, 0.5+dx, 1)
rect(0.5-dx, 0,0.5+dx, 0.5,col="black")
text(0.5-1.5*dx, 0.25, "Carriers",
     font=2, adj=c(1,0.5), cex=CEX)
text(0.5-1.5*dx, 0.75, "Non-Carriers", 
     adj=c(1,0.5), cex=CEX)

post=c("st","nd","rd",rep("th",3))

for (i in 0:6){
   
 if(i>0){
    text(i+0.5,1.02,paste(toString(i),post[i],sep=""),
      adj=c(0.5,0),col="grey50",cex=CEX)
    rect(i+0.5-dx, 1/2 - (1/2)^(i+1), i+0.5+dx, 1)
    rect(i+0.5-dx, 1/2 - (1/2)^(i), 
      i+0.5+dx, 1/2 - (1/2)^(i+1),col="grey50")
 }
      
 x = i+0.5-dx*1.5
 yh = 1/2 - (1/2)^(i+0.5)
 if(i>0){
    text(x,yh,"H", adj=c(1,0.5),col="grey50",cex=CEX)
    text(x+0.4,yh,"1", adj=c(1,0.5),font=2,)
 } 
 y=(1+1/2 - (1/2)^(i+1))/2
 if(i>0) text(x, y,"NL",  adj=c(1,0.5),
              col="grey50", cex=CEX)
 x = x+0.4
 den = 2^i+1
 num=1
 text(x, y, toString(num),adj=c(0.5,-0.25),font=2)
 segments( x-(1+(den>9))*dx, y, x+(1+(den>9))*dx,y)
 text(x, y, toString(den),adj=c(0.5,1.25),font=2)
 text(x, -0.05, "P[C]",adj=c(0.5,1.25),font=2)
 
 
 
 if(1==2) lines(c(i+0.5-1.2*dx, i+0.5-1.4*dx,
         i+0.5-1.4*dx, i+0.5-1.2*dx),
      c(1/2 - (1/2)^(i+1)-0.003,
        1/2 - (1/2)^(i+1)-0.003,
        1/2 - (1/2)^(i),  
        1/2 - (1/2)^(i)) ,col="grey50")
        
  if(1==2) lines(c(i+0.5-1.2*dx, i+0.5-1.4*dx,
          i+0.5-1.4*dx, i+0.5-1.2*dx),
          
        c(1/2 - (1/2)^(i+1)+0.003,
          1/2 - (1/2)^(i+1)+0.003,1,1),col="grey50") 
} 

text(3.75,0.27,"NL: Son is Normal",
     col="grey50",adj=c(0,0.5),cex=CEX )
text(3.75,0.22,"H:   Son has Haemophilia",
     col="grey50",adj=c(0,0.5),cex=CEX )

lines(c(-1,-1.1,-1.1,-1),c(0,0,1,1))
text(-1.175,0.5,"Posssible Haemophilia Carriers (based on their family history)",
srt=90,adj=c(0.5,0))

```

> ASIDE: This is similar to how researchers develop strains of “transgenic” mice, by introducing an altered gene (transgene) into the genome. In order to breed true, theanimals must be made to be homozygous, i.e., to have two copies of the introduced gene (+ +). Molecular biology techniques can detect whether the transgene is present in an individual animal (without having to sacrifice the animal), but cannot distinguish a hemizygote, with one copy of the gene (+ -), from a homozygote (+ +). This difference can only be detected by breeding strategies. First generations: A copy of the transgene is injected into the pronucleus of a newly fertilized ovum, prior to fusion with the male pronucleus. Thus all animals that develop from these zygotes can have at most one copy of the gene, from the ovum. After birth, screening is performed to detect these 'positive' animals, called founders. After sexual maturation, all founders are bred to normal 'wild type' (WT) animals, to ensure that the transgene has been incorporated in such a way as to be heritable. Pairs of positive (hemizygous) animals in this F1 generation are then bred to each other. By Mendelian genetics, the distri- bution of F2 offspring should be 1:2:1, homozygous transgenic : hemizygous transgenic : homozygous normal. The homozygous normal animals are not used. The question is, how to tell the homozygous transgenic mice (the desired ones) from the hemizygous transgenic ones? Note that the mix in this reduced population is 1 homozygous transgenic to 2 hemizygous transgenic. F2 breeding: All ’positive’ F2 animals (i.e. all homozygous and hemizygous animals) are bred to wild type. Possible F3 genotypes are as follows: (by Mendelian genetics) Hemizygous (which comprise 2/3 of the F2 animals used) x wild type = 50:50, hemizygous (and therefore ‘positive’) : normal (and therefore ‘negative’), Homozygous (which comprise 1/3 of the F2 animals used) x wild type = all hemizygous (and therefore ‘positive’).
That is, while only half of the offspring from a Hemi x WT pair will be ‘positive’ when screened, all of the offspring of a Homo x WT pair will be ‘positive’.
The question: How many F3 offspring from a particular pairing does the researcher have to screen before declaring the positive parent as homozygous? Note: as soon as an offspring is screened as 'negative,' one knows the parent must have been hemizygous. A variant on the above diagram can help withe probabilities. Furthe details are avilable on the bios601 website, in the 'probability' chapter. 

Before moving on the the next type of parameter, a few points

* In both the hemophilia and transgenic mice examples, the 'starting' probability is objective and the post-data
probabilities  have a 'long-run' or 'in large numbers of similar instances' interpretation. One could make a diagram that shows the expected numbers  'in every 100 women like this.' 

* There is nothing special about the 'starting out' probability P[**C**] of 0.5. Before a pregnancy test, or a pre-natal diagnostic test, for example, the probability  of the target **C**ondition/state of interest/concern would be a function of many other factors, and could in theory take on any value between 0 and 1. The (starting out, pre-filter) proportion of malicious emails would depend on which of a person's email accounts it was. 

* In the language of diagnostic tests, each 'Son as a test of the mother's carrier status' has 50% sensitivity and 100% 'specificity'. For sensitivity, this puts it on par with the Pap test for cervical cancer: the main problem withe latter is in the sampling. For specificity, it is better than most tests.  

* The shiny app [From Pre-test to Post-test Probabilities](https://jameshanley.shinyapps.io/FromPreTestToPostTestProbabilities/) shows how the initial average (pre-test) probability is segregated into 2 post-test probabilities by the 2 possible test results, and the role of the 2 error-probabilities (just about all tests are fallible) in how well they push them out.


* The 'starting out' probability could  be  subjective. For example it could be one's impression (before getting to see up close how wrinkled their face is) as whether the person is a smoker, or one's assessment of the probability that the accused is guilty (before getting to hear the  DNA expert, or lie detection report)  based on how credible the accused appears to be, and all of the other evidence to date.

* The main point is that we are merging (adding) two sources of information. 


### Example: parameter is a proportion

In theory, in this genre, the true parameter value could in theory lie anywhere between 0 and 1, But again, just like in Turing's example, we seldom start from complete ignorance, or with -- in the title of pscychologist Stephen Pinker's book -- a blank slate. Even if you have never seen thumbtacks tossed onto on a surface, you can reason informally, and indicate what proportions are unlikely and likely, and where along the (0,1) scale you would 'put your money'.

You could do the same when asked what proportions of your time that you are being productive, or on the phone, or sedentary, or indoors. Mind you, you might be 'way off' with your claims, but the nice thing is that --- and this is the point of this course -- you can generate data to narrow down the true proportion.

The other nice thing with the Bayesian approach in particular is that -- no matter whether you believe  the proportion is low or medium or high, we can work out what your post-data beliefs should be. It is a matter of mathematics. If, before collecting any new data, we have 'no idea' -- a common phrase among todays's generation, one that, if it is uttered with empahsis on the 'no', irks JH to no end  -- what the true parameter value is, that is easily handled. Moreover, enough valid data will (or should!) trump the pre-data beliefs. 

> On a side note: Dick Pound, a former chancellor of McGill University, and first president of the World Anti-Doping Agency is a staunch advocate of strict drug testing for athletes.
Discussing the National Hockey League in November 2005, Pound said, 'you wouldn’t be far wrong if you said a third of hockey players are gaining some pharmaceutical assistance.' Pound would later admit that he completely invented the figure. Both the NHL and NHLPA have denied the claims, demanding Pound provide evidence rather than make what they term unsubstantiated claims. Since his comments were made, some NHL players have tested positive for banned substances, including Bryan Berard, José Théodore, and two of 250 players involved in Olympic testing. As of June 2006, there had been 1,406 tests in the program jointly administered by the league and the union, and none has come up with banned substances under NHL rules. Pound remained skeptical, claiming the NHL rules were too lax and unclear, as they do not test for some banned substance, including certain stimulants. In an interview with hockey blogger, B. D. Gallof, of Hockeybuzz on December 19, 2007, Pound was asked to expand on the 30% comment and subsequent reaction, expounded that stimulants was 'the NHL's drug of choice'. He also cited that the NHL will have no credibility on a drug policy if it, and other sports, continue to run things 'in-house'. https://en.wikipedia.org/wiki/Dick_Pound and https://www.cbc.ca/sports/hockey/dick-pound-slams-nhl-s-drug-policy-1.557993

Even before studying/asking them, investigators would have some sense of the proportions of patients whose disease would respond to a medication, or people who would volunteer for a  survey or  research study. These iimpressions would probably be based on previous analogous situations, and the 'literature', but would vary from pundit to pundit. But ultimately, they could be much improved and narrowed (and even replaced entirely)  by new-data-based ones. 
   
The proportion of the earth's surface that is covered by water is easy to determine: just look up a reputable source. But what if you weren't able to, but did have access to the database of 933 million recordings in the [SRTM30PLUS database](https://topex.ucsd.edu/cgi-bin/get_srtm30.cg). It  has altitude/depth measurements for 43,200 x 21,600 = 933,120,000 locations. This database is so large that you would have to sample from it. From a thousand randomly chosen loactions, you would be able to 'trust' the first decimal in your estimate; from a million you should be able to trust the second -- and maybe the third.


Since we already know/remember from high school 'roughly' what the proportion is, we will leave it for an exercsie in another chapter. In this chapter,  following the advice of master-teacher Fred Mosteller, we use  examples where the **correct answer is not known with any precision**. The proportion of these we probably know the least about is the thumbtack one. However, it has fewer personal benefits than knowing what proportion of  your time  you are being productive. Moreover, we have a nice written account of how you might go about learning this personal proportion.  


In his book Elementary Bayesian Statistics, Gordon Antelman  informally introduces and illustrate a Bayesian analysis of an uncertain proportion with a slightly modified version of a novel and useful application of work sampling discussed by Fuller (1985). We have changed his notation for the proportion of your time spent in productive work, and called it $\pi$, and also modified some of his words. 

> Suppose you, as a good up-to-date manager practicing continuous quality and productivity improvement, have some ideas on improving your own productivity. To see if these ideas have any merit, you would like to compare some 'before' measure of productivity with a comparable 'after' measure of productivity.  
For now — we shall come back to this example several times — let us focus on just a 'before' measure.
The measure to be used is the proportion of your time spent in productive work, call it $\pi$, as opposed to time spent doing something that would not have needed doing if things had been done right the first time. Examples of the latter might include searching for a misplaced document, recreating a deleted computer file, following up on a customer's complaint, or waiting past a scheduled time for a meeting to start. [Today, we would add being on social media, or browsing the web for non-work-related matters] 
Rather than saying $\pi$ is not (precisely) 'known', it is better to say that '$\pi$ is uncertain'; from your job experience, you would really know quite a lot about p. For example, you might be almost certain that it is greater than 0.50, less than 0.90, and you might assess
your odds that $\pi$ is between, say, 0.60 and 0.80 to be about 9 to 1. A precise statement of these beliefs will be your prior distribution for $\pi$.  
You would probably feel uncomfortable — most people do — about assessing this prior distribution, especially since there are an infinite number of states; viz., all of the values between zero and one. But, without any real loss, you can bypass the infinite-number problem by rounding the values of $\pi$ to the nearest 5% or 10%, making the problem discrete. Then you have a contemplatable Bayes' theorem, like those discussed in Chapter 4, with the finitely many $\pi$-values as the possible "states". (When we reconsider this example later in this chapter, you will see that, with a little theory, the infinite number of $\pi$-values can almost always be handled very neatly and more easily.)  

**PRIOR BELIEFS** For illustration, he supposes you choose just five possible values for $\pi$, and assess your prior distribution.
Since `R` does not allow Greek symbols, we will refer to it by uppercase P and assess your prior distribution. This possible prior distribution, shown below, would reflect, for example, that your judgment is that there is only about one chance in 20 that $\pi$ rounds to 0.50, about one chance in 20 that $\pi$ rounds to 0.90, about one chance in four that it rounds to 0.60, a little more than one chance in three that it rounds to 0.70, and a little less than one chance in three that it rounds to 0.80.


```{r,eval=T, echo=T, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F, fig.cap="Prior Probabilities for the parameter P, the proportion of time that I am being productive."}
 
P = c(0.50, 0.60, 0.70, 0.80, 0.90)
PriorProbForP = c(0.05, 0.25, 0.35, 0.30, 0.05)

par(mfrow=c(1,1),mar = c(5,5,1,1))
plot(P, PriorProbForP, type="h", xlim=c(0,1),
     lwd=20, ylim=c(0,0.5),  lend=1, col="grey80",
     xlab="P: Proportion of time that I am being productive",
     ylab="My probability that P rounds to... ",
     cex.lab = 1.25, cex.axis = 1.25)
text(0.5,0.25, "Prior Probabilities", font=2,
     col="grey75", adj=c(1,1), cex=1.75)

```


**DATA**: Suppose you are fitted with a beeper set to beep at random times; when the beeper beeps, you classify the task being worked on as **W** — for 'productive **Work**', or **F** — for '**Fixing**' (or today we mght say '**F**iddling' or '**F**ooling around' or wasting time).

Although we will skip the technicalities, it is important that  the experiment  be designed so that the trials are independent. Beeps should be unpredictable so you do not arrange, possibly subconsciously, to be doing productive work at the beep. They probably also should not be too close together to make the independence assumption more reasonable.

**Suppose the first four trials give the data $F_1, F_2, W_3$, and $F_4$.** 

Below is a picture showing the effects of the data FFWF on the prior distribution.  Three F's in four trials increase your  probabilities for the two smaller values of P  and decrease your probabilities for the three larger ones.

```{r,eval=T, echo=T, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F, fig.cap="Prior Probabilities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior probabilities, after observing that in n = 4 randomly sampled occasions, I was actually productive in only 1 of the 4."}
 
Lik = (1-P)^3 * P
PosteriorProbForP = (PriorProbForP * Lik) / sum(PriorProbForP * Lik)

par(mfrow=c(1,1),mar = c(5,5,1,1))
plot(P, PriorProbForP, type="h", xlim=c(0,1),
     lwd=20, ylim=c(0,0.5),  lend=1, col="grey80",
     xlab="P: Proportion of time that I am being productive",
     ylab="My probability that P rounds to... ",
     cex.lab = 1.25, cex.axis = 1.25)
text(0.5,0.25, "Prior Probabilities", font=2,
     col="grey75", adj=c(1,1), cex=1.75)
lines(P, PosteriorProbForP, type="h", lwd=6, col="red",lend=1)
text(0.5,0.4, "Posterior Probabilities\nafter observing 1W, 3F", font=2,
     col="red", adj=c(1,1), cex=1.75)

```

The sample alone most strongly supports a value for P of 0.25 (one W in four trials); had the prior included a value of P of 0.25, the (relative) increase in going from prior to posterior would have been greatest for that value.

For the assumed prior, in which only p's of 0.50, 0.60, 0.70, 0.80, or 0.90 are considered, the sample evidence FFWF in favor of a P near 0.25 can only push up the posterior probabilities for the nearest possible values - 0.50 and 0.60. (The seemingly harder consideration of **all possible p's between zero and one** will handle this kind of situation more logically.)

Below we show the '**continuous P **' version Antelman refers to. To make this, we calculated the mean and variance of his discrete (5-point) prior distribution, and converted them to the 2 parameters, $a$ and $b$, of the beta distribution with the same mean and variance.

Conveniently, the posterior density is also a beta distribution, but with parameters $a+1$ and $b+3$. 


```{r,eval=T, echo=T, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F, fig.cap="."}
 
mean.P.prior = sum( P* PriorProbForP )
var.P.prior  = sum( (P-mean.P.prior)^2 * PriorProbForP )

a.plus.b = mean.P.prior * (1-mean.P.prior) / var.P.prior  - 1
a = mean.P.prior * a.plus.b
b = (1-mean.P.prior) * a.plus.b

P = seq(0,1,0.01)
prior.density = dbeta(P,a,b)
posterior.density = dbeta(P,a+1,b+3)


```


```{r,eval=T, echo=F, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F, fig.cap="Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 4 randomly sampled occasions, I was actually productive  (W) in only 1 of the 4."}


par(mfrow=c(1,1),mar = c(5,5,1,1))
plot(P, prior.density, col="grey80",
     xlab="P: Proportion of time that I am being productive",
     ylab="probability density",
     cex.lab = 1.25, cex.axis = 1.25, type="l",lwd=5,
     ylim=c(0,max(posterior.density)) )
text(0.975,max(prior.density)/1.25, "Prior\nProbability\nDensity", font=2,
     col="grey75", adj=c(1,1), cex=1.75)
posterior.density = dbeta(P,a+1,b+3)
lines(P,posterior.density, type="l",lwd=5,col="red")

text(0.25,max(prior.density)/1.25, "Posterior Probability Density\nafter observing 1W, 3F", font=2,
     col="red", adj=c(0.5,1), cex=1.75)


```


Before moving on the the next type of parameter, a few points:

* The beta distribution nicely shows **how the prior information/impression and the new data get combined**. The $a$ and $b$ parameters of the prior distribution are `r round(a,1)` and `r round(b,1)`. Together, they determine the mean, $a/(a=b)$, the median, the  mode, $(a-1)/(a+b-2)$, and the variance,
$a^2b^2/(a+b+1)^2$ of the prior distribution. 
Their conterparts in the data-likelihood are 1 and 3. The '$a$' and '$b$' parameters of the posterior distribution are `r round(a+1,1)` and `r round(b+3,1)`: the $a$'s **add**, and the  $b$'s **add**. In other words, the distribution of one's pre-data beliefs is the distribution one would have after 'seeing' `r round(a,1)` W's and `r round(b,1)` F's;  the distribution of one's post-data beliefs is the distribution one would have after 'seeing' `r round(a+1,1)` W's and `r round(b+3,1)` F's. **The (synthetic) experience-equivalent of the numbers of Ws and F's in the prior are added to the  actual (observed) numbers of Ws and F's in the data to arrive at  the (new) posterior distribution.**

* You are probably wondering what the posterior distribution would look like **with more data**. Here is what it would look like after observing 7 out of 25 or 13 out of 25. The modes of the posterior distributions are still  somewhat influenced by the prior -- as they are still well above P=7/25 = 0.28 and P=13/25 = 0.52. If the data were 70/250 or 13/250, the modes would be closer to  P= 0.28 or P = 0.52; in other words, the data would 'swamp' -- or 'trump' -- the prior. 



```{r,eval=T, echo=F, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F, fig.cap="Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 25 randomly sampled occasions, I was actually productive (W) in only 7 of the 25, or 13 of the 25."}


par(mfrow=c(1,1),mar = c(5,5,1,1))
plot(P, prior.density, col="grey80",
     xlab="P: Proportion of time that I am being productive",
     ylab="probability density",
     cex.lab = 1.25, cex.axis = 1.25, type="l",lwd=5,
     ylim=c(0,1.75*max(posterior.density)) )
text(0.975,max(prior.density), "Prior\nProbability\nDensity", font=2,
     col="grey75", adj=c(1,1), cex=1.75)
for(num in c(7,13) ) {
  posterior.density = dbeta(P,a+num,b+(25-num))
  lines(P,posterior.density, type="l",lwd=2.5,col="red")
  p = P[which(posterior.density == max(posterior.density))]
  text(p,max(posterior.density), 
       paste(toString(num),"W,",toString(25-num),"F",sep=""), font=2, col="red", 
       adj=c(0.5,-0.5),cex= 1.25)
}
text(0.025,1.25*max(posterior.density), "Posterior Probability Density of P after observing:", font=2,
     col="red", adj=c(0,1), cex=1.25)


```

* Be thinking about your prior for the proportion of thumbtacks that land on their back, and the proportion of the Earth's surface that is covered by water, or [these words written on March 30, 2020, before any trial data] the proportion of patients with mild symptoms of covid-19  who would benefit from chloroquine.

* Think about how you might elicit a prior distribution. You might want to Google 'tools for eliciting prior distributions' -- or consult Chapter 5 of Spiegelhalter's book.


We now move on to last parameter genre we will consider.


### Example: parameter is a personal number or population mean

We will start with a discrete version of a commonly-wondered-about parameter, 
the age of a person whom you have just met, or seen a photo of.
We will then go on to a full numerical example, your average cholesterol or blood pressure level.

**Example 1**

**How old** (or **what age** -- if you prefer to avoid speaking of 'old') do you think this IBC2006 attendee was when this photo was taken? This is the _parameter of interest_.


```{r,eval=T, echo=F, fig.align="center", fig.height=4, fig.width=9, warning=FALSE, message=F, fig.cap="An attendee at the International Biometrics Conference, held at McGill in July 2006"}

knitr::include_graphics("images/IMG_DS.png",dpi=45)

```

He tells you  he **got his PhD 32 years earlier**. Based on the distribution of ages at which people get their PhD (shown in grey below), that puts his current age somewhere in the blue distribution.


```{r,eval=T, echo=F, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F, fig.cap="Current ages of persons who obtained PhD 32 years earlier."}


par(mfrow=c(2,1),mar=c(2,4,1,1),oma=c(1,1,1,1))
age=seq(15,45,5) ; w=c(rep(5,6),10)
number=c(0,334, 12520, 11570 , 6108 , 4039 , 4921)/w
plot(age,number,xlim=c(15,60),col="white",
xlab="",ylab="Ave. no. per 1 year age-band",
cex.lab=1.35,cex.axis=1.35) 
text(48,1700,"Age when earned doctorate",col="grey",adj=c(0.5,1),cex=1.5)

for (i in 1:7) rect(age[i],0,age[i]+w[i],number[i],
                    col="grey",border=NA)
text(31,2400,"http://www.nsf.gov/statistics/doctorates/summ97/part4.pdf",
  adj=c(0,0),cex=0.95)
text(36,2000,"Table 18 Distribution of 1997 Doctorate Recipients by Age at Doctorate",
  adj=c(0,0),cex=0.70)

later=32

plot(age+later,number,xlim=c(15+later,60+later),col="white",
xlab="",ylab="Ave. no. per 1 year age-band",
cex.lab=1.35,cex.axis=1.35) 
text(50+later,1800,"Age 32 years later",adj=c(0.5,1),cex=1.5,col="blue")
for (i in 1:7) rect(age[i]+later,0,age[i]+w[i]+later,number[i],
                    col="blue",border=NA)

```


```{r,eval=T, echo=F, fig.align="center", fig.height=5, fig.width=9, warning=FALSE, message=F}

mean = c(55, 60, 65) ; sd=4

a.now = age + 32

draw.3.panels = function(index) {

ia = index

par(mfrow=c(3,1),mar=c(2,4,1,1),oma=c(1,1,1,1))

plot(c(45,90),c(0,1/5),xlim=c(45,90),col="white",
xlab="",ylab="Prob. per 1 year age-band",cex.axis=2) 
text(68,0.95/5,"Age, based on photo",col="red",adj=c(0,1),cex=2.5)

for (i in 1:7) {
	no = pnorm(a.now[i]+w[i],mean[ia],sd) - pnorm(a.now[i],mean[ia],sd)
	rect(a.now[i],0, a.now[i]+w[i],no/w[i], 
	     col="red", border=NA)
	}

plot(age+later,number,xlim=c(45,90),col="white",
xlab="",ylab="Ave. no. per 1 year age-band",cex.axis=2) 
text(70,1800,"Age, based on PhD data",cex=2.5,col="blue",adj=c(0,1))
for (i in 1:7) rect(a.now[i],0,a.now[i]+w[i],number[i],
                    col="blue",border=NA)

plot(c(45,90),c(0,1/5),xlim=c(45,90),col="white",
xlab="",ylab="Prob per 1 year age-band",cex.axis=2) 
text(64,.9/5,"Age, based on photo and PhD data",col="green",adj=c(0,1),cex=2.5)
prior = pnorm(a.now+w,mean[ia],sd) - pnorm(a.now,mean[ia],sd)
post=prior*number/sum(prior*number)/w
for (i in 1:7) {
	rect(a.now[i],0, a.now[i]+w[i],post[i], 
	     col="green",border=NA)
	}

} # end fn.

```

This is a somewhat unusual example, as the blue distribution is very wide -- partly because we could not find age-at-graduation data specific to PhD graduates in statistics in 1974. We suspect that that specific distribution is a good deal narrower than the one shown.   In the next chapter, you will see that other indirect measures of age are a good bit tighter  than this.

Nevertheless, it emphasizes that, depending on what impression you got form the photo alone, you may now wish to revise your estimate of the person's age. We suspect that many of you would have initially though he **looks like he was in mid 50s**, and so would have made guesses like those shown in red in the top panel. If you are one of those, then you will want to **revise the age upwards**, as we do in the green in the bottom panel.

```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F, fig.cap="If he looked like he was in mid 50s"}

draw.3.panels(1)

```

If you initially thought he looked like he was 'around 60' you would have made guesses like those shown in red in the top panel. If you did, then you will want to **revise upwards a little**, as is shown in green, and now put hime him somewhere around 60, or a bit above.

```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F, fig.cap="If If he looked like he was around 60."}

draw.3.panels(2)

```
If you initially thought he looked like he was 'in his mid 60s' your estimate is more in line with the age-at-PhD data, and so you would not revise as much. You might bet a bit more on the 'around 60' age-bracket.

```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F, fig.cap="If If he looked like he was in his mid 60s."}

draw.3.panels(3)

```
	
As we noted, the PhD data have too much of a right tail, and so it is driving up the estimates. If you are now curious as how keen your 'age-estimation' skills are, here is a link to the Google Scholar page of the [statistician whose age we have been trying to determine](https://scholar.google.com/citations?user=e8DARmEAAAAJ&hl=en).

Age estimation via face images (image-based age estimation) is a growing research area, with many possible applications.

We now describe 2 more classical examples

**Example 2**

Spiegelhalter et al. address this in their  Example 3.4: 'Suppose we are interested in the **long-term systolic blood pressure  (SBP)
in mmHg of a particular 60-year-old female**.'

> We take two independent readings 6 weeks apart, and their mean is 130. We know that SBP is measured with a standard deviation $\sigma = 5.$ What should we estimate her SBP to be? 

They then go on to give the **frequentist** ('standard', 'orthodox') **95% confidence interval**,
of **123.1 to 136.9**, **centered on the measured value of 130** [we will come later to how they calculated this]. They continue, ...

> **However**, we may have **considerable additional information about SBPs 
which we can express as a prior distribution**. Suppose that a survey in the 
**same population** revealed that females aged 60 had a mean long-term 
SBP of 120 with standard deviation 10. **This population distribution can be
considered as a prior distribution for the specific individual**. 


The **posterior distribution**, computed from the combination of the 130 measured on the woman, 
and the prior, is **centered on 128.9** and 
 the 95\% interval is **122.4 to 135.4**. 

>This posterior distribution reveals some **'shrinkage' towards the population mean**, and a small increase in precision from not using the data alone.  
**Intuitively**, we can say that the woman has somewhat higher measurements than we would expect for someone her age, and hence we slightly adjust our estimate to allow for the possibility that her two measureshappened by chance to be on the high side. As additional measures are made, this possibility becomes less plausible and the prior knowledge will be systematically downgraded.

Before going on to example 3, we emphasize that the 128.9 is a **compromize** between the persnal mean of 130, and the (prior)  poulation mean of 120: it is a weighted average, with (relative) weights that are the reciprocals of the squares of the 2 standard deviations, the reciprocals of $\frac{5^2}{2}$ and $10^2,$ i.e., $\frac{2}{25} = 0.08$ and $\frac{1}{10^2} = 0.01.$ The 
128.9 is **8/9ths closer to the measured 130** than it is to the **population mean of 120**. It is slightly 'shrunk' toawards the population. This is why physicians might not believe that the 130 is correct, and might ask for more measurements before putting this woman on  blood pressure-lowering drugs.

[The already cited 'Bayesian integration in sensorimotor learning'  illustrates how as humans, we automatically **combine estimates of different precisions** into one more precise estimate, and do so using the same mathematical laws that are used in the Bayesian approach!]


**Example 3** 

One article that does go into some detail about a similar situation is the very nice medically-useful – and didactic article [Estimating an Individual’s True Cholesterol Level and Response to Intervention](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch10Bayes/Irwig-1991-cholesterol.pdf) by Les Irwig, Paul Glasziou, Andrew Wilson and Petra Macaskill.

It begins with a single measurement, before dealing with an average of several measurements on the same person. It also gives separate charts for persons of different ages, and deals not just with point and interval estimates, but also derives probability statements for the possibility that the person’s true cholesterol is above some threshold that should trigger intervention. The appendix is a nice tutorial for combining information.

Their _abstract_ begins:

> An individual’s blood cholesterol measurement may differ from the true level because of short-term biological and technical measurement variability. Using data on the within-individual and population variance of serum cholesterol, we addressed the following **clinical concerns: Given a cholesterol measurement, what is the individual’s likely true level?** The confidence interval for the true level is wide and asymmetrical around extreme measurements because of regres-sion to the mean. Of particular concern is the misclassification of people with a screening measurement below 5.2 mmol/L who may be advised that their cholesterol level is 'desirable' when their true level warrants further action.

The first half of the paper, which deals with two related topics, (a) Estimating the True Cholesterol Level, and (b) assesing the Probability of Misclassification shows the primary elements, and these notes will focus on the highlights. [after these, extensive excerpts will be included]

The results for (a) and (b) were presented as 2 Figures. The first gave the (posterior) credible interval for a person’s true cholesterol level based on either 1, or an average of 3, measurements, using on the horizontal axis the measured value, and on the vertical one the point and 95% credible interval. Using a graph (rather than a formula) allows the clinician to use it for all possible ‘what if’s.

Below, we will **illustrate** it using one specific example, a **person whose measured value was 7.15**.

The second uses the (posterior) credible interval to calculate the **probability that someone with a specific measured value has a true level that is above a certain threshold level used in treatment guidelines.**
Thus, the key tool is the posterior distribution itself, and so we give the statistical basis for this.

_Reasons to take a Bayesian approach_

The reason this problem arises in the first place is because of short term biological variability in the quantity of interest in the person in question. If we were measuring a person’s height, we could do so carefully at just one time-point: it would not be different a week or month from now; it remains quite stable over several years. [it does vary slightly over the day, but, be keep it simple, we could speak of one’s height at mid-day]. The same is not the case for a person’s cholesterol level: even if we measured it very carefully at one time, it would be genuinely different a week or month later, even in the absence of any intervention of lifestyle change. (The same applies, more strikingly, for other blood levels such as C-reactive protein (CRP), which is a marker of inflammation).

Thus any single measurement, or any average of a finite number of determinations, is imprecise. **So what’s new?** Don’t we meet this issue all the time in statistics?

The **point of Irwig's article** is that we should **not rely solely on the estimate based on the _person_’s measurements**, but rather **should combine it with an
estimate based on _outside information_.**


The same reasoning is at work when a physician repeats a measurement that seems extreme. In so doing, (s)he is not relying only on the point or interval estimate provided by the measurement itself: rather (s)he is **also using knowledge of how this measurement behaves in other similar persons**! And what we know about others, even if collectively, can help us with an individual.

All of the technical details are available in these notes, prepared for [bios601]()
Here we will skip to the 'botom line' diagrams.


```{r,eval=T, echo=F, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F, fig.cap="A Person's estimated true cholesterol level from one measurement for a population of individuals where the mean is 5.2 mmol/L (i.e, at the time of the article, men less than 35 and women less than 45 years old). The thicker solid line indicates the estimated true level, the  thinner solid lines are 80% confidence intervals. The dotted diagonal line is an equivalence line. The arrow shows the amount by which a single measurement of 7.15 mmol/L is shrunk towards the population mean, to a rcorrected value of 6.8 mmol/L."}

var.within.log.chol = c(0.00635, 0.00548, 0.00539, 0.00589)[4]
var.total.log.chol = c(0.04194, 0.03964, 0.03649, 0.03936)[4]
var.between.true.log.chol = var.total.log.chol - var.within.log.chol
precision.p =  1/ var.between.true.log.chol 
precision.e =  1/ var.within.log.chol

mu.log.chol=log( c(5.2, 5.8, 6.4) ) - var.total.log.chol/2

median = exp(mu.log.chol); 
measurement=log( seq(4, 10, 0.2) )

mean.log.A = mu.log.chol[1]

weight.e = precision.e / (precision.e + precision.p)
weight.p = precision.p / (precision.e + precision.p)

revised.est = measurement * weight.e + mean.log.A * weight.p
precision.revised.est = precision.e + precision.p
sd = sqrt(1/precision.revised.est)

plot(c(4,10), c(4,10), ylab="Estimated Level (mmol/L)",
     xlab="Measured Level (mmol/L)",col="white" ,
     cex.axis=1.5, cex.lab=1.5)
for(x in seq(4, 10, 0.2)) segments(x,4,x,10,col="lightblue",lwd=0.5)
for(x in seq(4, 10, 1.0)) segments(x,4,x,10,col="lightblue",lwd=2)
for(x in seq(4, 10, 0.2)) segments(4,x,10,x,col="lightblue",lwd=0.5)
for(x in seq(4, 10, 1.0)) segments(4,x,10,x,col="lightblue",lwd=2)
points(7.15,3.9,pch=19,cex=1)
points(7.15,7.15,pch=19,cex=1)
est = log(7.15) * weight.e + mean.log.A * weight.p
arrows(7.15,7.15,  7.15, exp(est), length=0.075, angle=35,lwd=1.5)
points(exp(measurement),exp(measurement),type="l", lty="dotted",lwd=2)
points(exp(measurement),exp(revised.est),type="l",lwd=2)
points(exp(measurement),exp(revised.est+1.96*sd),type="l",lwd=1)
points(exp(measurement),exp(revised.est-1.96*sd),type="l",lwd=1)
points(5.2,5.2,pch="+",cex=1.5)

```

Below, we show the detailed calculations for a person with  a single measurement of 7.15 mmol/L. For now, just focus on the  items shown in red (the prior distribution for $\theta$, blue (the 7.15 for the individual, and the associated likelihood function) and green (the posterior distribution of $\theta$). All the calculations are on the log scale.

```{r,eval=T, echo=F, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F, fig.cap="Worked example, for a person with a measured value of 7.15"}

mu.log.chol.Pop.A = mu.log.chol[1]
var.log.True.chol.Pop.A = var.between.true.log.chol
sigma.p=sqrt(var.log.True.chol.Pop.A)
observed.log.chol = log(7.15) ; # observed.log.chol
var.log.Indiv.log.chol = var.within.log.chol
sigma.e=sqrt(var.log.Indiv.log.chol)

# mu.log.chol.Pop.A - 2.5*sqrt(var.log.True.chol.Pop.A)
# mu.log.chol.Pop.A + 2.5*sqrt(var.log.True.chol.Pop.A)

v0=1.0; v1=2.3; dv=0.1
values=seq(v0,v1,dv)
finer.values=seq(v0,v1,dv/5)

prior=dnorm(finer.values,mu.log.chol.Pop.A,sigma.p)



par(mfrow=c(1,1),mar=c(0.1,0.1,0.1,0.1),oma=c(0.1,0.1,0.1,0.1))

y.max=4; y.min= -38

plot(finer.values,prior,type="l",
ylim=c(y.min,y.max),
xlim=c(v0-0.125*dv,v1+0.75*dv),
xaxt="n",yaxt="n",lwd=2.5,col="red")
h=dnorm(mu.log.chol.Pop.A+sigma.p,mu.log.chol.Pop.A,sigma.p)
segments(mu.log.chol.Pop.A               ,0+h, 
         mu.log.chol.Pop.A+sigma.p       ,0+h,
   lty="dotted",lwd=0.5)
text(mu.log.chol.Pop.A+sigma.p/2,0+h, 
     toString(round(sigma.p,3)),
        cex=0.5,col="darkgrey",adj=c(0.5,1.5))
text(mu.log.chol.Pop.A+sigma.p/2,0+3*h/5, 
     paste("[",toString(round(1/sigma.p^2,2)),"]"),
        cex=0.4,col="darkgrey",adj=c(0.5,1.5))

segments(mu.log.chol.Pop.A,0, 
   mu.log.chol.Pop.A,dnorm(mu.log.chol.Pop.A,mu.log.chol.Pop.A,sigma.p),
   lty="dotted",lwd=0.5)

text(1,3.5,
expression(paste("Parameter of Interest: ",
                 theta,
                 " = A Specific Person's True log[Cholesterol Level]")), cex=1.5,adj=c(0,0))

segments(0,2.7, 3,2.5,lty="dotted",lwd=2)

segments(v0,0,v1,0,lwd=0.5,col="lightblue")
text(1,1.75,"Distrn.: True log[Level]s in all such persons", cex=0.8,adj=c(0,0),,col="red")
text(1,0.7,"[ Vertical axis such that a.u.c. = 1 ]", cex=0.75,adj=c(0,0),,col="red")



text(1.9,1.4,
 expression(
   paste(
     "Before observing ", 
      italic(y),
      ", this distribution" 
      ) 
  ),
 cex=0.85,adj=c(0,-0.25),col="red")
 
 text(1.9,0.7,
 expression(
   paste(
      "serves as ", 
      italic(PriorProb),
      "( ",
      theta,
      " )" 
      ) 
  ),
 cex=1.25,adj=c(0,0.15),col="red")
 # text(c(mu.log.chol.Pop.A),-1,
 #    toString(round(exp(mu.log.chol.Pop.A),2)),
 #  cex=1.5,col="red", adj=c(0.5,1))
 


for (v in seq(v0,v1,dv)) {
	segments(v,-0.1,v,0)
	text(v,-0.15,toString(v),cex=0.8,adj=c(0.5,1))
	} 
	
text(v1+0.5*dv,0,
 expression(
   paste(
     theta,
      ) 
  ),
 cex=1.5,adj=c(0,0.5))

segments(0, -1, 3, - 1,lty="dotted",lwd=2)	
y0 = -2
segments(v0,y0,v1,y0,lwd=0.5,col="lightblue")
for (v in seq(v0,v1,dv)) {
	segments(v,y0-0.1,v,y0)
	text(v,y0-0.2,toString(v),cex=0.8,adj=c(0.5,1))
	} 
text(observed.log.chol,y0-1,"y",cex=1.1,adj=c(0.5,0.25),
     font=3,col="blue" )
text(c(observed.log.chol),y0-3,
     toString(round(exp(observed.log.chol),2)),
  cex=1.5,col="blue", adj=c(0.5,0.5))
text(1.25,y0-1,"Observed log[level] for person in question",  
   cex=1.0,adj=c(0,0.9),col="blue")
points(c(observed.log.chol),y0,pch=19,cex=0.75,col="blue")
text(v1+0.5*dv,y0,
 expression(
   paste(
     italic(Y)
      ) 
  ),
 cex=1.25,adj=c(0,0.5))

segments(0, -4, 3, -4,lty="dotted",lwd=2)

text(1, y0-10, expression(
 paste("Prob( ", 
       italic(y), " | ", 
       theta, 
       " ) for selected values of ", 
       theta, 
       "  ..." )),
   cex = 1.0,adj=c(0,0.5))    

   
int=floor((observed.log.chol-v0)/dv)
offset=observed.log.chol-(v0+int*dv)

y0 = -4.5; theta=c(); p.y.given.theta=c()
for (true.val in (dv/2+seq(2.15,1.65,-dv/2)) ){
	y0=y0-1.25
	val = true.val +
	  seq(-0.25,0.25,0.0125) + offset
	ht=0.1*dnorm(val,true.val,sigma.e)
	for(j in 1:length(ht)){
	 if( abs(val[j]-observed.log.chol) < 0.1*dv) {
		  segments(val[j],y0,val[j],y0+ht[j],lwd=1.5)
		  theta=c(theta,true.val);
		  p.y.given.theta=c(p.y.given.theta,ht[j])
		  }
	 if( abs(val[j]-observed.log.chol) >= 0.1*dv)
		  segments(val[j],y0,val[j],y0+ht[j],col="grey")
		}
	 segments(min(val)-0.025,y0,max(val)+0.025,y0,
	  lwd=0.5,col="lightblue")
	 #points(c(true.val),c(y0),cex=0.2,pch=19)
	 text(true.val,c(y0-0.075),quote(theta),cex=0.5,adj=c(0.5,1))
	}

         
text(1, y0-1.5, expression(
 paste("Selected portion of Likelihood function, L( ",
       theta, 
       " ), based on observed data, ",
       italic(y)
        )),
   cex = 1.0,adj=c(0,0.5))
text(1, y0-2.5, "[ same (arbitrary) scale for vertical axis ]",
   cex = 1.0,adj=c(0,0.5))

for (j in 1:length(theta))
 segments(theta[j],(y0-3.5),
          theta[j],(y0-3.5)+p.y.given.theta[j],lwd=1.5)
          
y0=y0-7
text(1, y0, expression(
 paste("Entire Likelihood function, L( ",
       theta, 
       " ):"
        )),
   cex = 1.0,adj=c(0,0.5),col="blue")



text(1, y0-1, "[ vertical axis scaled so a.u.c. = 1 ]",
   cex = 0.8,adj=c(0,0.5))



y0=y0-2.5
points(finer.values,
      y0+dnorm(finer.values,observed.log.chol,sigma.e),
      type="l", col="blue")

points(c(observed.log.chol),y0,pch=19,cex=0.75,col="blue")
text(c(observed.log.chol),y0-1,
     toString(round(exp(observed.log.chol),2)),
  cex=1.5,col="blue", adj=c(0.5,0.5))

segments(observed.log.chol,y0, 
         observed.log.chol,y0+
           dnorm(observed.log.chol,observed.log.chol,sigma.e),
   lty="dotted",lwd=0.5)

h=dnorm(observed.log.chol+sigma.e,observed.log.chol,sigma.e)
segments(observed.log.chol               ,y0+h, 
         observed.log.chol+sigma.e       ,y0+h,
   lty="dotted",lwd=0.5)
text(observed.log.chol+sigma.e/2,y0+h, 
     toString(round(sigma.e,3)),
        cex=0.7,col="darkgrey",adj=c(0.5,1.5))
text(observed.log.chol+sigma.e/2,y0+h/2, 
     paste("[", toString(round(1/sigma.e^2,2)) ,"]"),
        cex=0.6,col="darkgrey",adj=c(0.5,1.5))
        
text(v1+0.5*dv,y0,
 expression(
   paste(
     theta,
      ) 
  ),
 cex=1.5,adj=c(0,0.5))

segments(v0,y0,v1,y0,lwd=0.5,col="blue")
for (v in seq(v0,v1,dv)) {
	segments(v,y0-0.1,v,y0)
	text(v,y0-0.2,toString(v),cex=0.8,adj=c(0.5,1))
	} 

y0=y0-2.5;


segments(0,y0, 3,y0,lty="dotted",lwd=2)

y0=y0-2;

text(1, y0, 
     expression(
       paste(
       italic(PostProb),
      "( ",
      theta,
      " | ",
      italic(y),
      " ) "
      )
     ),
   cex = 1.75,adj=c(0,0.25), font=2,col="green"
)
text(1, y0-1, 
     expression(
       paste(
      " = ",
      "S.F. x Prob( ",
       italic(y) ,
       " | ",
       theta,
       " ) x ",
       italic(PriorProb),
      "( ",
      theta,
      " ) = ", 
       "S.F. x L( ",
       theta, 
       " )  x  ",
       italic(PriorProb),
      "( ",
      theta,
      " )"
      )
     ),
   cex = 0.9,adj=c(0,0.5)
)
text(1, y0-2.5, "[ S.F. = Scaling Factor, chosen so a.u.c. = 1 ]",
   cex = 0.9,adj=c(0,0.5))


y0=y0-5

segments(v0,y0,v1,y0,lwd=0.5,col="blue")
for (v in seq(v0,v1,dv)) {
	segments(v,y0-0.1,v,y0)
	text(v,y0-0.2,toString(v),cex=0.8,adj=c(0.5,1))
	}

sigma.combined = sqrt(1/precision.revised.est) 
weight.e = precision.e / (precision.e + precision.p)
weight.p = precision.p / (precision.e + precision.p)
mu.combined = weight.p*mu.log.chol.Pop.A + weight.e*observed.log.chol
weight.e = precision.e / (precision.e + precision.p)
weight.p = precision.p / (precision.e + precision.p)
points(finer.values,
      y0+    dnorm(finer.values,mu.combined,sigma.combined),
      type="l",lwd=2.5,col="green")
points(c(mu.combined),y0,pch=19,cex=0.75,col="green")

text(c(mu.combined),y0-1,toString(round(exp(mu.combined),2)),
  cex=1.5,col="green", adj=c(0.5,1))
      
segments(mu.combined,y0, 
         mu.combined,y0+
           dnorm(mu.combined,mu.combined,sigma.combined),
   lty="dotted",lwd=0.5)
   
h=dnorm(mu.combined+sigma.combined,mu.combined,sigma.combined)
segments(mu.combined               ,y0+h, 
         mu.combined+sigma.combined,y0+h,
   lty="dotted",lwd=0.5)
text(mu.combined+sigma.combined/2,y0+h, 
     toString(round(sigma.combined,3)),
        cex=0.7,col="darkgrey",adj=c(0.5,1.5))
text(mu.combined+sigma.combined/2,y0+h/2, 
     paste("[",toString(round(1/sigma.combined^2,2)),"]"),
        cex=0.6,col="darkgrey",adj=c(0.5,1.5))

text(v1+0.5*dv,y0,
 expression(
   paste(
     theta,
      ) 
  ),
 cex=1.5,adj=c(0,0.5))
 
 text(1, y0-2,"Adapted from Irwig et al, JAMA 1991",cex=0.75,adj=c(0,0),
 font=3)
 
 ```
 
**combine estimates of different precisions** into one more precise estimate, and do so using the same mathematical laws that are used in the Bayesian approach!]


**Example 4** 

We end with a strking example of the limitations of using one-at-a-time frequentist intervals, each in isolation from the next. It is based on the older baseball example in this [classic article](http://www.medicine.mcgill.ca/epidemiology/hanley/bios602/MultilevelData/EfronMorrisJASA1975.pdf).


The task is to predict each player's batting average over the remainder of the season using  only the data from the first 4 weeks of the season. Since it proved  difficult to replicate their selection croteria, we used instead the first 40 of [these players](http://mlb.mlb.com/stats/sortable.jsp#elem=%5Bobject+Object%5D&tab_level=child&click_text=Sortable+Player+hitting&game_type='R'&season=2019&season_type=ANY&league_code='MLB'&sectionType=sp&statType=hitting&page=1&ts=1586307216206&playerType=ALL&timeframe=&last_x_days=&split=&sortColumn=ab&sortOrder='desc'&extended=0).


```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F}

 
path ="http://www.biostat.mcgill.ca/hanley/statbook/MLBatBats.txt"

players=read.table(path)

v.tot = round(var(players$ave),6)

v.binomial = round( mean(players$ave*(1-players$ave)/players$ab), 6)

v.net = v.tot - v.binomial


```

The numbers of at bats per player up to mid April ranged from  `r round(summary(players$ab))[1]` to  `r round(summary(players$ab))[6]` (median `r round(summary(players$ab))[3]` ). Efron and Morris used a constant 45. The numbers from mid April to the end of September ranged from `r round(summary(players$AB))[1]` to  `r round(summary(players$AB))[6]` (median `r round(summary(players$AB))[3]` ), or about 9 times as many as in the initial sample.



Clearly, the extreme results in these first 4 weeks were poor estimates of the perfornmances during the rest of the season. They 'shrink' quite a bit.


```{r,eval=T, echo=F, fig.align="center", fig.height=6, fig.width=9, warning=FALSE, message=F, fig.cap="Batting averages of selected Major League baseball players, 2019"}


par(mfrow=c(1,1),mar = c(5,1,1,1))

plot(c(0.15,0.5),c(0,1.1),col="white",
     ylab="", yaxt="n",
     xlab="Batting Average, April 16 to end of September",
     cex.lab=1.75, cex.axis=1.75)
segments(players$ave, players$y, players$AVE, players$Y,
         col=2:20)
segments(seq(.15,.50,0.01),0,seq(.15,.50,0.01),1,col="grey85")
segments(seq(.15,.50,0.05),0,seq(.15,.50,0.05),1,col="grey65")
text(0.325,1.085,"Batting averages, March 20 to April 15, 2019",cex=1.75)

```

Clearly, not all of the variance of `r format(v.tot, nsmall = 4)` is 'real'. Much of it, about `r format(v.binomial,nsmall = 4)` worth, is nothing more than binomial variation -- with $n$'s  of the order of 60, and  player-specific true proportions that vary from maybe 0.225 to 0.325. Thus, only about `r format(v.net,nsmall=4)` of it reflects the variance of the player-specific true (long-term) success rates.

The square root of this, sd = `r format(round(sqrt(v.net),3),nsmall=3)`, broadly fits  with the spead we see in the performances during the remainder of the season: the averages range from `r  round(summary(players$AVE)[1],3)` to `r  round(summary(players$AVE)[6],3)`. 



	
## Frequentist approach

2 separate issues  Ho and intervals
(treated together in Bayesian)

• Statistical Quality Control procedures [for Decisions] • Sample survey organizations: Confidence intervals • Statistical Tests of Hypotheses
Unlike Bayesian inference, there is no quantified pre-test or pre- data "impression"; the ultimate statements are about data, conditional on an assumed null or other hypothesis.
Thus, an explanation of a p-value must start with the conditional "IF the parameter is ... the probability that the data woul



## Does the approach matter?

semantics
