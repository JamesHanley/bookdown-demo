<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Distributions | Introduction to Statistical Analysis: a regression-from-the-outset approach</title>
  <meta name="description" content="A regression-from-the-outset based approach" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Distributions | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A regression-from-the-outset based approach" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Distributions | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  
  <meta name="twitter:description" content="A regression-from-the-outset based approach" />
  

<meta name="author" content="Sahir, Shirin and Jim" />


<meta name="date" content="2020-06-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="randomVariables.html"/>
<link rel="next" href="math.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">In Planning Stage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#target"><i class="fa fa-check"></i><b>0.1</b> Target</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#topicstextbooks"><i class="fa fa-check"></i><b>0.2</b> Topics/textbooks</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#regression-from-the-outset"><i class="fa fa-check"></i><b>0.3</b> Regression from the outset</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#parameters-first-data-later"><i class="fa fa-check"></i><b>0.4</b> Parameters first, data later</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#lets-switch-to-y-bar-and-drop-x-bar."><i class="fa fa-check"></i><b>0.5</b> Let’s switch to “y-bar”, and drop “x-bar”.</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#computing-from-the-outset"><i class="fa fa-check"></i><b>0.6</b> Computing from the outset</a></li>
<li class="chapter" data-level="0.7" data-path="index.html"><a href="index.html#appendix"><i class="fa fa-check"></i><b>0.7</b> Appendix:</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#goals"><i class="fa fa-check"></i><b>1.1</b> Goals</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#structure"><i class="fa fa-check"></i><b>1.2</b> Structure</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#attitudes-etc."><i class="fa fa-check"></i><b>1.3</b> Attitudes, etc….</a></li>
</ul></li>
<li class="part"><span><b>I Part I</b></span></li>
<li class="chapter" data-level="2" data-path="paras.html"><a href="paras.html"><i class="fa fa-check"></i><b>2</b> Statistical Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="paras.html"><a href="paras.html#parameters"><i class="fa fa-check"></i><b>2.1</b> Parameters</a></li>
<li class="chapter" data-level="2.2" data-path="paras.html"><a href="paras.html#parameter-contrasts"><i class="fa fa-check"></i><b>2.2</b> Parameter Contrasts</a><ul>
<li class="chapter" data-level="2.2.1" data-path="paras.html"><a href="paras.html#parameter-relations-in-numbers-and-words"><i class="fa fa-check"></i><b>2.2.1</b> Parameter relations in numbers and words</a></li>
<li class="chapter" data-level="2.2.2" data-path="paras.html"><a href="paras.html#parameter-relations-in-symbols-and-with-the-help-of-an-index-category-indicator"><i class="fa fa-check"></i><b>2.2.2</b> Parameter relations in symbols, and with the help of an index-category indicator</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="paras.html"><a href="paras.html#parameter-functions"><i class="fa fa-check"></i><b>2.3</b> Parameter functions</a></li>
<li class="chapter" data-level="2.4" data-path="paras.html"><a href="paras.html#phraseology-to-avoid"><i class="fa fa-check"></i><b>2.4</b> Phraseology to avoid</a></li>
<li class="chapter" data-level="2.5" data-path="paras.html"><a href="paras.html#summary"><i class="fa fa-check"></i><b>2.5</b> SUMMARY</a></li>
<li class="chapter" data-level="2.6" data-path="paras.html"><a href="paras.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="paras.html"><a href="paras.html#references"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html#the-bayesian-approach"><i class="fa fa-check"></i><b>3.1</b> The Bayesian Approach</a><ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#example-parameter-is-2-valued-yes-or-no"><i class="fa fa-check"></i><b>3.1.1</b> Example: parameter is 2-valued: yes or no</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference.html"><a href="inference.html#example-parameter-is-a-proportion"><i class="fa fa-check"></i><b>3.1.2</b> Example: parameter is a proportion</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference.html"><a href="inference.html#examples-parameter-is-a-personal-number-or-population-mean"><i class="fa fa-check"></i><b>3.1.3</b> Examples: parameter is a personal number or population mean</a></li>
<li class="chapter" data-level="3.1.4" data-path="inference.html"><a href="inference.html#the-bayesian-bottom-line"><i class="fa fa-check"></i><b>3.1.4</b> The Bayesian Bottom Line</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inference.html"><a href="inference.html#frequentist-approach"><i class="fa fa-check"></i><b>3.2</b> Frequentist approach</a><ul>
<li class="chapter" data-level="3.2.1" data-path="inference.html"><a href="inference.html#frequentist-test-of-a-null-hypothesis"><i class="fa fa-check"></i><b>3.2.1</b> (Frequentist) Test of a Null Hypothesis</a></li>
<li class="chapter" data-level="3.2.2" data-path="inference.html"><a href="inference.html#ingredients-and-methods-of-procedure-in-a-statistical-test"><i class="fa fa-check"></i><b>3.2.2</b> Ingredients and methods of procedure in a statistical test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="inference.html"><a href="inference.html#does-the-approach-matter"><i class="fa fa-check"></i><b>3.3</b> Does the approach matter?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Parameter Intervals</a><ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> ‘100% confidence’ intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#more-nuanced-intervals"><i class="fa fa-check"></i><b>4.2</b> More-nuanced intervals</a></li>
<li class="chapter" data-level="4.3" data-path="CI.html"><a href="CI.html#summary-1"><i class="fa fa-check"></i><b>4.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="paraMu.html"><a href="paraMu.html"><i class="fa fa-check"></i><b>5</b> The ‘mean’ parameter <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="paraMu.html"><a href="paraMu.html#two-genres"><i class="fa fa-check"></i><b>5.1</b> Two genres</a></li>
<li class="chapter" data-level="5.2" data-path="paraMu.html"><a href="paraMu.html#fitting-these-to-data-estimating-them-from-data"><i class="fa fa-check"></i><b>5.2</b> Fitting these to data / Estimating them from data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="paraPi.html"><a href="paraPi.html"><i class="fa fa-check"></i><b>6</b> The (proportion) parameter</a><ul>
<li class="chapter" data-level="6.1" data-path="paraPi.html"><a href="paraPi.html#example-one"><i class="fa fa-check"></i><b>6.1</b> Example one</a></li>
<li class="chapter" data-level="6.2" data-path="paraPi.html"><a href="paraPi.html#example-two"><i class="fa fa-check"></i><b>6.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="paraLambda.html"><a href="paraLambda.html"><i class="fa fa-check"></i><b>7</b> The (event rate) parameter</a><ul>
<li class="chapter" data-level="7.1" data-path="paraLambda.html"><a href="paraLambda.html#etc"><i class="fa fa-check"></i><b>7.1</b> Etc</a></li>
<li class="chapter" data-level="7.2" data-path="paraLambda.html"><a href="paraLambda.html#etc-1"><i class="fa fa-check"></i><b>7.2</b> ETC</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html"><i class="fa fa-check"></i><b>8</b> Contrast: 2 mean parameters</a><ul>
<li class="chapter" data-level="8.1" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html#estimand-estimator-estimate"><i class="fa fa-check"></i><b>8.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html"><i class="fa fa-check"></i><b>9</b> Contrast: 2 proportion parameters</a><ul>
<li class="chapter" data-level="9.1" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html#estimand-estimator-estimate-1"><i class="fa fa-check"></i><b>9.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html"><i class="fa fa-check"></i><b>10</b> Contrast: 2 speed parameters</a><ul>
<li class="chapter" data-level="10.1" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html#estimand-estimator-estimate-2"><i class="fa fa-check"></i><b>10.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="part"><span><b>II Part II</b></span></li>
<li class="chapter" data-level="11" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>11</b> Probability</a><ul>
<li class="chapter" data-level="11.1" data-path="probability.html"><a href="probability.html#objectives"><i class="fa fa-check"></i><b>11.1</b> Objectives</a></li>
<li class="chapter" data-level="11.2" data-path="probability.html"><a href="probability.html#probability-scales"><i class="fa fa-check"></i><b>11.2</b> Probability Scales</a></li>
<li class="chapter" data-level="11.3" data-path="probability.html"><a href="probability.html#basic-rules-for-probability-calculations"><i class="fa fa-check"></i><b>11.3</b> Basic rules for probability calculations</a></li>
<li class="chapter" data-level="11.4" data-path="probability.html"><a href="probability.html#conditional-probabilities-and-independence"><i class="fa fa-check"></i><b>11.4</b> Conditional probabilities, and (in)dependence</a></li>
<li class="chapter" data-level="11.5" data-path="probability.html"><a href="probability.html#changing-the-conditioning-the-direction-matters"><i class="fa fa-check"></i><b>11.5</b> Changing the Conditioning: the direction matters</a></li>
<li class="chapter" data-level="11.6" data-path="probability.html"><a href="probability.html#summary-slides"><i class="fa fa-check"></i><b>11.6</b> Summary Slides</a></li>
<li class="chapter" data-level="11.7" data-path="probability.html"><a href="probability.html#exercises-1"><i class="fa fa-check"></i><b>11.7</b> Exercises:</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="randomVariables.html"><a href="randomVariables.html"><i class="fa fa-check"></i><b>12</b> Random Variables/Variation</a><ul>
<li class="chapter" data-level="12.1" data-path="randomVariables.html"><a href="randomVariables.html#objectives-1"><i class="fa fa-check"></i><b>12.1</b> Objectives</a></li>
<li class="chapter" data-level="12.2" data-path="randomVariables.html"><a href="randomVariables.html#random-variables"><i class="fa fa-check"></i><b>12.2</b> Random Variables</a></li>
<li class="chapter" data-level="12.3" data-path="randomVariables.html"><a href="randomVariables.html#expectation-mean-of-a-random-variable"><i class="fa fa-check"></i><b>12.3</b> Expectation (mean) of a Random Variable</a></li>
<li class="chapter" data-level="12.4" data-path="randomVariables.html"><a href="randomVariables.html#expected-value-of-a-function-of-a-random-variable"><i class="fa fa-check"></i><b>12.4</b> Expected value of a FUNCTION of a random variable</a></li>
<li class="chapter" data-level="12.5" data-path="randomVariables.html"><a href="randomVariables.html#variance-and-thus-sd-of-a-random-variable"><i class="fa fa-check"></i><b>12.5</b> Variance (and thus, SD) of a random variable</a><ul>
<li class="chapter" data-level="12.5.1" data-path="randomVariables.html"><a href="randomVariables.html#definitions"><i class="fa fa-check"></i><b>12.5.1</b> Definitions</a></li>
<li class="chapter" data-level="12.5.2" data-path="randomVariables.html"><a href="randomVariables.html#some-good-reasons-for-using-variance-which-averages-the-squares-of-the-deviations-from-the-mean."><i class="fa fa-check"></i><b>12.5.2</b> Some (good) reasons for using variance, which averages the squares of the deviations from the mean.</a></li>
<li class="chapter" data-level="12.5.3" data-path="randomVariables.html"><a href="randomVariables.html#but-for-end-users-today-."><i class="fa fa-check"></i><b>12.5.3</b> But, for end-users today ….</a></li>
<li class="chapter" data-level="12.5.4" data-path="randomVariables.html"><a href="randomVariables.html#example-of-variance-calculation-using-one-pass-formula"><i class="fa fa-check"></i><b>12.5.4</b> Example of Variance-calculation using one-pass formula</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="randomVariables.html"><a href="randomVariables.html#variance-and-sd-of-a-function-of-a-random-variable"><i class="fa fa-check"></i><b>12.6</b> Variance and SD of a FUNCTION of a random variable</a></li>
<li class="chapter" data-level="12.7" data-path="randomVariables.html"><a href="randomVariables.html#sumsmeansdifferences-of-rvs"><i class="fa fa-check"></i><b>12.7</b> Sums/means/differences of RVs</a><ul>
<li class="chapter" data-level="12.7.1" data-path="randomVariables.html"><a href="randomVariables.html#a-sum-of-2-or-n"><i class="fa fa-check"></i><b>12.7.1</b> A sum (of 2 or <span class="math inline">\(n\)</span>)</a></li>
<li class="chapter" data-level="12.7.2" data-path="randomVariables.html"><a href="randomVariables.html#measurement-errors"><i class="fa fa-check"></i><b>12.7.2</b> Measurement Errors</a></li>
<li class="chapter" data-level="12.7.3" data-path="randomVariables.html"><a href="randomVariables.html#mean-of-2-or-n-rvs"><i class="fa fa-check"></i><b>12.7.3</b> Mean (of 2 or <span class="math inline">\(n\)</span> RVs)</a></li>
<li class="chapter" data-level="12.7.4" data-path="randomVariables.html"><a href="randomVariables.html#difference-of-2-rvs"><i class="fa fa-check"></i><b>12.7.4</b> Difference of 2 RVs</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="randomVariables.html"><a href="randomVariables.html#linear-combinations-of-rvs-regression-slopes"><i class="fa fa-check"></i><b>12.8</b> Linear combinations of RVs (regression slopes)</a></li>
<li class="chapter" data-level="12.9" data-path="randomVariables.html"><a href="randomVariables.html#exercises-2"><i class="fa fa-check"></i><b>12.9</b> Exercises</a></li>
<li class="chapter" data-level="12.10" data-path="randomVariables.html"><a href="randomVariables.html#summary-slides-1"><i class="fa fa-check"></i><b>12.10</b> Summary Slides</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>13</b> Distributions</a><ul>
<li class="chapter" data-level="13.1" data-path="distributions.html"><a href="distributions.html#objectives-2"><i class="fa fa-check"></i><b>13.1</b> Objectives</a></li>
<li class="chapter" data-level="13.2" data-path="distributions.html"><a href="distributions.html#named-distributions"><i class="fa fa-check"></i><b>13.2</b> Named Distributions</a><ul>
<li class="chapter" data-level="13.2.1" data-path="distributions.html"><a href="distributions.html#bernoulli"><i class="fa fa-check"></i><b>13.2.1</b> Bernoulli</a></li>
<li class="chapter" data-level="13.2.2" data-path="distributions.html"><a href="distributions.html#binomial"><i class="fa fa-check"></i><b>13.2.2</b> Binomial</a></li>
<li class="chapter" data-level="13.2.3" data-path="distributions.html"><a href="distributions.html#poisson"><i class="fa fa-check"></i><b>13.2.3</b> Poisson</a></li>
<li class="chapter" data-level="13.2.4" data-path="distributions.html"><a href="distributions.html#normal"><i class="fa fa-check"></i><b>13.2.4</b> Normal</a></li>
<li class="chapter" data-level="13.2.5" data-path="distributions.html"><a href="distributions.html#hypergeometric"><i class="fa fa-check"></i><b>13.2.5</b> Hypergeometric</a></li>
<li class="chapter" data-level="13.2.6" data-path="distributions.html"><a href="distributions.html#chi-square"><i class="fa fa-check"></i><b>13.2.6</b> Chi-square</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="distributions.html"><a href="distributions.html#exercises-3"><i class="fa fa-check"></i><b>13.3</b> Exercises</a><ul>
<li class="chapter" data-level="13.3.1" data-path="distributions.html"><a href="distributions.html#clusters-of-miscarriages-based-on-article-by-l-abenhaim"><i class="fa fa-check"></i><b>13.3.1</b> Clusters of Miscarriages [based on article by L Abenhaim]</a></li>
<li class="chapter" data-level="13.3.2" data-path="distributions.html"><a href="distributions.html#prone-ness-to-miscarriages"><i class="fa fa-check"></i><b>13.3.2</b> ‘Prone-ness’ to Miscarriages ?</a></li>
<li class="chapter" data-level="13.3.3" data-path="distributions.html"><a href="distributions.html#automated-chemistries-from-ingelfinger-et-al"><i class="fa fa-check"></i><b>13.3.3</b> Automated Chemistries (from Ingelfinger et al)</a></li>
<li class="chapter" data-level="13.3.4" data-path="distributions.html"><a href="distributions.html#binomial-or-opportunistic-capitalization-on-chance-multiple-looks-at-data"><i class="fa fa-check"></i><b>13.3.4</b> Binomial or Opportunistic? (Capitalization on chance… multiple looks at data)</a></li>
<li class="chapter" data-level="13.3.5" data-path="distributions.html"><a href="distributions.html#can-one-influence-the-sex-of-a-baby"><i class="fa fa-check"></i><b>13.3.5</b> Can one influence the sex of a baby?</a></li>
<li class="chapter" data-level="13.3.6" data-path="distributions.html"><a href="distributions.html#its-the-3rd-week-of-the-course-it-must-be-binomial"><i class="fa fa-check"></i><b>13.3.6</b> It’s the 3rd week of the course: it must be Binomial</a></li>
<li class="chapter" data-level="13.3.7" data-path="distributions.html"><a href="distributions.html#tests-of-intuition"><i class="fa fa-check"></i><b>13.3.7</b> Tests of intuition</a></li>
<li class="chapter" data-level="13.3.8" data-path="distributions.html"><a href="distributions.html#ci-for-proportion-when-observe-0n-or-nn"><i class="fa fa-check"></i><b>13.3.8</b> CI for proportion when observe 0/n or n/n</a></li>
<li class="chapter" data-level="13.3.9" data-path="distributions.html"><a href="distributions.html#neg.-correlations-under-binomial-variation"><i class="fa fa-check"></i><b>13.3.9</b> neg. correlations … under-binomial variation</a></li>
<li class="chapter" data-level="13.3.10" data-path="distributions.html"><a href="distributions.html#weights-of-offspring-pupstwins"><i class="fa fa-check"></i><b>13.3.10</b> weights of offspring (pups/twins)</a></li>
<li class="chapter" data-level="13.3.11" data-path="distributions.html"><a href="distributions.html#suicides"><i class="fa fa-check"></i><b>13.3.11</b> suicides</a></li>
<li class="chapter" data-level="13.3.12" data-path="distributions.html"><a href="distributions.html#horsekicks"><i class="fa fa-check"></i><b>13.3.12</b> horsekicks</a></li>
<li class="chapter" data-level="13.3.13" data-path="distributions.html"><a href="distributions.html#visits-to-dentists-cochran"><i class="fa fa-check"></i><b>13.3.13</b> Visits to dentists (Cochran)</a></li>
<li class="chapter" data-level="13.3.14" data-path="distributions.html"><a href="distributions.html#jhs-steps-per-day"><i class="fa fa-check"></i><b>13.3.14</b> JH’s steps per day</a></li>
<li class="chapter" data-level="13.3.15" data-path="distributions.html"><a href="distributions.html#cd4-counts"><i class="fa fa-check"></i><b>13.3.15</b> CD4 counts</a></li>
<li class="chapter" data-level="13.3.16" data-path="distributions.html"><a href="distributions.html#tweets"><i class="fa fa-check"></i><b>13.3.16</b> tweets</a></li>
<li class="chapter" data-level="13.3.17" data-path="distributions.html"><a href="distributions.html#accidents"><i class="fa fa-check"></i><b>13.3.17</b> accidents</a></li>
<li class="chapter" data-level="13.3.18" data-path="distributions.html"><a href="distributions.html#bootstrap"><i class="fa fa-check"></i><b>13.3.18</b> bootstrap</a></li>
<li class="chapter" data-level="13.3.19" data-path="distributions.html"><a href="distributions.html#no.-of-seats-doors-cyclinders-in-cars"><i class="fa fa-check"></i><b>13.3.19</b> no. of seats / doors / cyclinders in cars</a></li>
<li class="chapter" data-level="13.3.20" data-path="distributions.html"><a href="distributions.html#sex-ratio-in-accidents-fars"><i class="fa fa-check"></i><b>13.3.20</b> sex ratio in accidents (FARS)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Part III</b></span></li>
<li class="chapter" data-level="14" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>14</b> Mathematics</a><ul>
<li class="chapter" data-level="14.1" data-path="math.html"><a href="math.html#notation"><i class="fa fa-check"></i><b>14.1</b> Notation</a></li>
<li class="chapter" data-level="14.2" data-path="math.html"><a href="math.html#powers-logarithms-and-antilogarithms"><i class="fa fa-check"></i><b>14.2</b> Powers, Logarithms and Anti–logarithms</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="computing01.html"><a href="computing01.html"><i class="fa fa-check"></i><b>15</b> Computing Session 1</a><ul>
<li class="chapter" data-level="15.1" data-path="computing01.html"><a href="computing01.html#objectives-3"><i class="fa fa-check"></i><b>15.1</b> Objectives</a></li>
<li class="chapter" data-level="15.2" data-path="computing01.html"><a href="computing01.html#background-to-two-datasets"><i class="fa fa-check"></i><b>15.2</b> Background to two datasets</a><ul>
<li class="chapter" data-level="15.2.1" data-path="computing01.html"><a href="computing01.html#climate"><i class="fa fa-check"></i><b>15.2.1</b> Climate</a></li>
<li class="chapter" data-level="15.2.2" data-path="computing01.html"><a href="computing01.html#ages-of-cars"><i class="fa fa-check"></i><b>15.2.2</b> Ages of Cars</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="computing01.html"><a href="computing01.html#statisticalcomputing-tasks"><i class="fa fa-check"></i><b>15.3</b> Statistical/Computing Tasks</a><ul>
<li class="chapter" data-level="15.3.1" data-path="computing01.html"><a href="computing01.html#guesses-re-date-of-ice-breakup"><i class="fa fa-check"></i><b>15.3.1</b> Guesses re Date of Ice Breakup</a></li>
<li class="chapter" data-level="15.3.2" data-path="computing01.html"><a href="computing01.html#how-old-are-uk-cars"><i class="fa fa-check"></i><b>15.3.2</b> How old are UK cars?</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="computing01.html"><a href="computing01.html#the-p-and-q-functions-an-orientation"><i class="fa fa-check"></i><b>15.4</b> The p and q functions: an orientation</a></li>
<li class="chapter" data-level="15.5" data-path="computing01.html"><a href="computing01.html#shapes-of-distributions"><i class="fa fa-check"></i><b>15.5</b> Shapes of Distributions</a></li>
<li class="chapter" data-level="15.6" data-path="computing01.html"><a href="computing01.html#exercises-4"><i class="fa fa-check"></i><b>15.6</b> Exercises</a><ul>
<li class="chapter" data-level="15.6.1" data-path="computing01.html"><a href="computing01.html#guesses-in-nenana-ice-classic"><i class="fa fa-check"></i><b>15.6.1</b> Guesses in Nenana Ice Classic</a></li>
<li class="chapter" data-level="15.6.2" data-path="computing01.html"><a href="computing01.html#exercise-ages-of-uk-cars"><i class="fa fa-check"></i><b>15.6.2</b> Exercise: Ages of UK Cars</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="computing01.html"><a href="computing01.html#summary-2"><i class="fa fa-check"></i><b>15.7</b> SUMMARY</a><ul>
<li class="chapter" data-level="15.7.1" data-path="computing01.html"><a href="computing01.html#computing"><i class="fa fa-check"></i><b>15.7.1</b> Computing</a></li>
<li class="chapter" data-level="15.7.2" data-path="computing01.html"><a href="computing01.html#statistical-concepts-and-principles"><i class="fa fa-check"></i><b>15.7.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="computing02.html"><a href="computing02.html"><i class="fa fa-check"></i><b>16</b> Computing: Session No. 2</a><ul>
<li class="chapter" data-level="16.1" data-path="computing02.html"><a href="computing02.html#objectives-4"><i class="fa fa-check"></i><b>16.1</b> Objectives</a></li>
<li class="chapter" data-level="16.2" data-path="computing02.html"><a href="computing02.html#scientific-background"><i class="fa fa-check"></i><b>16.2</b> Scientific background</a></li>
<li class="chapter" data-level="16.3" data-path="computing02.html"><a href="computing02.html#random-variation"><i class="fa fa-check"></i><b>16.3</b> Random Variation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="computing02.html"><a href="computing02.html#measurement-errors-1"><i class="fa fa-check"></i><b>16.3.1</b> Measurement errors</a></li>
<li class="chapter" data-level="16.3.2" data-path="computing02.html"><a href="computing02.html#biological-variation"><i class="fa fa-check"></i><b>16.3.2</b> Biological variation</a></li>
<li class="chapter" data-level="16.3.3" data-path="computing02.html"><a href="computing02.html#example-2"><i class="fa fa-check"></i><b>16.3.3</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="computing02.html"><a href="computing02.html#when-these-laws-dont-apply"><i class="fa fa-check"></i><b>16.4</b> When these Laws don’t apply</a></li>
<li class="chapter" data-level="16.5" data-path="computing02.html"><a href="computing02.html#summary-3"><i class="fa fa-check"></i><b>16.5</b> SUMMARY</a><ul>
<li class="chapter" data-level="16.5.1" data-path="computing02.html"><a href="computing02.html#computing-1"><i class="fa fa-check"></i><b>16.5.1</b> Computing</a></li>
<li class="chapter" data-level="16.5.2" data-path="computing02.html"><a href="computing02.html#statistical-concepts-and-principles-1"><i class="fa fa-check"></i><b>16.5.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="computing03.html"><a href="computing03.html"><i class="fa fa-check"></i><b>17</b> Computing: Session No. 3</a><ul>
<li class="chapter" data-level="17.1" data-path="computing03.html"><a href="computing03.html#objectives-5"><i class="fa fa-check"></i><b>17.1</b> Objectives</a></li>
<li class="chapter" data-level="17.2" data-path="computing03.html"><a href="computing03.html#exercises-7"><i class="fa fa-check"></i><b>17.2</b> Exercises</a><ul>
<li class="chapter" data-level="17.2.1" data-path="computing03.html"><a href="computing03.html#pooled-blood"><i class="fa fa-check"></i><b>17.2.1</b> Pooled Blood</a></li>
<li class="chapter" data-level="17.2.2" data-path="computing03.html"><a href="computing03.html#life-tables-1990"><i class="fa fa-check"></i><b>17.2.2</b> Life Tables [1990]</a></li>
<li class="chapter" data-level="17.2.3" data-path="computing03.html"><a href="computing03.html#life-tables-2018"><i class="fa fa-check"></i><b>17.2.3</b> Life Tables [2018]</a></li>
<li class="chapter" data-level="17.2.4" data-path="computing03.html"><a href="computing03.html#a-simplified-epidemic"><i class="fa fa-check"></i><b>17.2.4</b> A simplified epidemic</a></li>
<li class="chapter" data-level="17.2.5" data-path="computing03.html"><a href="computing03.html#screening-for-hiv"><i class="fa fa-check"></i><b>17.2.5</b> Screening for HIV</a></li>
<li class="chapter" data-level="17.2.6" data-path="computing03.html"><a href="computing03.html#duplicate-birthdays"><i class="fa fa-check"></i><b>17.2.6</b> Duplicate Birthdays</a></li>
<li class="chapter" data-level="17.2.7" data-path="computing03.html"><a href="computing03.html#chevalier-de-méré"><i class="fa fa-check"></i><b>17.2.7</b> Chevalier de Méré</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="computing03.html"><a href="computing03.html#other-exercises-under-construction"><i class="fa fa-check"></i><b>17.3</b> Other Exercises (under construction)</a><ul>
<li class="chapter" data-level="17.3.1" data-path="computing03.html"><a href="computing03.html#hiv-transmission"><i class="fa fa-check"></i><b>17.3.1</b> HIV transmission</a></li>
<li class="chapter" data-level="17.3.2" data-path="computing03.html"><a href="computing03.html#the-2018-book-of-guesses"><i class="fa fa-check"></i><b>17.3.2</b> The 2018 Book of Guesses</a></li>
<li class="chapter" data-level="17.3.3" data-path="computing03.html"><a href="computing03.html#trends-over-the-last-100-years"><i class="fa fa-check"></i><b>17.3.3</b> Trends over the last 100 years</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="computing04.html"><a href="computing04.html"><i class="fa fa-check"></i><b>18</b> Computing: Session No. 4</a><ul>
<li class="chapter" data-level="18.1" data-path="computing04.html"><a href="computing04.html#objectives-6"><i class="fa fa-check"></i><b>18.1</b> Objectives</a></li>
<li class="chapter" data-level="18.2" data-path="computing04.html"><a href="computing04.html#exercises-8"><i class="fa fa-check"></i><b>18.2</b> Exercises</a><ul>
<li class="chapter" data-level="18.2.1" data-path="computing04.html"><a href="computing04.html#ages-of-uk-cars"><i class="fa fa-check"></i><b>18.2.1</b> Ages of UK cars</a></li>
<li class="chapter" data-level="18.2.2" data-path="computing04.html"><a href="computing04.html#lengths-of-babies-names"><i class="fa fa-check"></i><b>18.2.2</b> Lengths of Babies’ Names</a></li>
<li class="chapter" data-level="18.2.3" data-path="computing04.html"><a href="computing04.html#life-tables-2018-1"><i class="fa fa-check"></i><b>18.2.3</b> Life Tables [2018]</a></li>
<li class="chapter" data-level="18.2.4" data-path="computing04.html"><a href="computing04.html#variable-length-parallel-parking-spaces"><i class="fa fa-check"></i><b>18.2.4</b> Variable-length (parallel) parking spaces</a></li>
<li class="chapter" data-level="18.2.5" data-path="computing04.html"><a href="computing04.html#galtons-data-on-family-heights"><i class="fa fa-check"></i><b>18.2.5</b> Galton’s data on family heights</a></li>
<li class="chapter" data-level="18.2.6" data-path="computing04.html"><a href="computing04.html#height-differences-of-random-m-f-pairs"><i class="fa fa-check"></i><b>18.2.6</b> Height differences of random M-F pairs</a></li>
<li class="chapter" data-level="18.2.7" data-path="computing04.html"><a href="computing04.html#same-sex-or-opposite-sex"><i class="fa fa-check"></i><b>18.2.7</b> Same-sex or opposite-sex?</a></li>
<li class="chapter" data-level="18.2.8" data-path="computing04.html"><a href="computing04.html#box-minus-bowl"><i class="fa fa-check"></i><b>18.2.8</b> Box minus bowl</a></li>
<li class="chapter" data-level="18.2.9" data-path="computing04.html"><a href="computing04.html#car-parking-fixed-and-variable"><i class="fa fa-check"></i><b>18.2.9</b> car parking fixed and variable</a></li>
<li class="chapter" data-level="18.2.10" data-path="computing04.html"><a href="computing04.html#correcting-length-biased-sampling"><i class="fa fa-check"></i><b>18.2.10</b> Correcting length-biased sampling</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="computing04.html"><a href="computing04.html#other-exercises-under-construction-1"><i class="fa fa-check"></i><b>18.3</b> Other Exercises (under construction)</a><ul>
<li class="chapter" data-level="18.3.1" data-path="computing04.html"><a href="computing04.html#the-2018-book-of-guesses-1"><i class="fa fa-check"></i><b>18.3.1</b> The 2018 Book of Guesses</a></li>
<li class="chapter" data-level="18.3.2" data-path="computing04.html"><a href="computing04.html#trends-over-the-last-100-years-1"><i class="fa fa-check"></i><b>18.3.2</b> Trends over the last 100 years</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="dalite.html"><a href="dalite.html"><i class="fa fa-check"></i><b>19</b> DALITE</a><ul>
<li class="chapter" data-level="19.1" data-path="dalite.html"><a href="dalite.html#aim"><i class="fa fa-check"></i><b>19.1</b> Aim</a></li>
<li class="chapter" data-level="19.2" data-path="dalite.html"><a href="dalite.html#how-it-works"><i class="fa fa-check"></i><b>19.2</b> How it works</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Analysis: a regression-from-the-outset approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="distributions" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Distributions</h1>
<div id="objectives-2" class="section level2">
<h2><span class="header-section-number">13.1</span> Objectives</h2>
<p>Distributions of individual values take their shapes and spreads from the features of the setting, and thus do not follow any general laws. The shapes and the spreads of distributions of statistical summaries and parameter estimates made from aggregates of individual observations tend to be more regular and more predictable, and thus more law-abiding.</p>
<p>So, the <strong>specific objectives</strong> in this chapter are to truly understand</p>
<ul>
<li><p>the distinction between a natural and investigator-made distributions, and between observable and conceptual ones.</p></li>
<li><p>why we should not automatically associate certain distributions with certain types of random variables</p></li>
<li><p>why we need to understand the pre-requisites for random variables following the distributions they do</p></li>
<li><p>why we rely so much on the Normal distribution, and why it is so ‘Central’ to statistical inference concerning parameters.</p></li>
<li><p>why the pre-occupation with checking ‘Normality’ (Gaussian-ness) is misguided</p></li>
<li><p>why Normality is not even relevant when the ‘variable’ is not ‘random’, and appears on the right hand side of a regression model.</p></li>
<li><p>the few contexts where shape does matter</p></li>
</ul>
</div>
<div id="named-distributions" class="section level2">
<h2><span class="header-section-number">13.2</span> Named Distributions</h2>
<p>(in historical order)</p>
<p>Bernoulli [and scaled Bernoulli]</p>
<p>Binomial</p>
<p>Poisson</p>
<p>Normal</p>
<p>t</p>
<p>Wilcoxon</p>
<p>=======</p>
<p>To get a full list of the named distributions available in <code>R</code> you can use the following command: <code>help(Distributions)</code></p>
<div id="bernoulli" class="section level3">
<h3><span class="header-section-number">13.2.1</span> Bernoulli</h3>
<p>The simplest random variable is one that take just 2 possible values, such as YES/NO, MALE/FEMALE, 0/1, ON/OFF, POSITIVE/NEGATIVE, PRESENT/ABSENT, EXISTS/DOES NOT, etc.</p>
<p>This random variable <span class="math inline">\(Y\)</span> is governed by just one parameter, namely the probability, <span class="math inline">\(\pi\)</span>, that it takes on the YES (or ‘1’) value. Of course you can reverse the scale, and speak about the probability of a NO (or ‘0’) result.</p>
<p>It is too bad that when <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Wikipedia</a>, which has a unified way of showing the main features of statistical distributions, does not follow its own principles and show a graph of various versions of a Bernoulli distribution. So here is such a graph.</p>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-66"></span>
<img src="statbook_files/figure-html/unnamed-chunk-66-1.png" alt="Various Bernoulli random variables/distributions. We continue our convention of using the letter Y (instead of X) as the generic name for a random variable. Moreover, in keeping with this view, all of the selected Bernoulli distributions are plotted with their 2 possible values shown on the vertical axis." width="864" />
<p class="caption">
Figure 13.1: Various Bernoulli random variables/distributions. We continue our convention of using the letter Y (instead of X) as the generic name for a random variable. Moreover, in keeping with this view, all of the selected Bernoulli distributions are plotted with their 2 possible values shown on the vertical axis.
</p>
</div>
<p>Please, when reading the Wikipedia entry, replace all instances of <span class="math inline">\(X\)</span> and <span class="math inline">\(x\)</span> by <span class="math inline">\(Y\)</span> and <span class="math inline">\(y\)</span>. Note also that we will use <span class="math inline">\(\pi\)</span> where Wikipedia, and some textbooks, use <span class="math inline">\(p\)</span>. As much as we can, we use Greek letters for parameters and Roman letters for their empirical (sample) counterparts. Also, to be consistent, if the random variable itslef is called <span class="math inline">\(Y\)</span>, then it makes sense to use <span class="math inline">\(y\)</span> as the possible relaizations of it, rather than the illogical <span class="math inline">\(k\)</span> that Wikipedia uses.]</p>
<p>In the sidebar, Wikipedia shows the probability mass function (<em>pmf</em>, the probabilities that go with the possible <span class="math inline">\(Y\)</span> values) in two separate rows, but in the text the <em>pmf</em> is also shown more concisely, as (in our notation)</p>
<p><span class="math display">\[f(y) = \pi^y (1-\pi)^{1-y}, \ \ y = 0, 1.\]</span> If we wish to align with how the <code>R</code> software names features of distributions, we might want to switch from <span class="math inline">\(f\)</span> to <span class="math inline">\(d\)</span>. <code>R</code> uses <span class="math inline">\(d\)</span> because it harmonizes with the probability <span class="math inline">\(d\)</span>ensity function (<em>pdf</em>) notation that its uses for random variables on an interval scale, even though some statistical ‘purists’ see that as mixing terminology: they use the term probablility mass function for discrete random variables, and probablity density function for ones on an interval scale.</p>
<p><span class="math display">\[d_{Bernoulli}(y) = \pi^y (1-\pi)^{1-y}.\]</span></p>
<p>Sadly, Bernoulli does not get its own entry in <code>R</code>’s list of named distributions, presumably because it is a special case of a binomial distribution, one where <span class="math inline">\(n\)</span> = 1. So we have to call <code>dbinom(x,size,prob)</code> to get the density (probability mass) function of the binomial distribution with parameters <code>size</code> (<span class="math inline">\(n\)</span>) and <code>prob</code> (<span class="math inline">\(\pi\)</span>), and set <span class="math inline">\(n\)</span> to 1.</p>
<p>The 3 arguments to <code>dbinom(x,size,prob)</code> are:</p>
<ul>
<li><code>x</code>: a vector of quantiles (here just 0 or/and 1),</li>
<li><code>size</code>: the number of ‘trials’ (our ‘<span class="math inline">\(n\)</span>’, so 1 for Bernoulli), and</li>
<li><code>prob</code>: the probability of ‘success’ on each ‘trial’. We think of it as the probability that a realization of $Y4, i.e, <span class="math inline">\(y\)</span> will equal 1, or as <span class="math inline">\(\pi.\)</span>)</li>
</ul>
<p>Thus, <code>dbinom(x=0,size=1,prob=0.3)</code> yields 0.7, while <code>dbinom(x=1,size=1,prob=0.3)</code> yields 0.3 and <code>dbinom(x=c(0,1),size=1,prob=0.3)</code> yields the vector 0.7, 0.3.</p>
<p>Incidentally, please do not adopt the convention that <span class="math inline">\(x\)</span> (or our <span class="math inline">\(y\)</span>) is the number of ‘successes’ in <span class="math inline">\(n\)</span> trials. It is the number of ‘positives’ in a sample of <span class="math inline">\(n\)</span> independent draws from a population in which a proportion <span class="math inline">\(\pi\)</span> are positive.</p>
<p><strong><em>Expectation (E) and Variance (V)</em> of a Bernoulli random variable.</strong></p>
<p>Shortening <span class="math inline">\(Prob(Y=y)\)</span> to <span class="math inline">\(P_y\)</span>, we have</p>
<ul>
<li>From first principles, <span class="math display">\[E[Y] = 0 \times P_0 + 1 \times P_1 = 0 \times (1-\pi) + 1 \times \pi = \pi,\]</span> while <span class="math display">\[V[Y] = (0-\pi)^2 \times P_0 + (1-\pi)^2 \times P_1  = \pi^2(1-\pi) + (1-\pi)^2\pi =  \underline{\pi(1-\pi)}.\]</span></li>
</ul>
<p>This functional form for the (‘unit’) variance is not entirely surprising: it is obvious from the selected distributions whon that the most concentrated Bernoulli distributions are the ones where the proportion <span class="math inline">\((\pi)\)</span> of Y = 1 values is either close to 1 or to zero, and that the most spread out Bernoulli distributions are the ones where <span class="math inline">\(\pi\)</span> is close to 1/2. And, and a function of <span class="math inline">\(\pi\)</span>, the Variance must be symmetric about <span class="math inline">\(\pi\)</span> = 1/2.</p>
<p>The fact that the greatest uncertainty (‘entropy’, lack of order or predictability) is when <span class="math inline">\(\pi\)</span> = 1/2 is one of the factors that makes sports contests more engaging when teams or players are well matched. Later, when we come to study what influences the imprecision of sample surveys, we will see that for a given sample size, the imprecision is largest when <span class="math inline">\(\pi\)</span> is closer to 1/2.</p>
<p><strong>Why focus on the <em>variance</em> of a Bernoulli random variable?</strong> because, later, when we use the more intesting binomial distribution, we can call on first prionciples to recall what its expectation and variance are. A Binomial random variable is the sum of <span class="math inline">\(n\)</span> independently distributed Bernoulli random variables, all with the same expectation <span class="math inline">\(\pi\)</span> and <strong>unit variance</strong> <span class="math inline">\(\pi(1-\pi).\)</span> Thus its expectation (<span class="math inline">\(E\)</span>) and variance (<span class="math inline">\(V\)</span>) are the <strong>sums of these ‘unit’ versions</strong>, i.e., <span class="math inline">\(E[binom.sum] = n \times \pi\)</span> and <span class="math inline">\(V[binom.sum] = n \times \pi(1-\pi).\)</span> Moreover, again from first principles, we can deduce that if instead of a sample <em>sum</em>, we are interested in a sample <em>mean</em> (here the <em>mean</em> of the 0’s and 1’s is the sample <em>proportion</em>), its expected value is <span class="math display">\[\boxed{\boxed{E[binom.prop&#39;n] = \frac{n \pi}{n} = \pi; \   V[.] = \frac{n  \pi(1-\pi))}{n^2} = \frac{\pi(1-\pi)}{n}; \ SD[.] = \frac{\sqrt{\pi(1-\pi)}}{ \sqrt{n}} = \frac{\sigma_{0,1}}{\sqrt{n}} } }  \]</span></p>
<p>Note here the generic way we write the SD of the sampling distribution of a sample proportion, in the same way that we write the SD of the sampling distribution of a sample mean, as <span class="math inline">\(\sigma_u/\sqrt{n},\)</span> where <span class="math inline">\(\sigma_u\)</span> is the ‘unit’ SD, the standard deviation of the values of <em>individuals</em>. The individual values in the case of a Bernoulli randomn variable are just 0s and 1s, and their SD is <span class="math inline">\(\sqrt{\pi(1-\pi)}.\)</span> We call this SD the SD of the 0’1 and 1’s, or <span class="math inline">\(\sigma_{0,1}\)</span> for short.</p>
<p>Notice how, even though it might look nicer and simpler to compute, and involves just 1 square root calculation, we did not write the SD of a binomial proportion as<br />
<span class="math display">\[SD[binom.proportion] = \sqrt{\frac{\pi(1-\pi)}{ n} }.\]</span> We choose instead to use the <span class="math inline">\(\sigma/\sqrt{n}\)</span> version, to show that it has the same <em>form</em> as the SD for the sampling distribution of a sample mean. Now that we no longer need to savw keystrokes on a hand caloculator, we should move away from computational forms and focus instead on the intuitive form. Sadly, many textbooks re-use the same concept in disjoint chapters without telling readers they are cases of the same SD formula.</p>
<p>There is a lot to be gained by thinking of proportions as means, but where the <span class="math inline">\(Y\)</span> values are just 0’s and 1’s. You can use the <code>R code</code> below to simulate a very large number of 0’s and 1’s, and calculate their variance. The <code>sd</code> function in <code>R</code> doesn’t know or care that the values you supply it are limited to just 0s and 1s, or spread along an interval. Better still don’t use the <code>rbinom</code> function; instead use the <code>sample</code> function, with replacement.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">750</span>

zeros.and.ones =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, n , 
   <span class="dt">prob=</span><span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.8</span>),<span class="dt">replace=</span><span class="ot">TRUE</span> )

m =<span class="st"> </span><span class="kw">matrix</span>(zeros.and.ones,n<span class="op">/</span><span class="dv">75</span>,<span class="dv">75</span>)
<span class="kw">noquote</span>(<span class="kw">apply</span>(m,<span class="dv">1</span>,paste,<span class="dt">collapse=</span><span class="st">&quot;&quot;</span>))</code></pre></div>
<pre><code>##  [1] 111111111110011111111111111111101111111110011101010111111011011101001000110
##  [2] 110011111111111111111111001011111011100111011101111101011101111111111111011
##  [3] 011011110111111111101111011111110111011111000111011111101110111101101011101
##  [4] 110100011111111111011111111011111111111111111101101101111110010011111111111
##  [5] 101111011111101011111111110111011110110111111110011011011010111111010111111
##  [6] 111110111111110010101111111111111101111110001111111111100110110111110110111
##  [7] 011111101111110111111111110111111110011101111111101111110111111111111110011
##  [8] 111110111101010111111111110101101110111010111100111101010110101001001010100
##  [9] 110111100111111100111001111111101111111110101010110101101111111111000111111
## [10] 011111011111111011111101111111011010001011111010101101111000100111111011111</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(zeros.and.ones)<span class="op">/</span>n</code></pre></div>
<pre><code>## [1] 0.7786667</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>( <span class="kw">sd</span>(zeros.and.ones),<span class="dv">4</span>)</code></pre></div>
<pre><code>## [1] 0.4154</code></pre>
<p>Try the above code with a larger <span class="math inline">\(n\)</span> and a different <span class="math inline">\(\pi\)</span> and convince yourself that the variance (and thus the SD) of the individual 0 and 1 values (a) have nothing to do with how many there are and everything to do with what proportion of them are of each type and (b) are larger when the proportions are close to each other, and smaller when they are not.</p>
<p><strong>Scaled-Bernoulli random variables</strong> (to come)</p>
<p><strong>Could we get by without studying the Binomial Distribution?</strong> The answer is ‘for most applications, yes.’ The reason is that in in most cases, we are able to use a Gaussian (Normal) approximation to the binomial distribution. Thus, all we need are its expectation and variance (standard deviation): we don’t need the <code>dbinom()</code> probability mass function, or the <code>pbinom()</code> that gives the cumulative distribution function and thus the tail areas, or the <code>qbinom()</code> function that gives the quantiles. But sometimes we deal with situations where the binomial distributions are not symmetric and close-enough-to-Gaussian.</p>
<p>Below we recount how, in 1738, almost 4 decades before Gauss was born, when summing the probabilities of a binomial distribution with a large <span class="math inline">\(n\)</span>, <a href="https://en.wikipedia.org/wiki/De_Moivre–Laplace_theorem">deMoivre</a> effectively used the as-yet unrecognized ‘Gaussian’ distribution as a very accurate approximation. Without calling it this, he relied on the standard deviation of the binomial distribution.</p>
<p>===</p>
</div>
<div id="binomial" class="section level3">
<h3><span class="header-section-number">13.2.2</span> Binomial</h3>
<p><strong>The Binomial Distribution is a model for the (sampling) variability of a proportion or count in a randomly selected sample</strong></p>
<p><strong>The Binomial Distribution: what it is</strong></p>
<ul>
<li><p>The <span class="math inline">\(n+1\)</span> probabilities <span class="math inline">\(p_{0}, p_{1}, ..., p_{y}, ..., p_{n}\)</span> of observing <span class="math inline">\(y\)</span> = <span class="math inline">\(0, 1, 2, \dots , n\)</span> ‘positives’ in <span class="math inline">\(n\)</span> independent realizations of a Bernoulli random variable <span class="math inline">\(Y\)</span>with probability, <span class="math inline">\(\pi,\)</span> that Y=1, and (1-<span class="math inline">\(\pi\)</span>) that it is 0. The number is the sum of <span class="math inline">\(n\)</span> independen Bernoulli random variables with the same probability, such as in s.r.s of <span class="math inline">\(n\)</span> individuals.</p></li>
<li><p>Each of the <span class="math inline">\(n\)</span> observed elements is binary (0 or 1)</p></li>
<li><p>There are <span class="math inline">\(2^{n}\)</span> possible <em>sequences</em> … but only <span class="math inline">\(n+1\)</span> possible <em>values</em>, i.e. <span class="math inline">\(0/n,\;1/n,\;\dots ,\;n/n\)</span> can think of <span class="math inline">\(y\)</span> as sum of <span class="math inline">\(n\)</span> Bernoulli r. v.’s. [Later on, in ptractive, we will work in the same scale as parameter. i.e., (0,1). not the (0,n) ‘count’ scale.]</p></li>
<li>Apart from <span class="math inline">\(n\)</span>, the probabilities <span class="math inline">\(p_{0}\)</span> to <span class="math inline">\(p_{n}\)</span> depend on only 1 parameter:</li>
<li>the probability that a selected individual will be ‘positive’ i.e., have the trait of interest</li>
<li><p>the proportion of ‘positive’ individuals in the sampled population</p></li>
<li>Usually we denote this (un-knowable) proportion by <span class="math inline">\(\pi\)</span> (or sometimes by the more generic <span class="math inline">\(\theta\)</span>)</li>
<li>Textbooks are not consistent (see below); we try to use Greek letters for parameters,</li>
<li><p>Note Miettinen’s use of UPPER-CASE letters, [e.g. <span class="math inline">\(P\)</span>, <span class="math inline">\(M\)</span>] for <em>PARAMETERS</em> and lower-case letters [e.g., <span class="math inline">\(p\)</span>, <span class="math inline">\(m\)</span>] for <em>statistics</em> (<em>estimates} of parameters</em>).</p></li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">Author(s)</th>
<th align="center">PARAMETER</th>
<th align="center">Statistic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Clayton and Hills</td>
<td align="center"><span class="math inline">\(\pi\)</span></td>
<td align="center"><span class="math inline">\(p = D/n\)</span></td>
</tr>
<tr class="even">
<td align="left">Moore and McCabe, Baldi and Moore</td>
<td align="center"><span class="math inline">\(p\)</span></td>
<td align="center"><span class="math inline">\(\hat{p} = y/n\)</span></td>
</tr>
<tr class="odd">
<td align="left">Miettinen</td>
<td align="center"><span class="math inline">\(P\)</span></td>
<td align="center"><span class="math inline">\(p = y/n\)</span></td>
</tr>
<tr class="even">
<td align="left">This book</td>
<td align="center"><span class="math inline">\(\pi\)</span></td>
<td align="center"><span class="math inline">\(p = D/n\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>Shorthand: <span class="math inline">\(Y \sim Binomial(n, \pi)\)</span> or <span class="math inline">\(y \sim Binomial(n, \pi)\)</span></li>
</ul>
<p><strong>How it arises</strong></p>
<ul>
<li>Sample Surveys</li>
<li>Clinical Trials</li>
<li>Pilot studies</li>
<li>Genetics</li>
<li>Epidemiology</li>
</ul>
<p><strong>Uses</strong></p>
<ul>
<li>to make inferences about <span class="math inline">\(\pi\)</span> from observed proportion <span class="math inline">\(p = y/n\)</span>.</li>
<li><p>to make inferences in more complex situations, e.g. …</p></li>
<li><p>Prevalence Difference: <span class="math inline">\(\pi_{index.category}\)</span> - <span class="math inline">\(\pi_{reference.category}\)</span></p></li>
<li><p>Risk Difference (RD): <span class="math inline">\(\pi_{index.category}\)</span> - <span class="math inline">\(\pi_{reference.category}\)</span></p></li>
<li><p>Risk Ratio, or its synonym Relative Risk (RR): <span class="math inline">\(\frac{\pi_{index.category}}{\pi_{reference.category}}\)</span></p></li>
<li><p>Odds Ratio (OR): <span class="math inline">\(\frac{\pi_{index.category}/(1-\pi_{index.category})}{  \pi_{reference.category}/(1-\pi_{reference.category}) }\)</span></p></li>
<li><p>Trend in several <span class="math inline">\(\pi\)</span>’s</p></li>
</ul>
<p><strong>Requirements for <span class="math inline">\(Y\)</span> to have a Binomial<span class="math inline">\((n, \pi)\)</span> distribution</strong></p>
<ul>
<li><p>Each element in the ‘population’ is 0 or 1, but we are only interested in estimating the proportion (<span class="math inline">\(\pi\)</span>) of 1’s; we are not interested in individuals.</p></li>
<li><p>Fixed sample size <span class="math inline">\(n\)</span>.</p></li>
<li><p>Elements selected at random and independent of each other; each element in population has the same probability of being sampled: i.e., we have <span class="math inline">\(n\)</span> independent Bernoulli random variables with the same expectation (statisticians say ‘<em>i.i.d</em>’ or ‘<em>independent and identically distributed</em>’).</p></li>
<li><p>It helps to distinguish the population values, say <span class="math inline">\(Y_1\)</span> to <span class="math inline">\(Y_N\)</span>, from the <span class="math inline">\(n\)</span> sampled values <span class="math inline">\(y_1\)</span> to <span class="math inline">\(y_n\)</span>. Denote by <span class="math inline">\(y_i\)</span> the value of the <span class="math inline">\(i\)</span>-th sampled element. Prob[<span class="math inline">\(y_i\)</span> = 1] is constant (it is <span class="math inline">\(\pi\)</span>) across <span class="math inline">\(i\)</span>. In the <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Mean-Quantile/inside_outside.pdf">What proportion of our time do we spend indoors?</a> example, it is the <strong>random/blind sampling</strong> of the temporal and spatial patterns of 0s and 1s that <strong>makes <span class="math inline">\(y_1\)</span> to <span class="math inline">\(y_n\)</span> independent of each other</strong>. The <span class="math inline">\(Y\)</span>’s, the elements in the population can be related to each other [e.g. there can be a peculiar spatial distribution of persons] but if elements are chosen at random, the chance that the value of the <span class="math inline">\(i\)</span>-th element chosen is a 1 cannot depend on the value of <span class="math inline">\(y_{i−1}\)</span> or any other <span class="math inline">\(y\)</span>: the sampling is ‘blind’ to the spatial location of the 1’s and 0s.</p></li>
</ul>
<p><strong>Binomial probabilities, illustrated using a Binomial Tree</strong></p>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-68"></span>
<img src="statbook_files/figure-html/unnamed-chunk-68-1.png" alt="From 5 (independent and identically distributed) Bernoulli observations to Binomial(n=5), with the Bernoulli probability left unspecified. There are 2 to the power n possible (distinct) sequences of 0's and 1's, each with its probability. We are not interested in these 2 to the power n probabilities, but in the probability that the sample  contains y 1's and (n-y) 0's. There are only (n+1) possibilities for y, namely 0 to n. Fortunately, each of the n.choose.y sequences that lead to the same sum or count y, has the same probability. So we group the 2.to.power.n sequences into (n+1) sets, according to the sum or count. Each sequence in the set with  y 1's and (n-y) 0's has the same probability, namely  the prob.to.the.power.y times (1-prob).to.the.power.(n-y). Thus, in lieu of adding all such probabilities, we simply multiply this  probability by the number, n.choose-y -- shown in black -- of unique sequences in the set. Check: the frequencies in black add to 2.to.power.n. Nowadays, the (n+1) probabilities are easily obtained by supplying a value for the 'prob' argument in the R function dbinom(), instead of  computing the binomial coefficient n.choose-y by hand." width="864" />
<p class="caption">
Figure 13.2: From 5 (independent and identically distributed) Bernoulli observations to Binomial(n=5), with the Bernoulli probability left unspecified. There are 2 to the power n possible (distinct) sequences of 0’s and 1’s, each with its probability. We are not interested in these 2 to the power n probabilities, but in the probability that the sample contains y 1’s and (n-y) 0’s. There are only (n+1) possibilities for y, namely 0 to n. Fortunately, each of the n.choose.y sequences that lead to the same sum or count y, has the same probability. So we group the 2.to.power.n sequences into (n+1) sets, according to the sum or count. Each sequence in the set with y 1’s and (n-y) 0’s has the same probability, namely the prob.to.the.power.y times (1-prob).to.the.power.(n-y). Thus, in lieu of adding all such probabilities, we simply multiply this probability by the number, n.choose-y – shown in black – of unique sequences in the set. Check: the frequencies in black add to 2.to.power.n. Nowadays, the (n+1) probabilities are easily obtained by supplying a value for the ‘prob’ argument in the R function dbinom(), instead of computing the binomial coefficient n.choose-y by hand.
</p>
</div>
<p>If you rotate the binomial tree to the right by 90 degrees, and use your imagination, you can see how it resembles the <a href="https://en.wikipedia.org/wiki/Quincunx">quincunx</a> constructed by <a href="https://en.wikipedia.org/wiki/Bean_machine">Francis Galton</a>. He used it to show how the Central Linit Theorem, applied to the sum of several ‘Bernoulli deflections to the right and left’, makes a Binomial distribution approach a Gaussian one. Several <a href="https://en.wikipedia.org/wiki/Bean_machine#Games">games</a> and game shows are built on this pinball machine, for example, <a href="https://fivethirtyeight.com/features/what-if-god-were-a-giant-game-of-plinko/">Plinko</a> and, more recently, <a href="https://www.nbc.com/the-wall?nbc=1">The Wall</a>. Galton’s quincunx has its own cottage industry, and versions of it often displayed in Science Museums. The present authors inherited a low tech version of the <a href="http://www.galtonboard.com">Galton Board</a>, where the ‘shot’ are turnip seeds, from former McGill <a href="https://www.mcgill.ca/medicine/staff-resources/inmemoriam/2003">Professor – and early teacher of course 607 – FDK Liddell</a>.</p>
<p><strong>Calculating Binomial probabilities</strong></p>
<p><em>Exactly</em></p>
<ul>
<li><p>probability mass function (p.m.f.) :</p></li>
<li><p>formula: <span class="math inline">\(Prob[y] = \  ^n C _y \ \pi^y \  (1 − \pi)^{n−y}\)</span>.</p></li>
<li><p>recursively: <span class="math inline">\(Prob[0] = (1−\pi)^n\)</span>;    <span class="math inline">\(Prob[y] = \frac{n−y+1}{y} \times \frac{\pi}{1-\pi} \times Prob[y−1]\)</span>.</p></li>
<li>Statistical Packages:</li>
<li><p>R functions <code>dbinom()</code>, <code>pbinom()</code>, <code>qbinom()</code><br />
probability mass, distribution/cdf, and quantile functions.</p></li>
<li><p>Stata function <code>Binomial(n,k,p)</code></p></li>
<li><p>SAS <code>PROBBNML(p, n, y)</code> function</p></li>
<li><p>Spreadsheet — Excel function <code>BINOMDIST(y, n, π, cumulative)</code></p></li>
<li><p>Tables: CRC; Fisher and Yates; Biometrika Tables; Documenta Geigy</p></li>
</ul>
<p><em>Using an approximation</em></p>
<ul>
<li><p>Poisson Distribution (<span class="math inline">\(n\)</span> large; small <span class="math inline">\(\pi\)</span>)</p></li>
<li><p><strong>Normal (Gaussian) Distribution</strong> (<span class="math inline">\(n\)</span> large or midrange <span class="math inline">\(\pi\)</span>, so that the expected value, <span class="math inline">\(n \times \pi\)</span>, is sufficiently far ‘in from’ the ‘edges’ of the scale, i.e., sufficiently far in from 0 and from <span class="math inline">\(n\)</span>, so that a Gaussian distribution doesn’t flow past one of the edges. The Normal approximation is good for when you don’t have access to software or Tables, e.g, on a plane, or when the internet is down, or the battery on your phone or laptop had run out, or it takes too long to boot up Windows!).<br />
To use the Normal approximatiom, be aware of the <strong>scale you are working in</strong>, .e.g., if say <span class="math inline">\(n = 10\)</span>, whether the summary is a <strong>count</strong> or a <strong>proportion</strong> or a <strong>percentage</strong>.</p></li>
</ul>
<table>
<thead>
<tr class="header">
<th align="right"></th>
<th align="left">r.v.</th>
<th align="right">e.g.</th>
<th align="center">E</th>
<th align="center">S.D.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>count</strong>:</td>
<td align="left"><span class="math inline">\(y\)</span></td>
<td align="right">2</td>
<td align="center"><span class="math inline">\(n \times \pi\)</span></td>
<td align="center"><span class="math inline">\(\{n \times \pi \times (1-\pi) \}^{1/2}\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="left"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center">i.e., <span class="math inline">\(n^{1/2} \times \sigma_{Bernoulli}\)</span></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="left"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="right"><strong>proportion</strong>:</td>
<td align="left"><span class="math inline">\(p=y/n\)</span></td>
<td align="right">0.2</td>
<td align="center"><span class="math inline">\(\pi\)</span></td>
<td align="center"><span class="math inline">\(\{\pi \times (1-\pi) / n \}^{1/2}\)</span></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="left"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center">i.e., <span class="math inline">\(\sigma_{Bernoulli} / n^{1/2}\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="left"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="right"><strong>percentage</strong>:</td>
<td align="left"><span class="math inline">\(100p\%\)</span></td>
<td align="right">20%</td>
<td align="center"><span class="math inline">\(100 \times \pi\)</span></td>
<td align="center"><span class="math inline">\(100 \times SD[p]\)</span></td>
</tr>
</tbody>
</table>
<p>The first person to suggest an approximation, using what we now call the ‘Normal’ or ‘Gaussian’ of ‘Laplace-Gaussian’ distribution, was <a href="http://www.biostat.mcgill.ca/hanley/statbook/TheDoctrineOfChancesAnnotated.pdf">deMoivre, in 1738</a>. There is a debate among historians as to whether this marks the first description of the Normal distribution: the piece does not explicitly point to the probability density function <span class="math inline">\(\frac{1}{\sigma \sqrt{2 \pi}} \times exp[-z^2/2\sigma^2],\)</span> but it does highlight the role of the quantity <span class="math inline">\((1/2) \times \sqrt{n}\)</span>, the standard deviation of the sum of <span class="math inline">\(n\)</span> independent Bernoulli random variables, each with expectation 1/2 and thus a ‘unit’ standard deviation of 1/2, and also the SD quantity <span class="math inline">\(\sqrt{\pi(1-\pi)}\)</span> <span class="math inline">\(\times\)</span> <span class="math inline">\(\sqrt{n}\)</span> in the more general case. DeMoivre arrived at the familiar ‘68-95-99.7 rule’ : the percentages of a normal distribution that lie within 1, 2 and 3 SD’s of its mean.</p>
<p><strong>Factors that modulate the shapes of Binomial distributions</strong></p>
<ul>
<li><p>size of <span class="math inline">\(n\)</span>: the larger the n, the more symmetric</p></li>
<li><p>value of <span class="math inline">\(\pi\)</span>: the closer to 1/2, the more symmetric</p></li>
</ul>
<p>In these small-<span class="math inline">\(n\)</span> contexts, only those distribtions where <span class="math inline">\(\pi\)</span> is close to 0.5 are reasonably symmetric.</p>
<p>In larger-<span class="math inline">\(n\)</span> contexts (see below), as long as there is ‘room’ for them to be, binomial distribtions where the expected value <span class="math inline">\(E = n \times \pi\)</span> is at least 5-10 ‘in from the edges’ (i.e. to the right of 0, or the left of <span class="math inline">\(n\)</span>, are reasonably symmetric.</p>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-70"></span>
<img src="statbook_files/figure-html/unnamed-chunk-70-1.png" alt="Binomial random variables/distributions, where n = 5, and the Bernoulli expectation (probability) is smaller (left panels) or larger (right panels)." width="864" />
<p class="caption">
Figure 13.3: Binomial random variables/distributions, where n = 5, and the Bernoulli expectation (probability) is smaller (left panels) or larger (right panels).
</p>
</div>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-71"></span>
<img src="statbook_files/figure-html/unnamed-chunk-71-1.png" alt="Various Binomial random variables/distributions, where n = 20. The dotted horizontal lines in light blue are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units from the (0,n) boundaries are shown in blue." width="864" />
<p class="caption">
Figure 13.4: Various Binomial random variables/distributions, where n = 20. The dotted horizontal lines in light blue are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units from the (0,n) boundaries are shown in blue.
</p>
</div>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-72"></span>
<img src="statbook_files/figure-html/unnamed-chunk-72-1.png" alt="Various Binomial random variables/distributions, where n = 50. The blue dotted lines are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units in from the (0,n) boundaries are shown in blue" width="864" />
<p class="caption">
Figure 13.5: Various Binomial random variables/distributions, where n = 50. The blue dotted lines are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units in from the (0,n) boundaries are shown in blue
</p>
</div>
<p>Back when binomial computations were difficult, and the normal approximation was not accurate, there was another approximation that saved labour, and in particular, avoided having to deal with the very large numbers involved in the <span class="math inline">\(^nC_y\)</span> binomial coefficients (even if built into a hand calculator, these can be problematic when <span class="math inline">\(n\)</span> is large).</p>
<p><a href="http://www.biostat.mcgill.ca/hanley/statbook/Poisson1837Excerpts.pdf">Poisson</a>, in 1837, having shown how to use the Normal distribution for binomial (and the closely-related negative binomial) calculations, devoted 1 small section (81) of less than 2 pages, to the case where (in our notation) <strong>one of the two chances <span class="math inline">\(\pi\)</span> or <span class="math inline">\((1-\pi)\)</span> <em>‘est très petite’</em></strong> [Poisson’s <span class="math inline">\(q\)</span> is our <span class="math inline">\(\pi\)</span>, his <span class="math inline">\(\mu\)</span> is our <span class="math inline">\(n\)</span>, his <span class="math inline">\(\omega\)</span> is our <span class="math inline">\(\mu\)</span>, and his <span class="math inline">\(n\)</span> is our <span class="math inline">\(y\)</span>]. In our notation, he arrived at <span class="math display">\[Prob[y \ or \ fewer \ events] = \bigg(1 + \frac{\mu}{1!} + \frac{\mu^2}{2!} + \dots + \frac{\mu^y}{y!} \bigg) \  e^{-\mu}.\]</span></p>
<p>In the last of his just three paragraphs on this digression, he notes the probability <span class="math inline">\(e^{-\mu}\)</span> of no event, and <span class="math inline">\(1 - e^{-\mu}\)</span> of at least 1. He also calculates that, when <span class="math inline">\(\mu\)</span> = 1, the chance is very close to 1 in 100 million that more than <span class="math inline">\(y\)</span> = 10 events would occur in a very large series of trials (of length <span class="math inline">\(n\)</span>), where the probability is 1/<span class="math inline">\(n\)</span> in each trial. Although he give little emphasis to his formula, and no real application, it is Poisson’s name that is now undividely associated with this formula.</p>
<p>Below are some examples of how well it approximates a Binomial in the ‘corner’ where <span class="math inline">\(\pi\)</span> is very small and <span class="math inline">\(n\)</span> is very large. If, of course, the product, <span class="math inline">\(\mu = n \times \pi\)</span> , reaches double digits, the Normal approximation distribution provides and accurate approximation for both the Binomial and the Poisson distributions, and – if one has ready acccess to the tail areas of a Normal distribution – with less effort. Today, of course, unless you are limited to a hand calculator when the internet and R and paper tables are nor available, you would not need to use any approximation, Normal or Poisson, to a binomial.</p>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-74"></span>
<img src="statbook_files/figure-html/unnamed-chunk-74-1.png" alt="Various Binomial random variables/distributions, with large n's and small Bernoulli probabilities, together with the Poisson distributions with the corresponding means. The Poisson distrubution provides a good approximation in the 'lowee corner of the Bimonial distribution with large n's and small probabilities. And, when the product, mu = n * probability, is in the double digits, the Normal approximation distribution provides and accurate approximation for both the Binomial and the Poisson distributions." width="864" />
<p class="caption">
Figure 13.6: Various Binomial random variables/distributions, with large n’s and small Bernoulli probabilities, together with the Poisson distributions with the corresponding means. The Poisson distrubution provides a good approximation in the ’lowee corner of the Bimonial distribution with large n’s and small probabilities. And, when the product, mu = n * probability, is in the double digits, the Normal approximation distribution provides and accurate approximation for both the Binomial and the Poisson distributions.
</p>
</div>
<p><strong>When the Binomial does not apply</strong></p>
<p>It does not apply if one (or both) of the ‘<strong>i.i.d.</strong>’ (<strong>independent</strong> and <strong>identical</strong>) conditions do not hold. The first of these conditions is often the one that is absent.</p>
<p>Two very nice <a href="http://www.biostat.mcgill.ca/hanley/statbook/CochranTable.png">examples</a> can be found in Cochran’s (old but still excellent) <a href="http://www.biostat.mcgill.ca/hanley/statbook/Cochran3rdEdition.png">textbook on sampling</a>. They were re-visited a few decades years ago in connection with what was (then) a new technique for dealing with <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/sim.pdf#page=2">correlated responses.</a></p>
<p>The table shows, for each of the 30 randomly sampled households, the number of household members had visited a physician in the previous year. Can we base the precision of the observed proportion, 30/104 or 28%, on a binomial distribution where <span class="math inline">\(n\)</span> = 104?</p>
<p>True, the ‘<em>n</em>’ in each household is not the same from household to household, but we can segregate the households by size, and carry out separate binomial calculations for each, all the time assuming a common binomial proportion across houdeholds.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">XLAB =<span class="st"> &quot;y: number in household who has visited an MD in last year&quot;</span>
YLAB =<span class="st"> &quot;Number of Households</span><span class="ch">\n</span><span class="st">for which y persons visited an MD&quot;</span>

<span class="kw">show.household.survey.data</span>(n.who.visited.md,XLAB,YLAB)</code></pre></div>
<pre><code>## [1] No. persons with history/trait of interest: 30 / 104 ; Prop&#39;n: 0.29
## [1]  
## Below, the row labels are the household sizes.
## Each column label is the number in a household with history/trait of interest.
## The entries are how many households had the indicated configuration.
## 
##           0    1    2    3    4    5    6    7    Total
## 1         1    .    .    .    .    .    .    .        1
## 2         3    .    1    .    .    .    .    .        4
## 3         7    2    1    2    .    .    .    .       12
## 4         4    1    3    .    1    .    .    .        9
## 5         .    .    1    .    .    1    .    .        2
## 6         1    .    .    .    .    .    .    .        1
## 7         1    .    .    .    .    .    .    .        1
## Total    17    3    6    2    1    1    0    0       30
## 
## Same row/col meanings, but theoretical, binomial-based,
## EXPECTED frequencies of households having these many persons
##  who would have -- if all 128 persons were independently sampled
## from a population where the proportion with 
## the history/trait of interest was as above.
## 
##           0     1     2     3     4     5     6     7 Total
## 1     0.712 0.288     .     .     .     .     .     .     1
## 2     1.013 0.821 0.166     .     .     .     .     .     4
## 3     1.081 1.314 0.533 0.072     .     .     .     .    12
## 4     1.025 1.663 1.011 0.273 0.028     .     .     .     9
## 5     0.912 1.849 1.499 0.608 0.123 0.010     .     .     2
## 6     0.779 1.894 1.920 1.038 0.315 0.051 0.003     .     1
## 7     0.646 1.834 2.231 1.507 0.611 0.149 0.020 0.001     1
## Total 6.167 9.663 7.360 3.498 1.077 0.210 0.024 0.001    30
## 
## We will just assess the fit on the &#39;totals&#39;row:
##  the numbers in the individual rows are too small to judge.
## 
##         0   1   2   3   4   5 6 7 Total
## [1,] 17.0 3.0 6.0 2.0 1.0 1.0 0 0    30
## [2,]  6.2 9.7 7.4 3.5 1.1 0.2 0 0    30
## 
## Better still, here is a picture:</code></pre>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-76"></span>
<img src="statbook_files/figure-html/unnamed-chunk-76-1.png" alt="Of 30 randomly sampled households, (O)bserved  numbers of households, shown in grey, where 0, 1, .. in household had visited  a physician in the previous year. The 30 households contained 104 persons, 28 of whom had visited a physician in the previous year. Also shown, in blue are the (E)xpected numbers of households, assuming the data were generated from 104 independent Bernoulli random variables, each with the same  probability 28/104. The observed variance is considerably LARGER than that predicted by a binomial distribution. You would see this even if the individuals in the house were not from the same family. For example, if the occupants were students, the proportion of them with such a history would be different (?lower) than if the occupants were older: this is the 'non-identical probabilities' aspect. The other possibility, the 'non-independence' aspect, is that health status and the seeking medical care are affected by shared family factors, such as behaviours, attitudes, lifestyle, and insurance coverage." width="864" />
<p class="caption">
Figure 13.7: Of 30 randomly sampled households, (O)bserved numbers of households, shown in grey, where 0, 1, .. in household had visited a physician in the previous year. The 30 households contained 104 persons, 28 of whom had visited a physician in the previous year. Also shown, in blue are the (E)xpected numbers of households, assuming the data were generated from 104 independent Bernoulli random variables, each with the same probability 28/104. The observed variance is considerably LARGER than that predicted by a binomial distribution. You would see this even if the individuals in the house were not from the same family. For example, if the occupants were students, the proportion of them with such a history would be different (?lower) than if the occupants were older: this is the ‘non-identical probabilities’ aspect. The other possibility, the ‘non-independence’ aspect, is that health status and the seeking medical care are affected by shared family factors, such as behaviours, attitudes, lifestyle, and insurance coverage.
</p>
</div>
<p>Cochran provides <a href="http://www.biostat.mcgill.ca/hanley/statbook/CochranHouseholdSurvey.png">this explanation</a> for the <strong>greater than binomial variation</strong> in the proportions</p>
<blockquote>
<p>The variance given by the ratio method, 0.00520, is much larger than that given by the binomial formula, 0.00197. For various reasons, families differ in the frequency with which their members consult a doctor. For the sample as a whole, the proportion who consult a doctor is only a little more than one in four, but there are several families in which every member has seen a doctor. Similar results would be obtained for any characteristic in which the members of the same family tend to act in the same way.</p>
</blockquote>
<p>Clearly, if we are dealing with an <strong>infectious disease</strong>, we would need to be very <strong>careful about our statistical model</strong> for the numbers in a family or household or care facility that are affected.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">XLAB =<span class="st"> &quot;y: number of males in household&quot;</span>
YLAB=<span class="st">  &quot;Number of Households</span><span class="ch">\n</span><span class="st"> where y persons were male&quot;</span>
<span class="kw">show.household.survey.data</span>(n.males,XLAB,YLAB)</code></pre></div>
<pre><code>## [1] No. persons with history/trait of interest: 53 / 104 ; Prop&#39;n: 0.51
## [1]  
## Below, the row labels are the household sizes.
## Each column label is the number in a household with history/trait of interest.
## The entries are how many households had the indicated configuration.
## 
##          0     1    2    3    4    5    6    7    Total
## 1        1     .    .    .    .    .    .    .        1
## 2        .     4    .    .    .    .    .    .        4
## 3        .     7    5    .    .    .    .    .       12
## 4        .     1    3    5    .    .    .    .        9
## 5        .     1    .    1    .    .    .    .        2
## 6        .     .    .    1    .    .    .    .        1
## 7        .     .    .    1    .    .    .    .        1
## Total    1    13    8    8    0    0    0    0       30
## 
## Same row/col meanings, but theoretical, binomial-based,
## EXPECTED frequencies of households having these many persons
##  who would have -- if all 128 persons were independently sampled
## from a population where the proportion with 
## the history/trait of interest was as above.
## 
##           0     1     2     3     4     5     6     7 Total
## 1     0.490 0.510     .     .     .     .     .     .     1
## 2     0.481 1.000 0.519     .     .     .     .     .     4
## 3     0.354 1.103 1.146 0.397     .     .     .     .    12
## 4     0.231 0.962 1.499 1.038 0.270     .     .     .     9
## 5     0.142 0.737 1.531 1.591 0.827 0.172     .     .     2
## 6     0.083 0.520 1.352 1.873 1.460 0.607 0.105     .     1
## 7     0.048 0.347 1.083 1.875 1.949 1.215 0.421 0.062     1
## Total 1.829 5.178 7.130 6.775 4.505 1.994 0.526 0.062    30
## 
## We will just assess the fit on the &#39;totals&#39;row:
##  the numbers in the individual rows are too small to judge.
## 
##        0    1   2   3   4 5   6   7 Total
## [1,] 1.0 13.0 8.0 8.0 0.0 0 0.0 0.0    30
## [2,] 1.8  5.2 7.1 6.8 4.5 2 0.5 0.1    30
## 
## Better still, here is a picture:</code></pre>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-77"></span>
<img src="statbook_files/figure-html/unnamed-chunk-77-1.png" alt="Of 30 randomly sampled households, (O)bserved  numbers of households, shown in grey, where 0, 1, .. in household were male. The 30 households contained 104 persons, 53 of whom were male. Also shown, in blue are the (E)xpected numbers of households, assuming the data were generated from 104 independent Bernoulli random variables, each with the same  probability 53/104. The observed variance is considerably SMALLER than that predicted by a binomial distribution. You would see this even if the individuals in the house were not from the same family. For example, if the occupants were students, the proportion of them with such a history would be different (?lower) than if the occupants were older: this is the 'non-identical probabilities' aspect. The other possibility, the 'non-independence' aspect, is that health status and the seeking medical care are affected by shared family factors, such as behaviours, attitudes, lifestyle, and insurance coverage." width="864" />
<p class="caption">
Figure 13.8: Of 30 randomly sampled households, (O)bserved numbers of households, shown in grey, where 0, 1, .. in household were male. The 30 households contained 104 persons, 53 of whom were male. Also shown, in blue are the (E)xpected numbers of households, assuming the data were generated from 104 independent Bernoulli random variables, each with the same probability 53/104. The observed variance is considerably SMALLER than that predicted by a binomial distribution. You would see this even if the individuals in the house were not from the same family. For example, if the occupants were students, the proportion of them with such a history would be different (?lower) than if the occupants were older: this is the ‘non-identical probabilities’ aspect. The other possibility, the ‘non-independence’ aspect, is that health status and the seeking medical care are affected by shared family factors, such as behaviours, attitudes, lifestyle, and insurance coverage.
</p>
</div>
<p>Cochran provides <a href="http://www.biostat.mcgill.ca/hanley/statbook/CochranHouseholdSurvey.png">this explanation</a> for the <strong>(unsually) less than binomial variation</strong> seen in this response variable.</p>
<blockquote>
<p>In estimating the proportion of males in the population, the results are different. By the same type of calculation, we find binomial formula: v(p) = 0.00240; ratio formula v(p) = 0.00114<br />
Here the <strong>binomial formula overestimates the variance</strong>. The reason is interesting. Most households are set up as a result of a marriage, hence contain at least one male and one female. Consequently the proportion of males per family varies less from one half than would be expected from the binomial formula. None of the 30 families, except one with only one member, is composed entirely of males, or entirely of females. <strong>If the binomial distribution were applicable, with a true P of approximately one half, households with all members of the same sex would constitute one quarter of the households of size 3 and one eighth of the households of size 4. This property of the sex ratio has been discussed by Hansen and Hurwitz (1942). Other illustrations of the error committed by improper use of the binomial formula in sociological investigations have been given by <a href="http://www.biostat.mcgill.ca/hanley/statbook/Kish1957.pdf">Kish (1957)</a>.</strong></p>
</blockquote>
<p><strong>Does the Binomial Distribution apply if… ?</strong></p>
<table style="width:44%;">
<colgroup>
<col width="27%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="left">Interested in:</td>
<td align="right"><span class="math inline">\(\pi\)</span></td>
<td align="left">the proportion of 16 year old girls in Quebec protected against rubella</td>
</tr>
<tr class="even">
<td align="left">Choose:</td>
<td align="right"><span class="math inline">\(n\)</span> = 100</td>
<td align="left">girls: 20 at random from each of 5 randomly selected schools [‘cluster’ sample]</td>
</tr>
<tr class="odd">
<td align="left">Count</td>
<td align="right"><span class="math inline">\(y\)</span></td>
<td align="left">how many of the <span class="math inline">\(n\)</span> = 100 are protected</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left"><span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y ~ \sim Binomial(n=100, \ \pi)\)</span> ?</td>
</tr>
<tr class="odd">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="even">
<td align="left">‘SMAC’:</td>
<td align="right"><span class="math inline">\(\pi\)</span></td>
<td align="left">Chemistry Auto-analyzer with n = 18 channels. Critertion for ‘positivity’ set so that Prob[‘abnormal’ result in Healthy person] = 0.03 for each of 18 chemistries tested</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Count</td>
<td align="right"><span class="math inline">\(y\)</span></td>
<td align="left">(In 1 patient) how many of <span class="math inline">\(n\)</span> = 18 give abnormal result.</td>
</tr>
<tr class="odd">
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left"><span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y ~ \sim Binomial(n=18, \ \pi=0.03)\)</span> ? <a href="https://www.amazon.ca/Biostatistics-Clinical-Medicine-Joseph-Ingelfinger/dp/0023597216">credit: Ingelfinger textbook</a></td>
</tr>
<tr class="odd">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="even">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="odd">
<td align="left">Sex Ratio:</td>
<td align="right"><span class="math inline">\(n=4\)</span></td>
<td align="left">children in each family</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"><span class="math inline">\(y\)</span></td>
<td align="left">number of girls in family</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="left"><span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y ~ \sim Binomial(n=4, \ \pi=0.49)\)</span> ?</td>
</tr>
<tr class="even">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="odd">
<td align="left">Interested in:</td>
<td align="right"><span class="math inline">\(\pi_u\)</span></td>
<td align="left">proportion in ‘usual’ exercise classes and in</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"><span class="math inline">\(\pi_e\)</span></td>
<td align="left">expt’l. exercise classes who ‘stay the course’</td>
</tr>
<tr class="odd">
<td align="left">Randomly</td>
<td align="right">4</td>
<td align="left">classes of</td>
</tr>
<tr class="even">
<td align="left">Allocate</td>
<td align="right">25</td>
<td align="left">students each to usual course</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"><span class="math inline">\(n_u = 4 \times 25 = 100\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"><span class="math inline">\(n_e\)</span> = 4</td>
<td align="left">classes of</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">25</td>
<td align="left">students each to experimental course</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"><span class="math inline">\(n_e = 4 \times 25 =100\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Count</td>
<td align="right"><span class="math inline">\(y_u\)</span></td>
<td align="left">how many of the <span class="math inline">\(n_u\)</span> = 100 complete course</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"><span class="math inline">\(y_e\)</span></td>
<td align="left">how many of the <span class="math inline">\(n_e\)</span> = 100 complete course</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="left"><span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y_u ~ \sim Binomial(n = 100, \ \pi_u)\)</span> ?</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left"><span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y_e ~ \sim Binomial(n = 100, \ \pi_e)\)</span> ?</td>
</tr>
<tr class="odd">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="even">
<td align="left">Pilot Study:</td>
<td align="right"></td>
<td align="left">To estimate proportion <span class="math inline">\(\pi\)</span> of population that is eligible and willing to participate in long-term research study, keep recruiting until obtain</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"><span class="math inline">\(y\)</span> = 5</td>
<td align="left">who are. Have to approach <span class="math inline">\(n\)</span> to get <span class="math inline">\(y\)</span>.</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left"><span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y ~ \sim Binomial(n, \ \pi)\)</span> ?</td>
</tr>
<tr class="odd">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
</tbody>
</table>
</div>
<div id="poisson" class="section level3">
<h3><span class="header-section-number">13.2.3</span> Poisson</h3>
<p>model for (Sampling) Variability of a Count in a given amount of ‘experience’</p>
<p>Today, Poisson’s distribution is seldom used as a computational shortcut to a binomial probability. Although Poisson did not appreciate it at the time, today his distribution has to do with the <strong>distribution of counts of events in a given amount of space or time or population-time</strong>. However, most mathematical statistics textbooks still derive the Poisson distribution as the limiting case of a Binomial.</p>
<p>Below, we will take a broader view, emphasize the ‘<strong>time</strong>’ dimension, and give some history. We will also focus on the many contexts when the Poisson distribution is <strong>too narrow</strong>, and thus an inappropriate model for counts. We will repeat the earlier message that <strong>just because a random variable involves counts doesn’t make its distribution Poisson</strong>. Examples of this include the fallacy of treating the distributions of white blood cell, or CD4 cell concentrations as if there were observed counts. And even if they are ‘real’ counts, the distribution (variability) of one’s step-counts acrross the day or across days, or of the numbers of births or deaths or car-crashes across the day, or across days, are usually far-from-Poisson.</p>
<p><strong>Indeed, we will take the attitude that counts rarely follow a Poisson distribution.</strong></p>
<p><strong>The Poisson Distribution: what it is, and some of its features</strong></p>
<ul>
<li><p>The (infinite number of) probabilities <span class="math inline">\(P_{0}, P_{1}, ..., P_{y}, ...,\)</span> of observing <span class="math inline">\(Y = 0, 1, 2, \dots , y, \dots\)</span> ‘events’/‘instances’ in a given amount of ‘experience.’</p></li>
<li><p>These probabilities, <span class="math inline">\(Prob[Y = y]\)</span>, or <span class="math inline">\(P_{Y}[y]\)</span>’s, or <span class="math inline">\(P_{y}\)</span>’s for short, are governed by a single parameter, the mean <span class="math inline">\(E[Y] = \mu.\)</span></p></li>
<li><p><span class="math inline">\(P[y]= \exp[-\mu]\: \frac{\mu^{y}}{y!};\)</span>  in <code>R</code>: <code>dpois( )</code> – too bad <span class="math inline">\(\mu\)</span> is referred to as <code>lambda</code>.</p></li>
<li><p>Shorthand: <span class="math inline">\(Y\sim\; \textrm{Poisson}(\mu)\)</span>.</p></li>
<li><p><span class="math inline">\(\sigma^2_Y = \textrm{Variance}[Y] = \mu \ ; \ \ \sigma_Y = \sqrt{\mu_{Y}}.\)</span></p></li>
<li><p>It can be approximated by <span class="math inline">\(N(\mu, \: \sigma_{Y} = \mu^{1/2})\)</span> when <span class="math inline">\(\mu &gt;&gt; 10\)</span>.</p></li>
<li><p>It is open-ended (unlike Binomial), but in practice, has finite range.</p></li>
<li><p>Poisson data are sometimes called ‘numerator only’: (unlike in the Binomial) you may not ‘see’ or count ‘non-events’: but there is (an invisible) denominator ‘behind’ the number of ‘wrong number’ phone calls you receive.</p></li>
</ul>
<p><strong>How it arises / derivations</strong></p>
<ul>
<li><p>Count of events (items) that occur randomly, with low homogeneous intensity, in time, space, or ‘item’-time (e.g. person–time).</p></li>
<li><p>As a limiting case of a Binomial(<span class="math inline">\(n,\pi\)</span>) when <span class="math inline">\(n \rightarrow \infty\textrm{ and } \pi \rightarrow 0,\)</span> but <span class="math inline">\(n \times \pi = \mu\)</span> is finite. [Poisson, in 1837, derived it as an approximation to the closely-related negative binomial distribution.]</p></li>
<li><p><span class="math inline">\(Y\sim Poisson(\mu_{Y}) \Leftrightarrow T \textrm{ time between events}\sim \: \textrm{Exponential}(\mu_{T} = 1/\mu_{Y}).\)</span> Here is <a href="http://www.epi.mcgill.ca/hanley/bios601/Intensity-Rate/Randomness_poisson.pdf">one attempt to explain this</a> and here is <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/Accromath-2015-1-4.pdf">another</a> together with this <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Mean-Quantile/forAccromathBackTranslate.pdf">translation and additional annotation</a></p></li>
<li><p>As the sum of <span class="math inline">\(\ge 2\)</span> <em>independent</em> Poisson rv’s, with the same <strong>or different</strong> <span class="math inline">\(\mu\)</span>’s:</p></li>
<li><p><span class="math inline">\(Y_{1} \sim \textrm{Poisson}(\mu_{1}) \: \: Y_{2} \sim \textrm{Poisson}(\mu_{2}) \Rightarrow Y = Y_{1} + Y_{2} \sim \textrm{Poisson}(\mu_{1}+\mu_{2}).\)</span></p></li>
<li><p>Examples: numbers of asbestos fibres, deaths from horse kicks, needle-stick or other percutaneous injuries, bus-driver accidents, twin-pairs, radioactive disintegrations, flying-bomb hits*, white blood cells, typographical errors, ‘wrong numbers’, cancers; chocolate chips, radioactive emissions in nuclear medicine, cell occupants – in a given volume, area, line-length, population-time, time, etc. <a href="http://www.epi.mcgill.ca/hanley/bios601/Intensity-Rate/">* included in</a></p></li>
</ul>
<p><strong>Calculating Poisson probabilities:</strong></p>
<ul>
<li>Exactly</li>
<li><code>R: dpois, ppois, qpois rpois</code> mass/cum./quant./rand<br />
</li>
<li>`SAS: POISSON;</li>
<li>`Stata: <br />
</li>
<li><p>Spreadsheet — Excel function <code>POISSON}</code> (<span class="math inline">\(y, \mu\)</span>, cumulative)</p></li>
<li><p>Using Gaussian Approximations to distribution of <span class="math inline">\(y\)</span> or transforms of it: described below, under Inference.</p></li>
<li><p>In ‘the old days’ i.e. in the years BC (<strong>B</strong>efore <strong>C</strong>omputers’), Poisson <em>tail probabilities</em> were often calculated using links to the tail areas of other better-tabulated continuous distributions, such as the Chi-Square distribution.</p></li>
</ul>
<p><strong>Some History</strong> (see <a href="http://www.epi.mcgill.ca/hanley/statbook/HaightHistorical.pdf">Haight</a> ).</p>
<ul>
<li>1718 deMoivre : the series <span class="math inline">\(e^{-\mu} \bigg(1 + \frac{\mu}{1!} + \frac{\mu^2}{2!} + \dots \bigg)\)</span> appears.</li>
<li>1837 Poisson (see above)</li>
<li>1860 <a href="https://en.wikipedia.org/wiki/Simon_Newcomb">Simon Newcomb</a>, a Canadian-born ‘astronomer, applied mathematician and autodidactic polymath who who was Professor of Mathematics in the United States Navy and at Johns Hopkins University, <a href="http://www.epi.mcgill.ca/hanley/statbook/Newcomb1860.pdf">derives the ’Poisson’ Law ’from scratch’</a> as a limiting case of the Binomial, and applies it to the determination of the ’probability that, if the stars were scattered at random over the heavens, any small space selected at random would contain <span class="math inline">\(s\)</span> stars.’</li>
<li>1878 <a href="http://www.biostat.mcgill.ca/hanley/statbook/Abbe1878.pdf">Abbe</a> determined the number of cells one needed to count in order to achieve a sufficiently small ‘probable error’ for an estimated blood cell concentration.</li>
<li>1905 <a href="http://www.biostat.mcgill.ca/hanley/statbook/SchweidlerPremiercongres1905.pdf">Schweidler</a> provided error calculations for counting radioactive transformations.</li>
<li>1989 <a href="http://www.biostat.mcgill.ca/hanley/statbook/dasGesetzDerKeinemZaklenBortkiewicz.pdf">Bortkewitsch book</a>, with many examples, and tables of the Poisson Distribution.</li>
<li>1907 <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Intensity-Rate/Student_counting.pdf">‘Student’</a> derived the Poisson distribution from scratch (as a liniting case of the binomial) and provided error calculations for measuring yeast cells in beer.</li>
<li>1909 Erlang, a Danish engineer concerned with the theory of telephone traffic, developed the Poisson Law by an elegant argument based on the steady state behaviour of the beginning and terminations of calls. Here are the original <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/statbook/ErlangInDanish1909.pdf">Danish version</a> and a <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/statbook/Erlang1909.pdf">translation</a>. This <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/statbook/ErlangSimpleProofPoisson.pdf#page=14">‘simple explanation’</a> in the English translation of a 1920 article has a critical cut and paste typographical error. The original <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/statbook/38MatematiskTidsskrift1920.pdf">Danish version</a> has it correct.</li>
<li>1910 <a href="https://www.physics.mcgill.ca/museum/rutherfordcollection.html">Rutherford</a> (having departed McGill in 1907) and Geiger (of the ‘counter’), physicists, and Bateman, a mathematician, developed the <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Intensity-Rate/RutherfordGeigerBateman1910.pdf">Poisson Law by an time-based argument</a> expressed as a differential equation, a Law that agreed with the empirical distibution in their large particle-counting experiment.</li>
<li>1946 <a href="https://www.actuaries.org.uk/system/files/documents/pdf/0481.pdf">Flying-bomb attacks on London during WW II</a>, along with 2019 Update: <a href="https://www.significancemagazine.com/10-news/627-finalists-announced-for-early-career-writing-award?highlight=WyJib21icyIsImJvbWJzJyJd">Significance Magazine Early-career Writing Award</a> announcement and <a href="https://www.significancemagazine.com/10-news/635-a-world-war-ii-investigation-updated-for-the-modern-era?highlight=WyJzaGF3Iiwic2hhdydzIl0=">intro to</a> and <a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2019.01315.x">full article</a>.</li>
<li>Commentators:<br />
</li>
<li><a href="http://www.biostat.mcgill.ca/hanley/statbook/Newbold1927.pdf">Newbold</a> gives a lot of credit to deMoivre.</li>
<li><a href="http://www.biostat.mcgill.ca/hanley/statbook/HaightHistorical.pdf">Haight</a> is the mots comprehensive.</li>
<li><a href="http://www.biostat.mcgill.ca/hanley/statbook/StiglerPoisson.pdf">Stigler</a> is a bit more nuanced.</li>
</ul>
<div id="detailed-example-1-distribution-of-particles-in-volumes" class="section level4">
<h4><span class="header-section-number">13.2.3.1</span> <strong>DETAILED EXAMPLE 1: Distribution of particles in volumes</strong></h4>
<p>The following two distributions are taken from ’Student’s 1907 article on <strong>counting yeast cells</strong> under a haemocytometer, an instrument for visual counting of the number of cells in a blood sample or other fluid under a microscope. In the article, he derived what today we would call the Poisson distribution, and offered guidance on how many fields/cells to count in order the ensure a small enough margin of error when estimating a concentration.</p>
<p>Student’s main messages</p>
<blockquote>
<p>The same accuracy is obtained by counting the same number of particles whatever the dilution. Hence the most accurate way is to dilute the solution to the point at which the particles may be counted most rapidly, and to count as many as time permits.</p>
</blockquote>
<blockquote>
<p>The standard deviation of the mean number of particles per unit volume is <span class="math inline">\(\sqrt{m/M},\)</span> where <span class="math inline">\(m\)</span> is the mean number and M the number of unit volumes counted.</p>
</blockquote>
<blockquote>
<p>This theoretical work was tested on four distributions which had been counted over the whole 400 squares of the haemocytometer. The particles counted were yeast cells which were killed by adding a little mercuric chloride to the water in which they had been shaken up. A small quantity of this was mixed with a 10% solution of gelatine, and after being well stirred up drops were put on the haemacytometer. This was then put on a plate of glass kept at a temperature just above the setting point of gelatine and allowed to cool slowly till the gelatine had set. Four different concentrations were used. <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Intensity-Rate/Student_counting.pdf#page=6">The full description can be found here.</a></p>
</blockquote>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-78"></span>
<img src="statbook_files/figure-html/unnamed-chunk-78-1.png" alt="Examples of distributions of Yeast Cells over 1 sq. mm. divided into 400 squares. The data in the right panel are based on the 400 counts in Table 1 of Student's article (concentration IV), while those in the left panel are simulated to match the reported mean (1.32 per square) and variance (1.28) for the 400 counts at concentration II. It appears that Student calculated the variance using a divisor of n rather than n-1." width="864" />
<p class="caption">
Figure 13.9: Examples of distributions of Yeast Cells over 1 sq. mm. divided into 400 squares. The data in the right panel are based on the 400 counts in Table 1 of Student’s article (concentration IV), while those in the left panel are simulated to match the reported mean (1.32 per square) and variance (1.28) for the 400 counts at concentration II. It appears that Student calculated the variance using a divisor of n rather than n-1.
</p>
</div>
<p>The data in the panel on the right panel can be <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Intensity-Rate/Student_counting.pdf#page=10">viewed at the bottom of this page.</a></p>
<p>If we were asked to randomly distribute items over the 400 squares and have the means and standard distribitions match the observed ones, but to do it without the aid of a formal statistical procedure, most of us would do a poor job. <strong>Humans are not very good at making up random sequences</strong>. (see elsewhere, in the computing exercises). JH tested this a few decades ago by asking elementary school students to mark six spots on a 6-49 ticket – when it uses to be mainly played using <a href="https://lottery.olg.ca/en-ca/lotto-games/lotto-649/about-lotto-649">paper tickets</a>, the 49 numbers were laid our as a 5 x 10 grid. They, and <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/c607/ch01/lotto_numbers_selected.pdf">adults likewise</a>, tend to avoid the ‘border cells’ and choose more in the middle of the grid.</p>
<p>Likewise, <strong>people are not good at detecting <em>non-randomness</em></strong>. (Without peeking at the statistical summaries under row A, or at row B or at row C, or elsewhere in the article) see if you can spot the non-randomness in one of the 3 panels (1969, 1970 1971) in <a href="http://www.biostat.mcgill.ca/hanley/statbook/LestWeForgetSelectiveServiceLotteries19172019.pdf#page=7">row (A) of Figure 4 in this article.</a>. Hint: one of the 3 has a problem. and see if you would make a good radiologist.</p>
<p>Student shook the liquid vigorously and took care to avoid ‘clumping’ of cells, and other artifacts, and measured the (as they turned out to be, small) correlations between the nunber of cells on a square and the number in each of the four cells nearest it.</p>
<p>He noticed that in both concentrations I and III the variance was greater than the mean, due to an [to us, slight] excess (of the “observed”) over the calculated (“fitted”) among the high numbers in the tail of the distribution. [Mean = 0.6825, variance 0.8117 in I; Mean = 1.80; variance = 1.96 in III]. He has pointed out earlier in the article that the budding of the yeast cell increases these high numbers, and there is also probably a tendency to stick together in groups which was not altogether abolished even by vigorous shaking.</p>
<p>He also tested the observed frequency distribution of the 400 counts (they varied from 0 to 6 in the left panel, and 1 to 12 in the right panel) against the expected numbers of 0’s, 1’s, 2’s, etc. under the (Poisson) law using a <span class="math inline">\(\chi^2\)</span> goodness of fit statistic, and found them to be reasonable. ‘The p-values, 0.04, 0.68, 0.25 and 0.64, though not particularly high, are not at all unlikely in four trials, supposing our theoretical law to hold, and we are not likely to be very far wrong in assuming it to do so.’</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-79"></span>
<img src="images/StudentPoissonFits.png" alt="Fits of Poisson Frequencies to the 4 sets of yeast cell counts made by Student." width="690" />
<p class="caption">
Figure 13.10: Fits of Poisson Frequencies to the 4 sets of yeast cell counts made by Student.
</p>
</div>
<p>Clearly, the narrow pure-Poisson variation we see in Student’s counts is a tribute to his ‘vigorous shaking’ that produced the randomness. [Think about how you would ensure this with raisins in dough, or chocolate chips in cookies.]</p>
</div>
<div id="detailed-example-2-distribution-of-events-in-time-intervals" class="section level4">
<h4><span class="header-section-number">13.2.3.2</span> <strong>DETAILED EXAMPLE 2: Distribution of events in time intervals</strong></h4>
<p>This next example – on <strong>counting</strong> <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Intensity-Rate/RutherfordGeigerBateman1910.pdf">alpha-radiation disintegrations</a> – has the same element of randomness, but in the <strong>time dimension</strong>. Here is Rutherford’s description:</p>
<blockquote>
<p>The source of radiation was a small disk coated with polonium, which was placed inside an exhausted tube, closed at one end by a zinc sulphide screen. The scintillations were counted in the usual way by means of a microscope on an area of about one sq. mm. of screen. During the timo of counting (5 days), in order to correct for the decay, the polonium was moved daily closer to the screen in order that the average number of <span class="math inline">\(\alpha\)</span> particles impinging on the screen should be nearly constant. The scintillations were recorded on a chronograph tape by closing an electric circuit by hand at the instant of each scintillation. Time-marks at intervals of one half-minute were also automatically recorded on iho same tape.<br />
After the eye was rested, scintillations were counted from 3 to 5 minutes. The motor running the tape was then stopped and the eye rested for several minutes ; then another interval of counting, and so on. It was found possible to count 2000 scintillations a day, and in all 10,000 were recorded. The records on the tape were then systematically examined. The length of tape corresponding to half-minute marks was subdivided into four equal parts by means of a celluloid film marked with five parallel lines at equal distances. By slanting the film at different angles, the outside lines were made to pass through the time-marks, and thee number of scintillations between the lines corresponding to 1/8 minute intervals were counted through the film. By this method correction was made for slow variations in the speed of the motor during the long interval required by the observations.</p>
</blockquote>
<p>The black dots on the winding line are meant to look like the marks on a 5-minute strip of the paper tape described by Rutherford.</p>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-80"></span>
<img src="statbook_files/figure-html/unnamed-chunk-80-1.png" alt="Simulated  distributions of events (scintillations produced  by a radioactive source) over a 5-minute time-span. Each 1/8th of a minute is shown is a different colour. The counts in each 7.5 second bin are shown at the bottom." width="864" />
<p class="caption">
Figure 13.11: Simulated distributions of events (scintillations produced by a radioactive source) over a 5-minute time-span. Each 1/8th of a minute is shown is a different colour. The counts in each 7.5 second bin are shown at the bottom.
</p>
</div>
<p>The <strong>full distribution</strong> (over 2608 intervals of 7.5 seconds duration each, i.e almost 5.5 hours in all), along with the fitted one, is <strong>graphed</strong> at the <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Intensity-Rate/RutherfordGeigerBateman1910.pdf#page=6">bottom of this page,</a> and <strong>tabulated</strong> <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Intensity-Rate/RutherfordGeigerBateman1910.pdf#page=5">on this page.</a>. For convenience, the two are combined in the following graph.</p>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-81"></span>
<img src="images/RutherfordTableFig.png" alt="Table and Figure from Rutherford, Geiger and Bateman, 1910." width="860" />
<p class="caption">
Figure 13.12: Table and Figure from Rutherford, Geiger and Bateman, 1910.
</p>
</div>
<p>Today, we could rework his data, and replot his Figure using <code>R</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y =<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">14</span>; freq =<span class="kw">c</span>(<span class="dv">57</span>,<span class="dv">203</span>,<span class="dv">383</span>,<span class="dv">525</span>,<span class="dv">532</span>,<span class="dv">408</span>,<span class="dv">273</span>,<span class="dv">139</span>,<span class="dv">45</span>,<span class="dv">27</span>,<span class="dv">10</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>))

<span class="kw">plot</span>(y,freq,
  <span class="dt">ylab=</span><span class="st">&quot;Number of intervals&quot;</span>,
  <span class="dt">xlab=</span><span class="st">&quot;y: number counted in 7.5 sec interval&quot;</span>,<span class="dt">type=</span><span class="st">&quot;h&quot;</span>,
  <span class="dt">lend=</span><span class="st">&quot;butt&quot;</span>, <span class="dt">lwd=</span><span class="dv">16</span>,<span class="dt">col=</span><span class="st">&quot;grey75&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.35</span>)

n =<span class="st"> </span><span class="kw">sum</span>(freq)
mean =<span class="st"> </span><span class="kw">sum</span>(freq<span class="op">*</span>y)<span class="op">/</span>n
var =<span class="st">  </span><span class="kw">sum</span>(freq<span class="op">*</span>y<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>n <span class="op">-</span><span class="st"> </span>mean<span class="op">^</span><span class="dv">2</span>
theoretical.freq =<span class="st"> </span><span class="kw">sum</span>(freq)<span class="op">*</span><span class="kw">dpois</span>(y,mean)

<span class="kw">lines</span>(y,theoretical.freq, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>,
      <span class="dt">lend=</span><span class="st">&quot;butt&quot;</span>, <span class="dt">lwd=</span><span class="dv">6</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)

<span class="kw">text</span>(<span class="dv">12</span>,<span class="dv">400</span>, <span class="kw">paste</span>(<span class="st">&quot;Mean(y): &quot;</span>,<span class="kw">toString</span>(<span class="kw">round</span>(mean,<span class="dv">2</span>)),
           <span class="st">&quot;</span><span class="ch">\n</span><span class="st">Var(y): &quot;</span>,<span class="kw">toString</span>(<span class="kw">round</span>(var,<span class="dv">2</span>)),<span class="dt">sep=</span><span class="st">&quot;&quot;</span>), 
     <span class="dt">adj=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">cex=</span><span class="fl">1.5</span>,<span class="dt">family=</span><span class="st">&quot;mono&quot;</span>  )

<span class="kw">text</span>(<span class="dv">5</span>,theoretical.freq[<span class="dv">6</span>],<span class="st">&quot;E&quot;</span>,
     <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">adj=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="fl">0.5</span>),<span class="dt">cex=</span><span class="fl">1.25</span>)
<span class="kw">text</span>(<span class="dv">5</span>,freq[<span class="dv">6</span>],<span class="st">&quot;O&quot;</span>,
     <span class="dt">col=</span><span class="st">&quot;grey65&quot;</span>, <span class="dt">adj=</span><span class="kw">c</span>(<span class="fl">1.6</span>,<span class="fl">0.5</span>),<span class="dt">cex=</span><span class="fl">1.25</span>)</code></pre></div>
<div class="figure" style="text-align: left"><span id="fig:unnamed-chunk-82"></span>
<img src="statbook_files/figure-html/unnamed-chunk-82-1.png" alt="Rutherford's Fig 1, replotted, and the discrete scale emphasized. O =' 'Observed' frequencies; E = 'Expected' or 'Theoretical' or 'Fitted' frequencies. Rutherford et al. derived the  theoretical distribution (blue) 'from scratch' by a very different method than Poisson (see 'Note' on pages 704-707 of their article), and made no reference to Poisson's formula." width="864" />
<p class="caption">
Figure 13.13: Rutherford’s Fig 1, replotted, and the discrete scale emphasized. O =‘’Observed’ frequencies; E = ‘Expected’ or ‘Theoretical’ or ‘Fitted’ frequencies. Rutherford et al. derived the theoretical distribution (blue) ‘from scratch’ by a very different method than Poisson (see ‘Note’ on pages 704-707 of their article), and made no reference to Poisson’s formula.
</p>
</div>
<p><strong>2020 re-enactment</strong></p>
<p>This <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Rutherford/minute-001.mp4">video</a> is JH’s ‘recreation’ of Rutherford and Geiger’s experiment. Since you are unlikely to be as motivated as their ‘counters’, the video is limited to a 7 second lead-in + 70 seconds of flashes. If you find it easier to count while listening rather than watching, here is an <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Rutherford/minute-001.wav">audio version.</a></p>
<p>Rutherford et al. counted an average of 3.89 flashed per 7.5 seconds, or approx 0.5 per second. The video has 24 frames per second, so their average would be approx 1 per 48 frames. Thus in one frame, the expected number of flashes would be 1/48, and so one can approximate the process by modelling the result in each frame as the realization of a Bernoulli random variable, with prob = 1/48 (or slightly hogher if we wanted to more closely match their average of just over 31 flashes per minute). In theory, there could be 2 (or even 3) flashes in a 1/24 second interval, but (as we will see below when we examine the Poisson probabilities) the chances of these are small. (We could of course have make the video smoother, by having say 50 frames a second, and an average of 1 flash per 100 frames. This would diminish the chance of 2 or more flashes in a frame, but it would make the video file larger, and the higher frame rate would diminish the chance of seeing a flash in a single frame – we haven’t tested the limits of detection. We could also, of course, have put the – very occasional – 2 flashes in two locations in the same 1/24 sec frame.)</p>
</div>
<div id="from-biologicalphysics-laboratories-to-human-population-time" class="section level4">
<h4><span class="header-section-number">13.2.3.3</span> <strong>FROM BIOLOGICAL/PHYSICS LABORATORIES to HUMAN POPULATION-TIME</strong></h4>
<p>We should not automatically assume that because they also involve counts, the numbers of human events in various segments of population time wiull follow a nice tight single Poisson distribution. Student (Gosset) could distribute the yeast cells randomly by shaking them; and Rutherford could maintian the intensity of the stream of recorded atomic disintegrations constant by managing the distance between the source and the target. Such (experimental) controls are not possible in human situations that are affeceted by changing behavioural and environmental conditions.</p>
<p>The first example from Bortkewitsch involve termporal variations in the counts of a quite rare condition – suicide, but there are several reasons not to equate these variations with those of Rutherford. First, they occurred over 25 years, rather than just within a total of just 5 hours spread over just five days. In that time, several things could have changed: the size of the source population might have changed somewhat; the economic and religious conditions are likely to have changed. there is the possibility of ‘copycat’ suicides, or multiple suicides. the attitudes to the reporting of suicide might have changed, and there must have been many more recorders/counters that the few that Rutherford used. Below, we will look at contemporary data from the USA, where many similar issues could be raised.</p>
<p>Note an extra feature: these events emerged from (occurred on) 2 segments of the population time .. the female population-time and the male population-time. This would be like having two radiation-counting studies proceeding simulataneously, one using polonium, and another using radium. The expected rates (the expected numbers of events per unit time) would be different.</p>
<p><strong><em>Example 1</em></strong>, From <a href="http://www.biostat.mcgill.ca/hanley/statbook/dasGesetzDerKeinemZaklenBortkiewicz.pdf#page=51">Bortkewitsch, 1898</a></p>
<p>The table below contains the numbers of boys (Knaben) and girls (Mädchen) under 10 years of age in Prussia in the period 1869-1893 who committed suicide.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Jayr =<span class="st"> </span><span class="dv">1869</span><span class="op">:</span><span class="dv">1893</span>

n.boys =<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">4</span>)
n.girls=<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)

ds=<span class="kw">data.frame</span>(Jayr,n.boys,n.girls)

<span class="kw">cat</span>(<span class="st">&quot;Generating the &#39;x&#39; and &#39;l_x&#39; columns on p. 17 of the book&quot;</span>)</code></pre></div>
<pre><code>## Generating the &#39;x&#39; and &#39;l_x&#39; columns on p. 17 of the book</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">( <span class="dt">freq =</span> <span class="kw">table</span>(ds<span class="op">$</span>n.boys) )</code></pre></div>
<pre><code>## 
## 0 1 2 3 4 6 
## 4 8 5 3 4 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y =<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">8</span>  <span class="co"># to align with Bortkewitsch&#39;s table of fitted frequencies</span>

Observed.freq =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(y) )

indices =<span class="st"> </span><span class="kw">as.numeric</span>( <span class="kw">dimnames</span>(freq)[[<span class="dv">1</span>]] )

Observed.freq[<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>indices] =<span class="st"> </span>freq

Fitted.freq =<span class="st"> </span><span class="kw">round</span>(<span class="kw">length</span>(ds<span class="op">$</span>n.boys) <span class="op">*</span><span class="st"> </span><span class="kw">dpois</span>(y, <span class="kw">mean</span>(ds<span class="op">$</span>n.boys) ),<span class="dv">1</span>) 

<span class="kw">cbind</span>(y, Observed.freq, Fitted.freq )</code></pre></div>
<pre><code>##       y Observed.freq Fitted.freq
##  [1,] 0             4         3.5
##  [2,] 1             8         6.9
##  [3,] 2             5         6.8
##  [4,] 3             3         4.4
##  [5,] 4             4         2.2
##  [6,] 5             0         0.8
##  [7,] 6             1         0.3
##  [8,] 7             0         0.1
##  [9,] 8             0         0.0</code></pre>
<p>USA</p>
<p><strong><em>Example 2</em></strong>, From <a href="http://www.biostat.mcgill.ca/hanley/statbook/dasGesetzDerKeinemZaklenBortkiewicz.pdf#page=51">Bortkewitsch, 1898</a></p>
<pre><code>## Female suicides in eight German states
## 
## The table below shows the number of female suicides in
## every calendar year from 1881 to 1894 in the following states
## 
## a) Schaumburg-Lippe, b) Waldeck, c) Lubeck, d) Reufs a. L., 
## e) Lippe, f) Schwarzburg-Rudolstadt, g) Mecklenburg- Strelitz
## h) Schwarzburg-Sondershausen.)
## 
## source: Allgemeines Statistisches Archiv, 4. Jahrgang, II. Hbd.,
## 1896. Art. Der Selbstmord von G. v. Mayr, S. 718.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">as.table</span>(number.female.suicides),<span class="dt">zero.print =</span> <span class="st">&quot;.&quot;</span>)</code></pre></div>
<pre><code>##   1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894
## a    .    2    .    2    .    .    3    3    1    3    1    3    1    1
## b    2    3    1    2    2    .    4    1    3    1    5    3    1    3
## c    3    2    1    4    3    .    3    2    3    4    1    1    4    6
## d    5    2    3    3    3    6    4    1    4    2    2    1    1    .
## e    4    1    1    6    1    4    6    .    6    4    .    3    3    2
## f    5    1    8    6    6    3    5    7    5    7    6    5    3    5
## g    1    3    4    4   10    9    4    2    8    9    4    8    6    2
## h    2    5    5    2    2    4   10    2    6    9    9    4    9   10</code></pre>
<p>We turn Table 1 into one that indicates indirectly how many times in each row of table 1 (and in all lines taken together) the annual result 0, 1, 2, etc. occurs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">FREQ =<span class="st"> </span><span class="kw">as.table</span>(freq)
<span class="kw">colnames</span>(FREQ) =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>((<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(freq[<span class="dv">1</span>,])<span class="op">-</span><span class="dv">1</span>),toString))
<span class="kw">rownames</span>(FREQ) =<span class="st"> </span><span class="kw">rownames</span>(tab) 
<span class="kw">print</span>(FREQ, <span class="dt">zero.print =</span> <span class="st">&quot;.&quot;</span>, <span class="dt">print.gap=</span><span class="dv">4</span>)</code></pre></div>
<pre><code>##         0    1    2    3    4    5    6    7    8    9    10
## [1,]    4    4    2    4    .    .    .    .    .    .     .
## [2,]    1    4    3    4    1    1    .    .    .    .     .
## [3,]    1    3    2    4    3    .    1    .    .    .     .
## [4,]    1    3    3    3    2    1    1    .    .    .     .
## [5,]    2    3    1    2    3    .    3    .    .    .     .
## [6,]    .    1    .    2    .    5    3    2    1    .     .
## [7,]    .    1    2    1    4    .    1    .    2    2     1
## [8,]    .    .    4    .    2    2    1    .    .    3     2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>( <span class="kw">as.table</span>(<span class="kw">apply</span>(freq,<span class="dv">2</span>,sum)) ,
       <span class="dt">zero.print =</span> <span class="st">&quot;.&quot;</span>, <span class="dt">print.gap=</span> <span class="dv">4</span> )</code></pre></div>
<pre><code>##  A     B     C     D     E     F     G     H     I     J     K    
##  9    19    17    20    15     9    10     2     3     5     3</code></pre>
<p>The latter table shows the effective dispersion of the annual results. nisse expressed. The corresponding expected dispersion, calculated on the basis of the mean values, obtained by division of the numbers in the last column of Table 1 obtained by 14 are like this:</p>
<p>The latter table If one looks at the last lines of tables 2 and 3, it shows themselves, with a few exceptions, which are due to this may have that the elements under consideration are very little numerous, a very satisfactory statistical match Experience with forecasting the theory. If you pull the x values’) 0 - 2 into a first one, the x values 3 - 4 in a second and the x values 5 and more in a third group together, it is found that the first group is expectant 45.2, in reality 45 x values are omitted, to the second 33.8 or 35 and on the third 33.0 resp. 32. A summary expression is found in the current example</p>
<p><strong><em>Example 3</em></strong>, From <a href="http://www.biostat.mcgill.ca/hanley/statbook/dasGesetzDerKeinemZaklenBortkiewicz.pdf#page=55">Bortkewitsch, 1898</a></p>
<p>The numbers of daily incidents for eleven professional cooperatives.</p>
<p>In the table below are the for a 9 year period Numbers of work accidents with a daily outcome, which are at the professional associations concerned have occurred every year, introduced. Of those based on the Accident Insurance Act of On July 6, 1884, professional associations set up those for which the statistics show the smallest numbers of such accidents proves. The professional associations are not by their name, which are not important here, but according to the regulatory numbers with which they appear in the statistical publications’) are provided.</p>
<p>Number.work.accidents = t ( matrix( c( 6, 8, 7, 5, 14, 8, 9, 4, 8, 2, 2, 2, 1, 1, 3, 5, 3, 4, 0, 1, 2, 2, 5, 0, 2, 7, 4, 3, 3, 5, 3, 10, 2, 5, 4, 4, 3, 9, 6, 11, 6, 8, 5, 4, 4, 1, 2, 2, 3, 1, 1, 1, 4, 2, 4, 8, 4, 3, 8, 3, 7, 4, 12, 2, 5, 1, 3, 2, 0, 0, 6, 7, 1, 3, 5, 4, 6, 7, 5, 8, 7, 5, 6, 5, 5, 3, 4, 0, 8, 5, 5, 5, 2, 2, 7, 5, 3, 6, 4),9,11) )</p>
<p><strong><em>Example 4</em></strong>, Deaths from horse-kicks</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-87"></span>
<img src="images/Bortkewitsch1898section12.jpg" alt="Section 12 of Bortkewitsch1898 book on the Poisson Distribution. The numbers " width="974" />
<p class="caption">
Figure 13.14: Section 12 of Bortkewitsch1898 book on the Poisson Distribution. The numbers
</p>
</div>
<p><strong><em>Example 5</em></strong> V1 Flying-bomb hits in London, 1944</p>
<p><strong><em>Example 6</em></strong> Daily Births</p>
<p><strong><em>Example 7</em></strong> Monthly Deaths</p>
<p><strong><em>Example 8</em></strong> Daily car crashes</p>
<p><strong>Does the Poisson Distribution apply to… ?</strong></p>
<ul>
<li>Yearly variations in numbers of  killed in plane crashes? </li>
<li>Daily variations in numbers of births?</li>
<li>Daily variations in numbers of deaths [extra variation over the seasons]</li>
<li>Daily variations in numbers of traffic accidents [variation over the seasons, and days of week, and with weather etc.]</li>
<li>Daily variations in numbers of deaths in France in summer 2002 &amp; 2003</li>
<li>Variations across cookies/pizzas in numbers of chocolate chips/olives.</li>
<li>Variations across days of year in numbers of deaths from sudden infant death syndrome. See </li>
</ul>
<p><a href="https://www.jstor.org/stable/1403193?seq=1">See</a></p>
<p>========</p>
<p><a href="https://en.wikipedia.org/wiki/Normal_distribution#CITEREFStigler1982" class="uri">https://en.wikipedia.org/wiki/Normal_distribution#CITEREFStigler1982</a></p>
<p>dark <a href="https://physics.stackexchange.com/questions/28563/hours-of-light-per-day-based-on-latitude-longitude-formula" class="uri">https://physics.stackexchange.com/questions/28563/hours-of-light-per-day-based-on-latitude-longitude-formula</a></p>
<p><a href="https://environhealthprevmed.biomedcentral.com/articles/10.1186/s12199-017-0637-4" class="uri">https://environhealthprevmed.biomedcentral.com/articles/10.1186/s12199-017-0637-4</a></p>
<p>car sppeds <a href="https://purr.purdue.edu/publications/2671/1" class="uri">https://purr.purdue.edu/publications/2671/1</a></p>
</div>
</div>
<div id="normal" class="section level3">
<h3><span class="header-section-number">13.2.4</span> Normal</h3>
</div>
<div id="hypergeometric" class="section level3">
<h3><span class="header-section-number">13.2.5</span> Hypergeometric</h3>
<p>Non-Central Hypergeometric</p>
</div>
<div id="chi-square" class="section level3">
<h3><span class="header-section-number">13.2.6</span> Chi-square</h3>
<p>===</p>
<p>EXERCISES</p>
<p>ADD YOUR OWN SEARCH RESULTS to table above</p>
<p>Hydroxychloroquine uniformity table.</p>
<p>few for differences</p>
<p>woolf</p>
<p>normal</p>
<p>transformations</p>
</div>
</div>
<div id="exercises-3" class="section level2">
<h2><span class="header-section-number">13.3</span> Exercises</h2>
<div id="clusters-of-miscarriages-based-on-article-by-l-abenhaim" class="section level3">
<h3><span class="header-section-number">13.3.1</span> Clusters of Miscarriages [based on article by L Abenhaim]</h3>
<p>Assume that:</p>
<ul>
<li>15% of all pregnancies end in a recognized spontaneous abortion (miscarriage) – this is probably a conservative estimate.</li>
<li>Across North America, there are 1,000 large companies. In each of them, 10 females who work all day with computer terminals become pregnant within the course of a year [the number who get pregnant would vary, but assume for the sake of this exercise that it is exactly 10 in each company]. *There is no relationship between working with computers and the risk of miscarriage.</li>
<li>a <code>cluster'' of miscarriages is defined as</code>at least 5 of 10 females in the same company suffering a miscarriage within a year’’</li>
</ul>
<p>Exercise: Calculate the number of ‘clusters’ of miscarriages one would expect in the 1,000 companies. <em>Hint</em>: begin with the probability of a cluster.</p>
</div>
<div id="prone-ness-to-miscarriages" class="section level3">
<h3><span class="header-section-number">13.3.2</span> ‘Prone-ness’ to Miscarriages ?</h3>
<p>Some studies suggest that the chance of a pregnancy ending in a spontaneous abortion is approximately 30%.</p>
<ol style="list-style-type: decimal">
<li>On this basis, if a woman becomes pregnant 4 times, what does the binomial distribution give as her chance of having 0,1,2,3 or 4 spontaneous abortions?</li>
<li>On this basis, if 70 women each become pregnant 4 times, what number of them would you expect to suffer 0,1,2,3 or 4 spontaneous abortions? (Think of the answers in (i) as proportions of women).</li>
<li>Compare these theoretically expected numbers out of 70 with the following observed data on 70 women, each of whom had 4 pregnancies:</li>
</ol>
<table>
<tbody>
<tr class="odd">
<td align="right">No. of spontaneous abortions:</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="right">No. of women with this many abortions:</td>
<td align="center">23</td>
<td align="center">28</td>
<td align="center">7</td>
<td align="center">6</td>
<td align="center">6</td>
</tr>
</tbody>
</table>
<ol start="4" style="list-style-type: decimal">
<li>Why don’t the expected numbers agree very well with the observed numbers? i.e. which assumption(s) of the Binomial Distribution are possibly being violated? (Note that the overall rate of spontaneous abortions in the observed data is in fact 84 out of 280 pregnancies or 30%)</li>
</ol>
</div>
<div id="automated-chemistries-from-ingelfinger-et-al" class="section level3">
<h3><span class="header-section-number">13.3.3</span> Automated Chemistries (from Ingelfinger et al)</h3>
<p>At the Beth Israel Hospital in Boston, an automated clinical chemistry analyzer is used to give 18 routinely ordered chemical determinations on one order (glucose, BUN, creatinine, …, iron). The ``normal’’ values for these 18 tests were established by the concentrations of these chemicals in the sera of a large sample of healthy volunteers. The normal range was defined so that an average of 3% of the values found in these healthy subjects fell outside.</p>
<ol style="list-style-type: decimal">
<li>Using the binomial formula [even if it is na&quot;{}ve to do so here], compute the probability that a healthy subject will have normal values on all 18 tests. Also calculate the probability of 2 or more abnormal values.</li>
<li>Which of the requirements for the binomial distribution are definitely satisfied, and which ones may not be?</li>
<li>Among 82 normal employees at the hospital, 52/82 (64%) had all normal tests, 19/82 (23%) had 1 abnormal test and 11/82 (13%) had 2 or more abnormal tests. Compare these observed percentages with the theoretical distribution obtained from calculations using the binomial distribution. Comment on the closeness of the fit.</li>
</ol>
</div>
<div id="binomial-or-opportunistic-capitalization-on-chance-multiple-looks-at-data" class="section level3">
<h3><span class="header-section-number">13.3.4</span> Binomial or Opportunistic? (Capitalization on chance… multiple looks at data)</h3>
<p>[Question from Ingelfinger et al. textbook] Mrs A has mild diabetes controlled by diet. Blood values vary rapidly, so think of each day as a new (independent) situation. Her morning urine sugar test is negative 80% of the time and positive (+) 20% of the time. [It is never graded higher than +].</p>
<ol style="list-style-type: decimal">
<li><p>At her regular visits to her physician, the physician always asks about last 5 days. At this particular visit, she tells the physician that the test has been + on each of the last 5 days.<br />
What is the probability that this would occur if her condition has remained unchanged? Does this observation give reason to think that her condition has changed?</p></li>
<li><p>Is the situation different if she observes, <em>between</em> visits, that the test is positive on 5 successive days and phones to express her concern?</p></li>
</ol>
</div>
<div id="can-one-influence-the-sex-of-a-baby" class="section level3">
<h3><span class="header-section-number">13.3.5</span> Can one influence the sex of a baby?</h3>
<p>This question was prompted by this <a href="http://www.biostat.mcgill.ca/hanley/statbook/SexRatioOvulationNEJM1979.pdf">article</a> in an 1979 issue of the New England Journal of Medicine.</p>
<ol style="list-style-type: decimal">
<li>Consider a binomial variable with <span class="math inline">\(n = 145\)</span> and <span class="math inline">\(\pi = 0.528\)</span>. Calculate the SD of, and therefore a measure of the variation in, the proportions that one would observe in different samples of 145 if <span class="math inline">\(\pi\)</span> = 0.528. [In other words, the SD of the sampling distribution of the sample proportion.]</li>
</ol>
<p>The following is abstracted from that NEJM article:</p>
<blockquote>
<p>The baby’s sex was studied in births to Jewish women who observed the orthodox ritual of sexual separation each month and who resumed intercourse within two days of ovulation. The proportion of male babies was 95/145 or 65.5% (!!) in the offspring of those women who resumed intercourse two days after ovulation (the overall percentage of male babies born to the 3658 women who had resumed intercourse within two days of ovulation [i.e. days -2, -1, 0, 1 and 2] was 52.8%).</p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li>How does the SD you calculated above help you judge the findings? And why did you <em>not</em> have to substitute the <span class="math inline">\(p=0.655\)</span> into the SD formula, and call it an SE of <span class="math inline">\(p\)</span>?</li>
</ol>
</div>
<div id="its-the-3rd-week-of-the-course-it-must-be-binomial" class="section level3">
<h3><span class="header-section-number">13.3.6</span> It’s the 3rd week of the course: it must be Binomial</h3>
<p>In which of the following would <span class="math inline">\(Y\)</span> <em><strong>not</strong></em> have a Binomial distribution? Why?</p>
<ol style="list-style-type: decimal">
<li>The pool of potential jurors for a murder case contains 100 persons chosen at random from the adult residents of a large city. Each person in the pool is asked whether he or she opposes the death penalty; <span class="math inline">\(Y\)</span> is the number who say ‘Yes.’</li>
<li><span class="math inline">\(Y\)</span> = number of women listed in different random samples of size 20 from the 1990 directory of statisticians.</li>
<li><span class="math inline">\(Y\)</span> = number of occasions, out of a randomly selected sample of 100 occasions during the year, in which you were indoors. (One might use this design to estimate what proportion of time you spend indoors)</li>
<li><span class="math inline">\(Y\)</span> = number of months of the year in which it snows in Montreal.</li>
<li><span class="math inline">\(Y\)</span> = Number, out of 60 occupants of 30 randomly chosen cars, wearing seatbelts.</li>
<li><span class="math inline">\(Y\)</span> = Number, out of 60 occupants of 60 randomly chosen cars, wearing seatbelts.</li>
<li><span class="math inline">\(Y\)</span> = Number, out of a department’s 10 microcomputers and 4 printers, that are going to fail in their first year.</li>
<li><span class="math inline">\(Y\)</span> = Number, out of simple random sample of 100 individuals, that are left-handed.</li>
<li><span class="math inline">\(Y\)</span> = Number, out of 5000 randomly selected from mothers giving birth each month in Quebec, who will test HIV positive.<br />
</li>
<li>You observe the sex of the next 50 children born at a local hospital; <span class="math inline">\(Y\)</span> is the number of girls among them.</li>
<li>A couple decides to continue to have children until their first girl is born; <span class="math inline">\(Y\)</span> is the total number of children the couple has.</li>
<li>You want to know what percent of married people believe that mothers of young children should not be employed outside the home. You plan to interview 50 people, and for the sake of convenience you decide to interview both the husband and the wife in 25 married couples. The random variable <span class="math inline">\(Y\)</span> is the number among the 50 persons interviewed who think mothers should not be employed.</li>
</ol>
</div>
<div id="tests-of-intuition" class="section level3">
<h3><span class="header-section-number">13.3.7</span> Tests of intuition</h3>
<ul>
<li><p>A coin will be tossed either 2 times or 20 times. You will win $2.00 if the number of heads is equal to the number of tails, no more and no less. Which is correct? (i) 2 tosses is better. (ii) 100 tosses is better. (iii) Both offer the same chance of winning.</p></li>
<li><p>Hospital A has 100 births a year, hospital B has 2500. In which hospital is it more that at least 55% of births in one year will be boys.</p></li>
</ul>
</div>
<div id="ci-for-proportion-when-observe-0n-or-nn" class="section level3">
<h3><span class="header-section-number">13.3.8</span> CI for proportion when observe 0/n or n/n</h3>
<p>Oncologists often first try out new experimental agents on patients with metastatic disease who have failed all standard therapies. One (old) rule for deciding to abandon an experimental agent was: if it was tried on 14 successive patients, but did not show anti-tumour in any of them, i.e. if it `struck out’ in all 14, yielding a response rate of 0/14.</p>
<p>Their reasoning is that this poor result ``rules out’’ (with 95% confidence) the possibility that it would be active in more than 20% of future patients. In other words, the data are incompatible with any <span class="math inline">\(\pi &gt; 20\%.\)</span></p>
<p>Check out their rule, by computing/obtaining a 95% 1-sided CI for <span class="math inline">\(\pi.\)</span></p>
<p>[Use a 1-sided CI if one is interested in putting just an  bound on the probability of benefit or risk: e.g., what is upper bound on $= $ probability of getting HIV from HIV-infected dentist? See JAMA article ``If Nothing Goes Wrong, Is Everything All Right? Interpreting Zero Numerators’’ by Hanley and Lippman-Hand</p>
<p><a href="http://www.epi.mcgill.ca/hanley/Reprints/If\_Nothing\_Goes\_1983.pdf" class="uri">http://www.epi.mcgill.ca/hanley/Reprints/If\_Nothing\_Goes\_1983.pdf</a></p>
<p>and the <code>rule of $3/n$' for a 1-sided 95\% CI. See also the second example (with sample size 4 !) in the section</code>in defense of (some) small studies’ in this article <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/Place\_of\_statistical\_1989.pdf" class="uri">http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/Place\_of\_statistical\_1989.pdf</a></p>
</div>
<div id="neg.-correlations-under-binomial-variation" class="section level3">
<h3><span class="header-section-number">13.3.9</span> neg. correlations … under-binomial variation</h3>
</div>
<div id="weights-of-offspring-pupstwins" class="section level3">
<h3><span class="header-section-number">13.3.10</span> weights of offspring (pups/twins)</h3>
</div>
<div id="suicides" class="section level3">
<h3><span class="header-section-number">13.3.11</span> suicides</h3>
</div>
<div id="horsekicks" class="section level3">
<h3><span class="header-section-number">13.3.12</span> horsekicks</h3>
</div>
<div id="visits-to-dentists-cochran" class="section level3">
<h3><span class="header-section-number">13.3.13</span> Visits to dentists (Cochran)</h3>
</div>
<div id="jhs-steps-per-day" class="section level3">
<h3><span class="header-section-number">13.3.14</span> JH’s steps per day</h3>
</div>
<div id="cd4-counts" class="section level3">
<h3><span class="header-section-number">13.3.15</span> CD4 counts</h3>
</div>
<div id="tweets" class="section level3">
<h3><span class="header-section-number">13.3.16</span> tweets</h3>
</div>
<div id="accidents" class="section level3">
<h3><span class="header-section-number">13.3.17</span> accidents</h3>
</div>
<div id="bootstrap" class="section level3">
<h3><span class="header-section-number">13.3.18</span> bootstrap</h3>
</div>
<div id="no.-of-seats-doors-cyclinders-in-cars" class="section level3">
<h3><span class="header-section-number">13.3.19</span> no. of seats / doors / cyclinders in cars</h3>
</div>
<div id="sex-ratio-in-accidents-fars" class="section level3">
<h3><span class="header-section-number">13.3.20</span> sex ratio in accidents (FARS)</h3>
<p>swedish twins reared together/apart Swedish Adoption/Twin Study on Aging (SATSA), 1984, 1987, 1990, 1993, 2004, 2007, and 2010 Nancy L. Pedersen Karolinska Institutet (Sweden)</p>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="randomVariables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="math.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/23-distributions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["statbook.pdf", "statbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
