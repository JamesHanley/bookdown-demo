# Computing Week2 {#computing02}

The **'computing' objectives** are to learn how to use `R` to 

* simulate random variation, and random variables

* visualize the consequences of aggregating independent random variables  
* discover the statistical laws that govern the variability of combinations of independent observations


The **'statistical' objectives** of this exercise are to

* be introduced to parameters that measure the spread of the distribution of a random variable, whether it be an error distribution, or a biological one.

* learn (empirically) the statistical laws that govern how the spread of a linear combination (e.g, the sum, or the mean) of several (generically, $n$) independent random variates is related to the spread of the individual random variables.

The ultimate objective is to be able to use these laws to help  investigators, such as Henry Cavendish, to (probabilistically) quantify 'how far off' their parameter estimates might be.



## Scientific background

>In Isaac Newton's Principia, Book III, The System of the World, Proposition 10, Theorem 10 we read: ‘If the earth were not denser than the seas, it would emerge from those seas and, according to the degree of its lightness, a part of the earth would stand out from the water, while all those seas flowed to the opposite side. By the same argument the spots on the sun are lighter than the solar shining matter on top of which they float. And in whatever way the planets were formed, at the time when the mass was fluid, all heavier matter made for the centre, away from the water. Accordingly, since the ordinary matter of our earth at its surface is about twice as heavy as water, and a little lower down, in mines, is found to be about three or four or even five times heavier than water, it is likely that the **total amount of matter in the earth is about five to six times greater than it would be if the whole earth consisted of water**, especially since it has already been shown above that the earth is about four times denser than Jupiter.’  
\ \ \ \ The fact that the average of Newton’s ‘five or six’ is very close to today’s value of the mean relative density of the Earth shows just how prescient he was. The mean density of the Earth was an extremely important quantity in early Renaissance science as it provided a strong clue as to planetary composition. 

Some of words above are taken  from this article by [astronomer David Hughes](http://www.biostat.mcgill.ca/hanley/statbook/DensityOfEarth.pdf).
Table 2 in the article shows 'Values suggested for the mean density of the Earth, as a function of the date of publication' starts with Newton's 1687 guesstimate, and ends, (23 estimates later) with Heyl and Chrzanowski's 1942 value.
Five of the 24 estimates are accompanied by a $\pm$ value, but what this values signifies is left unexplained. One of the 24s is Cavendish's 1798 estimate, which he obtained by taking the mean of 29 density measurements derived using a torsion balance on  17 days in the months of August and  September, and the following April and May. Cavendish's 'point estimate'  was 5.48. Since his'extreme results do not differ from the mean by more than 0.38,or $\frac{1}{14}$ of the whole' [...] Therefore, it seems very unlikely that the density of the earth should differ from the 5.48 by so much as $\frac{1}{14}$ of the whole.'

Here we have an effort, by no less than Isaac Newton, to give a numerical interval within which the true parameter value is likely to lie. Cavendish it seems very unlikely that the density of the earth should differ from the 5.48 by so much as $\frac{1}{14}$ of the whole.'  bounds on the amounts by which the 'likely'  


## Random Variation

### Measurement errors

The 'standard' has since been replaced by fancier methods, but for now let's imagine that every family in the world make their own independent physical copy (using say string or paper or cardboard) of the official [1 meter](https://en.wikipedia.org/wiki/History_of_the_metre) platimun bar that used to be stored in Paris. Suppose these copies had a measurement error of either +1 centimeter or -1 centimeter (with  plus errors and  minus errors being equally likely). Thus, these errors, or  'deviations' from the 100cm,  average out to 0. Each squared deviation is 1, so the mean squared deviation (called the error **variance**) is also 1. Its square root, also 1 in this instance, is called the **standard deviation** of the errors, or $\sigma_{e}$ for short.

**Now imagine taking a random sample of $n$ of these copies, and computing the sum and  the mean of the $n$ lengths.**

Although it is possible to mathematically enumerate/calculate the exact probalities that the sum or mean takes on the various possible values,  we will instead use `R` to approximate the probabilities by simulation. The probabilities in the following Figure are based on large enough numbers of  simulations that -- while they don't show the _perfect_ symmetry they would exhibit if we worked them out mathematically -- they give quite close approximations. 

The uppermost panel in the following figure shows the probability of obtaining various sums and various sample means.  


```{r,eval=T, echo=F, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}

plotMeterMeasurements = function(error){
  
    possible.n = c(1,2,5,10)
    n.sampled.possibilities = 100000
    nr = length(possible.n)
    par(mfrow=c(nr,1),mar = c(0.1,1,0.1,1) )

    for(r in 1:nr){
       n = possible.n[r]
       lengths = rep(0,n.sampled.possibilities)
       for (i in 1:n) { lengths = lengths + 
                        100 +
                sample(errors,n.sampled.possibilities,replace=TRUE)
        }
        f = table(lengths)
        max.e = 10

        XLIM=n*100 + c(-max.e,max.e)
        plot(f,yaxt="n", xaxt="n",
           xlim=c(XLIM[1]-4,XLIM[2]),
           ylim=c(-0.25,1/1.8)*n.sampled.possibilities)
        segments(XLIM[1], 0, XLIM[2], 0)
    
        segments(XLIM[1],0,XLIM[1],n.sampled.possibilities/1.95)
        for(pct in seq(25,50,25)){
    	     text(XLIM[1],n.sampled.possibilities*pct/100,
    	         paste(toString(pct),"%",sep=""),adj=c(1,0.5),cex=1.1)
        }
    
       dy = n.sampled.possibilities/8
       for(l in seq(XLIM[1],XLIM[2],2) ){
          text(l,0,toString(l),cex=1.75,adj=c(0.5,1.25))
          if(l == XLIM[1]) text(XLIM[1]-1,0,
                  paste("Sum of ",toString(n),":",sep=""),
                  cex=1.75,adj=c(1,1.25))
        }
      for(l in seq(XLIM[1],XLIM[2],n) ){
         text(l,-dy,toString(l/n),cex=1.75,adj=c(0.5,1.25),font=4)
         if(l == XLIM[1]) text(XLIM[1]-1,-dy,
                  paste("Mean of ",toString(n),":",sep=""),
                        cex=1.75,adj=c(1,1.25),font=4)
       }
    
  }

}

errors = c(-1,1)
plotMeterMeasurements(errors)
``` 

Instead of a 2-point error distribution, the next Figure shows how variable the sample totals and sample means would be if the errors were distributed as in the first row below. There, a quarter of the measurements have errors of +$\sqrt{2} \approx$  +1.4cm, half have no error, and a quarter have errors of -$\sqrt{2} \approx$  -1.4cm. Thus the error variance, the average of the squared deviations, is 
$$\sigma_e^2 = \frac{1}{4} \times (-1.4)^2  +  \frac{1}{2} \times (0)^2 +  \frac{1}{4} \times (1.4)^2 = 1,$$  so that the standard deviation is again $\sigma_e =  1.$

The row- (n-) specific distributions in the two Figures are  not exactly the same. For example, the possible means in samples of size $n$ = 2  have a 3-point distribution in the first one, but a 5-point distribution in the second one. But, as you will be asked to verify in the exercises below, the row-specific variances ate the same in the two figures.



```{r,eval=T, echo=F, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}

errors = sqrt(2)*c(-1,0,0,1)
plotMeterMeasurements(errors)
   
```

More important than this are the statistical laws governing how widely the sums (and the means) deviate from the 100cm. Clearly, the possible **sums** of 5 copies have a **wider** spread than the possible sums of 2, and the sums of 10  a wider spread than the sums of 5. Conversely, the **means** of 5 copies have a **narrower** spread than the means of 2, and the means of 10  a narrower spread than the means of 5.

**Instead of just telling you what the laws are, we ask you to use `R` to discover them yourself. 

### Discovering the Laws [via this computing exercise]

1. Put the two (equally likely) errors (or deviations), i.e., -1cm and +1cm, into an `R` vector of length 2. [By the way,  `c(a,b)` in `R` means **c**oncatenate `a` and `b` into a vector.] Then, make a new vector containing the squares of these deviations. [you can use `vector * vector` or `vector^2`]. Then use the built-in `R` function `mean` to compute the mean squared deviation. Although it is not needed in this case, use the `round` function to display the average squared deviation to a suitable number of decimals. Since what you have copputed is a `variance', use `this`variance` (or better still, `error.variance` as the name for the mean squared deviation. Finally, check by hand that the calculation is correct.

2. Use the `sqrt()` function (or the `^0.5` power) to compute the standard deviation of the errors.

3. Change the errors from -1cm and +1cm to say -5cm and +5cm, and repeat steps 1 and 2. From this, what did you learn about the variance and the standard deviation?

4. Add the errors onto 100cm to make 2 measurements (imperfect copies) of the meter bar, and calculate the mean, variance and standard deviation of the  2 measurements. From this, and by varying the sizes of the 2 errors, what did you learn about the mean the variance and the standard deviation of a shifted (re-located) random variable? {Hint: be careful to use your own variance and standard deviation functions, not the inbuilt `var` and `sd` functions. The reason is that whereas we say there are just 2 errors, in reality there are as many as there are copiers -- effectively an infinite number, about half of whom will make an error of +1cm, and about half of whome will an error of -1cm. Or you can say that the probabilities of  errors of -1cm and +1cm are 0.5, and 0.5.

5. In order to see the laws in action, and figure them out (with the aid of the `R` **code below**)

   + Begin with many simulated random **pairs** of measurements of (possibly imperfect copies) of the meter bar. For example, you might specify `no.of.pairs = 10000`. 

    + Then, using the 2-point (-1cm, +1cm) error distribution shown in the first Figure, simulate this large number of pairs of measurements. There are almost as many ways to do this as there are `R` programmers. One way (looking ahead to when we want to generalize and simulate larger samples) would be to use a matrix, i.e. a 2-way array, with as many rows as there are pairs (sets), and as many columns as the number of measurements per sample (here 2, but adjustable as you go along). See below.

    + For each of these simulated samples of size $n$ = 2, calculate the sample sum and sample mean. The `apply` function is very helpful here: you tell it to apply the desired function (FUN) separately for each **row** of the matrix by specifying `MARGIN = 1`. (Specifying `MARGIN = 2` would give you a separate result for each column.)
    
    + Now (finally) calculate the spread of the sample sums and sample means. Do so using both the standard deviation, and its square (the variance). The latter is not a natural quantity for non-statistians, but, as you will deduce, it is variances that scale with $n$.
    
    + Repeat these steps for samples of size $n$ = 1, 3, 4, 5, 6, 7 , 8, 9, and 10. and plot the variances and sd's against $n.$ What laws do these plots suggest?

According to Stephen Stigler, a historian of statistics, understanding of this law is one of the things that separates statisticians from mathematicains and computer scientists. Indeed, it is the second of what he calls the Seven Pillars od Statistics:

>The first recognition that the variation of sums did not increase proportionately with the number of independent terms added (and that the standard deviation of means did not decrease inversely with the number of terms) dates to the 1700s. This novel insight, that information on accuracy did not accumulate linearly with added data, came in the 1720s to Abraham de Moivre as he began to seek a way to accurately compute binomial probabilities with a large number of trials. In 1733 this would lead to his famous result, which we now call the normal approximation to the binomial, but by 1730 he already noticed that a crucial aspect of the distribution was tied to ... [Stigler The Seven Pillars of Statistical Wisdom. Chapter 2. Its Measurement and Rate of Change.]

His story of [The Trial of the Pyx](http://www.biostat.mcgill.ca/hanley/c323/pyx.pdf) dramatically illustrates the early and continued blindness to the correct form of the laws. But he doesn't think that Newtom, who was Master of the Mint for many years, took advantage of this blindness to become rich.  

**possible R code:** 

```{r,eval=T, echo=TRUE, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}

ERRORS = c(-1,1)

no.of.pairs = 10000 
means.samples.of.size.2 = rep(NA,no.of.pairs)

measurements = matrix(100+sample(ERRORS,size = 2*no.of.pairs, replace = TRUE),
                      nrow = no.of.pairs, ncol=2)
str(measurements)
head(measurements,4)
tail(measurements,4)

sums.samples.of.size.2 = apply(measurements,MARGIN=1,FUN=sum)
str(sums.samples.of.size.2)
means.samples.of.size.2 = apply(measurements,MARGIN=1,FUN=mean)

tail(sums.samples.of.size.2,4)
tail(means.samples.of.size.2,4)

table(sums.samples.of.size.2)
round( mean(sums.samples.of.size.2), 2)
round( var(sums.samples.of.size.2), 2)
round( sd(sums.samples.of.size.2) , 2)


table(means.samples.of.size.2)
round( mean(means.samples.of.size.2), 2)
round( var(means.samples.of.size.2), 2)
round( sd(means.samples.of.size.2) , 2)

  
```



References

David Hughes. The mean density of the Earth.
Journal of the British Astronomical Association, Vol. 116, No. 1, p.21. 2006

## Biological variation

### Example

We move now to contexts where the variation is primarily (or in a few deluxe cases where there are no measurement issues, entirely) due to genuine -- e.g., biological or geographical  -- variation. 
Below, you will address one such example, the weights of a specific  population where we have good national-level data to help us to set up simulations that demonstate and let us discover the statistical laws governing the (sampling) variability of various statistics derived from samples. For for now, we will consider a demographic characteristic, namely **age**, in a setting where it was documented for an entire population.  The ages were gathered on the night of Sunday April 2nd in the [1911 Census of Ireland,](http://www.census.nationalarchives.ie/help/about19011911census.html) the last one to be carried out under British rule. 
Here are the returns of one famous statistician, whose important work, published under the pen-name ['Student,'](http://www.census.nationalarchives.ie/reels/nai000230598/) we will meet shortly.

We will limit our attention to the county of Dublin, which at the time has a population of about half a million people, mostly urban. The male age-distribution [displayed here in the usual vertical orientation](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/MeanQuantiles2019.pdf#page=18) has the (1/2) _pyramid_ shape that characterizes developing countries. The somewhat different female age-distribution is in part because Ireland's capital city, Dublin City, attracted many female workers in their late teens and their 20s (cf. the last 3 entries in Gosset's return).
The [age-distributions today](https://www.cso.ie/en/releasesandpublications/ep/p-cp3oy/cp3/aad/)
show large city vs. rural differences.  

**Shapes of distributions**

Below, the age-distribution is rotated by 90 degrees, so that age is on the horizontal axis, and numbers of persons in the different age-bins (we will combine male and female) are on the vertical axis. In this more familiar orientation, it is easier to see that the **age-distribution does not have the symmetry observed  in the earlier section**, where the variation was entirely due to measurement variations around some constant.

(Incidentally, in the 'age-distributions of today's Irish population, there are none of the 'spikes' at ages 40, 50, 60, .. that were seem in the 1911 data. The origin of these spikes is left for you to puzzle about. There are many modern examples of this phenomenon, as in this example of the [recording of emergency department arrival and departure times](http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/ch-chapterPlusNotes-ch3-2019.pdf#page=21) . Another difficulty faced by earlier-century census takers is described in this account 
[The Puzzled Census-Taker](https://books.google.ca/books?id=fUsPAAAAYAAJ&pg=PA118&dq=John+Godfrey+Saxe+census-taker&hl=en&sa=X&ved=0ahUKEwjQv6f04OvVAhUL5IMKHUZfBfgQ6AEIJjAA#v=onepage&q=John%20Godfrey%20Saxe%20census-taker&f=false) )


This -- first of many -- distribution of a human characteristic will serve as a strong messsage that **we should not expect such distributions to be symmetric, let alone bell-shaped ('Gaussian' / 'Normal').** The default stance should  be that they are not. The same  stance should apply to distributions of characteristics of 'man-made' or 'human-made'  institutions --  such as the distribution of the sizes of the hopitals or schools in a province or country. Nor should we expect symmetric distributions in the physical world, such as the distribution of the depths of the ocean,  heights of land,   lengths of rivers, daily temperatures in a region over  a year,  magnitudes of earthquakes, or intervals between eruptions of the Old Faithful geyser.

Foe example, the distributions of the amounts of income tax paid by individuals in Quebec, the salaries of professional ice-hockey players, and the lengths of stay {LOS] of hospital patients do not have symmetric shapes (some distributions may even be multi-modal). Despite this, in _some_ instances it may be critical to know the mean of the distibution, so that one can calculate the total revenue, or total payroll, or how much the savings would be if the mean LOS were reduced by 2%. 
We have to think about a  summary  appropriate  to the situation. If you are representing the players, would you cite the median or the mean when telling the team-owners how poorly paid the 'average' player is? What if represented the owners, and wanted to show how much the team costs in payroll?

**Measures of 'centre'**

(Online, there are now many jokes about **silly statistics** and silly statisticians, such as on [this site](http://www.workjoke.com/statisticians-jokes.html).
A radio prgram used to end with
>Well, that's the news from Lake Wobegon, where all the women are strong, all the men are good-looking, and all the children are above average.
There are stories  of a statistician [drowning in a river that was 3 feet deep on 
average](https://ask.metafilter.com/261337/Source-of-well-known-joke-about-statisticians)
or being [comfortable on average](https://www.barrypopik.com/index.php/new_york_city/entry/head_in_the_oven_feet_in_the_freezer). One year in the 607 summer course for doctors, JH used some of these stories to warn against silly averages, and even quoted Francis Galton.

>Why do statisticians commonly limit their inquiries to Averages?
It is difficult to understand why statisticians commonly limit their inquiries to Averages, and do not revel in more comprehensive views.
Their souls seem as dull to the charm of variety as that of the native of one of our flat English counties, whose retrospect of Switzerland was that, if its mountains could be thrown into its lakes, two nuisances would be got rid of at once. [Natural Inheritance, 1889.]

A urologist in the class gave a better example.

>The average person has 1 ovary and 1 testicle.

Despite these jibes, there are some situations where it does make sense to talk about the mean value, even if nobody has a value close to the mean.

When it comes to the age distribution of a population, the reports of national statistical agencies use a number of summary parameters, sometimes the median, sometimes the mean, and sometimes a 'dependency ratio'  such as non-working-age to working-age numbers. For now, we will pursue the 'mean' parameter $\mu$. Later, we also pursue other numerical parameters for the centre. Part of the reason for our first choice is that the behavioural properties of the sample _mean_ (an estimator of the {population mean_) are much easier to describe with standard statistical laws. Fortunately, nowadays, we have intensive computer techniques that use data-driven  techniques rather than formulae-based ones, that can handle estimators of other parameters. 

**Mean-Pursuit**

In the following `R` code, we simulate trying to estimate the mean age ($\mu$), using just a random sample of $n$ persons. Clearly, the $n$'s we will use are too small to give estimates that are 'close enough for government work'
but the purpose is to understand **what size $n$ would ensure sufficiently precise estimates.** 




```{r,eval=T, echo=FALSE, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}

ds=read.table(
"http://www.biostat.mcgill.ca/hanley/bios601/AgeFrequencies.txt",
as.is=TRUE,header=TRUE)

AGES = ds$Age
Proportions = ds$Freq/sum(ds$Freq)

A.50 = AGES[min(which(cumsum(Proportions) > 0.50))]
mu = sum(AGES*Proportions)

```

In the following code, AGES refers to the ages (0 to 109) and Proportions the proportions of the population in each 1-year age bin.



```{r,eval=T, echo=TRUE, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}


no.of.sims = 10000 ;  # no. of samples of each size
                      # enough to generate relatively smmoth histograms  
sample.sizes = c(1,2,4,10,25) ; 

par(mfrow=c(length(sample.sizes),1),mar = c(0.5,1,0.5,1) )

for (n in sample.sizes ){        # loop over the various sample sizes
   ages = matrix(sample(AGES,          # 1 row per simulation
              size = n*no.of.sims,     # to save time, do all at once
              replace = TRUE,          # only because data compressed
              prob = Proportions),     # = FALSE if has indiv. data
          nrow = no.of.sims, ncol=n) # put into rows / columns
   if(n > 1 & n <= 10){
     print( noquote(
      paste("Ages of sampled persons in first 2 samples of size",  
            toString(n)) )   ) 
     print(head(ages,2))
   }
   
   if( n == max(sample.sizes) ){
     cat("The first panel shows the age-distribution of the entire population.\n")
     cat("The remaining ones show the  distributions of the sample sums and means.\n")
     message("test")
     
   } 
  
   # compute the row-specific (simulation-specific) sums and means
   # apply sum/mean to MARGIN=1, i.e., to each simulation (each row)

   sums.samples.of.size.n = apply(ages,MARGIN=1,FUN=sum)
   means.samples.of.size.n = apply(ages,MARGIN=1,FUN=mean)

   fr = table(sums.samples.of.size.n)          # fr = frequency
   Y = max(Proportions*no.of.sims)/sqrt(0.8*n) # scale the y axis
   plot(fr,lw=0.4,xlim=c(0,n*(max(AGES)+12) ), 
                  ylim=c(-0.25,1)*Y, xaxt="n")
   text(n*105,0.55*Y,paste("n =",toString(n)),cex=2,font=3,adj=c(0,1))
   for(a in seq(0,100,5)) {
     text(n*a, -0.01*Y, toString(n*a),adj=c(0.5,1),cex=1.5)
     txts = paste("Sum of",toString(n),"Ages")
     if(n==1) txts = "Individual Ages"
     if(a==100 ) text(n*105, -0.01*Y,
          txts,adj=c(0,1),cex=1.5)
     if(n > 1) text(n*a, -0.15*Y, toString(a),adj=c(0.5,1),font=4,cex=1.5)
     if(a==100 & n > 1) text(n*105, -0.15*Y,
          paste("Mean of",toString(n)," Ages"),adj=c(0,1),font=4,cex=1.5)
   }
   
   # how big is the spread (sd) of the simulated sums and means ?
   
   sd.sums  = round( sd(sums.samples.of.size.n), 1 )
   sd.means = round( sd(means.samples.of.size.n),1 )
   
   txt.s = paste( "SD of Sum:", toString(sd.sums) )
   if(n==1) txt.s = paste("SD of Individual Ages:",toString(sd.sums) )
   txt.m = paste("\n\n  SD of Mean:",toString(sd.means))
   if(n==1) txt.m = "\n\n "
   text(n*mu +  sd.sums,Y*0.7,
        paste(txt.s,txt.m), cex=1.5,adj=c(0,0.5) )
   points(n*mu,0,pch=19,col="red",cex=1.5)
   if(n==1){
     text(A.50-0.1,0.95*Y,"50% <- | -> 50%",adj=c(0.5,1),cex=1.5,col="blue")
     segments(A.50-0.1,0.95*Y, A.50-0.1,0,col="blue")
     text(115,Y,
       "Reported Ages of Population of Dublin\nIrish Census of 1911",
       adj=c(1,1),cex=1.25)
   } 
}

```

**Exercises**

* Based on the numbers in the 5 panels, derive the statistical law that connects the spreads of the sampling distributions of the sample sum to the spread of the individual ages. (Since the calculated sd's are based on a finite set of simulations, the numbers may not fit  the law _exactly_ ; also, the sd's shown are rounded)

* Likewise, state the statistical law that connects the spreads of the sampling distributions of the sample mean to the spread of the individual ages. Use this law to predict the spread of the sampling distribution if we were to use a smaple size of $n$ =100.

* Are these laws the same as the ones that apply when the only source of variation is measurement error?

* In the 'measuring the meter bar' example, the error distribution was symmetric; in this example, the ages do not have a symmetric distribution: it has a long right tail. Describe how the shape of the (sampling) distribution changes with the sample size involved. Look online for the name of this law or theorem.


### Example 2

The frequency distribution of the self-reported weights of a given population of adults is available here.


```{r,eval=T, echo=T, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}

ds = read.table("http://www.biostat.mcgill.ca/hanley/statbook/Weights.txt")

MEAN=round(weighted.mean(ds$Weight.lbs,w=ds$Freq))

VAR = sum( ds$Freq * (ds$Weight.lbs-MEAN)^2 ) / sum(ds$Freq)

SD = sqrt(VAR)

```


Elevators

On the average, conventioneers weigh about 150 pounds; the SD is 25 pounds.
i. If a large elevator for a convention centre is designed to lift a maximum of 15,500 pounds, the chance it will be overloaded by a random group of 100 conventioneers is closest to which of the following: 0.1 of 1%, 2%, 5%, 50%, 95%, 98%, 99.9% ? Explain your reasoning.
ii. The weights of conventioneers are unlikely to have a Gaussian (“Normal”) distribution. In the light of this information, are you still comfortable using the Normal distribution for your calculations in part i? Explain carefully. Explain why the ‘random’ is key to being able to answer part i. and what impact it would have if it is not the case.



```{r,eval=T, echo=T, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}

plot(ds$Weight.lbs,ds$Freq,type="h",xlab="Weight in lbs",
     ylab="Frequency",cex.lab=1.5, cex.axis=1.5)

#ds$smoothed.Freqs = filter(ds$Freq,filter=rep(1/20,20))

#lines(ds$Weight.lbs,ds$smoothed.Freqs)
      
ds$fitted.Gaussian.frequencies = 
   sum(ds$Freq)*dnorm(ds$Weight.lbs, mean=MEAN, sd = SD)

lines(ds$Weight.lbs,ds$fitted.Gaussian.frequencies)

   
```

Pyx
