# Computing Week2 {#computing02}

The **'computing' objectives** are to learn how to use `R` to 

* simulate random variation, and random variables

* visualize the consequences of aggregating independent random variables  
* discover the statistical laws that govern the variability of combinations of independent observations


The **'statistical' objectives** of this exercise are to

* be introduced to parameters that measure the spread of the distribution of a random variable, whether it be an error distribution, or a biological one.

* learn (empirically) the statistical laws that govern how the spread of a linear combination (e.g, the sum, or the mean) of several (generically, $n$) independent random variates is related to the spread of the individual random variables.

The ultimate objective is to be able to use these laws to help  investigators, such as Henry Cavendish, to (probabilistically) quantify 'how far off' their parameter estimates might be.



## Scientific background

>In Isaac Newton's Principia, Book III, The System of the World, Proposition 10, Theorem 10 we read: ‘If the earth were not denser than the seas, it would emerge from those seas and, according to the degree of its lightness, a part of the earth would stand out from the water, while all those seas flowed to the opposite side. By the same argument the spots on the sun are lighter than the solar shining matter on top of which they float. And in whatever way the planets were formed, at the time when the mass was fluid, all heavier matter made for the centre, away from the water. Accordingly, since the ordinary matter of our earth at its surface is about twice as heavy as water, and a little lower down, in mines, is found to be about three or four or even five times heavier than water, it is likely that the **total amount of matter in the earth is about five to six times greater than it would be if the whole earth consisted of water**, especially since it has already been shown above that the earth is about four times denser than Jupiter.’  
\ \ \ \ The fact that the average of Newton’s ‘five or six’ is very close to today’s value of the mean relative density of the Earth shows just how prescient he was. The mean density of the Earth was an extremely important quantity in early Renaissance science as it provided a strong clue as to planetary composition. 

Some of words above are taken  from this article by [astronomer David Hughes](http://www.biostat.mcgill.ca/hanley/statbook/DensityOfEarth.pdf).
Table 2 in the article shows 'Values suggested for the mean density of the Earth, as a function of the date of publication' starts with Newton's 1687 guesstimate, and ends, (23 estimates later) with Heyl and Chrzanowski's 1942 value.
Five of the 24 estimates are accompanied by a $\pm$ value, but what this values signifies is left unexplained. One of the 24s is Cavendish's 1798 estimate, which he obtained by taking the mean of 29 density measurements derived using a torsion balance on  17 days in the months of August and  September, and the following April and May. Cavendish's 'point estimate'  was 5.48. Since his'extreme results do not differ from the mean by more than 0.38,or $\frac{1}{14}$ of the whole' [...] Therefore, it seems very unlikely that the density of the earth should differ from the 5.48 by so much as $\frac{1}{14}$ of the whole.'

Here we have an effort, by no less than Isaac Newton, to give a numerical interval within which the true parameter value is likely to lie. Cavendish it seems very unlikely that the density of the earth should differ from the 5.48 by so much as $\frac{1}{14}$ of the whole.'  bounds on the amounts by which the 'likely'  


## Random Variation

### Measurement errors

The 'standard' since been replaced by fancier methods, but for now let's imagine that every family in the world make their own independent physical copy (using say string or paper or cardboard) of the official [1 meter](https://en.wikipedia.org/wiki/History_of_the_metre) platimun bar that used to be stored in Paris. Suppose these copies had a measurement error of either +1 centimeter or -1 centimeter (with  plus errors and  minus errors being equally likely). Thus, these errors, or  'deviations' from the 100cm,  average out to 0. Each squared deviation is 1, so the mean squared deviation (called the error **variance**) is also 1. Its square root, also 1 in this instance, is called the **standard deviation** of the errors, or $\sigma_{e}$ for short.

Now imagine taking a random sample of $n$ of these copies, and computing the sum and  the mean of the $n$ lengths.

Although it is possible to mathematically enumerate/calculate the exact probalities that the sum or mean takes on the various possible values,  we will instead use `R` to approximate the probabilities by simulation. The probabilities in the following Figure are based on large enough numbers of  simulations that -- while they don't show the _perfect_ symmetry they would exhibit if we worked them out mathematically -- they give quite close approximations. 

The uppermost panel in the following figure shows the probability of obtaining various sums and various sample means.  


```{r,eval=T, echo=F, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}

plotMeterMeasurements = function(error){
  
    possible.n = c(1,2,5,10)
    n.sampled.possibilities = 100000
    nr = length(possible.n)
    par(mfrow=c(nr,1),mar = c(0.1,1,0.1,1) )

    for(r in 1:nr){
       n = possible.n[r]
       lengths = rep(0,n.sampled.possibilities)
       for (i in 1:n) { lengths = lengths + 
                        100 +
                sample(errors,n.sampled.possibilities,replace=TRUE)
        }
        f = table(lengths)
        max.e = 10

        XLIM=n*100 + c(-max.e,max.e)
        plot(f,yaxt="n", xaxt="n",
           xlim=c(XLIM[1]-4,XLIM[2]),
           ylim=c(-0.25,1/1.8)*n.sampled.possibilities)
        segments(XLIM[1], 0, XLIM[2], 0)
    
        segments(XLIM[1],0,XLIM[1],n.sampled.possibilities/1.95)
        for(pct in seq(25,50,25)){
    	     text(XLIM[1],n.sampled.possibilities*pct/100,
    	         paste(toString(pct),"%",sep=""),adj=c(1,0.5),cex=1.1)
        }
    
       dy = n.sampled.possibilities/8
       for(l in seq(XLIM[1],XLIM[2],2) ){
          text(l,0,toString(l),cex=1.75,adj=c(0.5,1.25))
          if(l == XLIM[1]) text(XLIM[1]-1,0,
                  paste("Sum of ",toString(n),":",sep=""),
                  cex=1.75,adj=c(1,1.25))
        }
      for(l in seq(XLIM[1],XLIM[2],n) ){
         text(l,-dy,toString(l/n),cex=1.75,adj=c(0.5,1.25),font=4)
         if(l == XLIM[1]) text(XLIM[1]-1,-dy,
                  paste("Mean of ",toString(n),":",sep=""),
                        cex=1.75,adj=c(1,1.25),font=4)
       }
    
  }

}

errors = c(-1,1)
plotMeterMeasurements(errors)
``` 

Instead of a 2-point error distribution, the next Figure shows how variable the sample totals and sample means would be if the errors were distributed as in the first row below. There, a quarter of the measurements have errors of +$\sqrt{2} \approx$  +1.4cm, half have no error, and a quarter have errors of -$\sqrt{2} \approx$  -1.4cm. Thus the error variance, the average of the squared deviations, is 
$$\sigma_e^2 = \frac{1}{4} \times (-1.4)^2  +  \frac{1}{2} \times (0)^2 +  \frac{1}{4} \times (1.4)^2 = 1,$$  so that the standard deviation is again $\sigma_e =  1.$

The row- (n-) specific distributions in the two Figures are  not exactly the same. For example, the possible means in samples of size $n$ = 2  have a 3-point distribution in the first one, but a 5-point distribution in the second one. But, as you will be asked to verify in the exercises below, the row-specific variances ate the same in the two figures.



```{r,eval=T, echo=F, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}

errors = sqrt(2)*c(-1,0,0,1)
plotMeterMeasurements(errors)
   
```

More important than this are the statistical laws governing how widely the sums (and the means) deviate from the 100cm. Clearly, the possible **sums** of 5 copies have a **wider** spread than the possible sums of 2, and the sums of 10  a wider spread than the sums of 5. Conversely, the **means** of 5 copies have a **narrower** spread than the means of 2, and the means of 10  a narrower spread than the means of 5.

**Instead of just telling you what the laws are, we ask you to use `R` to discover them yourself. 

### Discovering the Laws [via this computing exercise]

1. Put the two (equally likely) errors (or deviations), i.e., -1cm and +1cm, into an `R` vector of length 2. [By the way,  `c(a,b)` in `R` means **c**oncatenate `a` and `b` into a vector.] Then, make a new vector containing the squares of these deviations. [you can use `vector * vector` or `vector^2`]. Then use the built-in `R` function `mean` to compute the mean squared deviation. Although it is not needed in this case, use the `round` function to display the average squared deviation to a suitable number of decimals. Since what you have copputed is a `variance', use `this`variance` (or better still, `error.variance` as the name for the mean squared deviation. Finally, check by hand that the calculation is correct.

2. Use the `sqrt()` function (or the `^0.5` power) to compute the standard deviation of the errors.

3. Change the errors from -1cm and +1cm to say -5cm and +5cm, and repeat steps 1 and 2. From this, what did you learn about the variance and the standard deviation?

4. Add the errors onto 100cm to make 2 measurements (imperfect copies) of the meter bar, and calculate the mean, variance and standard deviation of the  2 measurements. From this, and by varying the sizes of the 2 errors, what did you learn about the mean the variance and the standard deviation of a shifted (re-located) random variable? {Hint: be careful to use your own variance and standard deviation functions, not the inbuilt `var` and `sd` functions. The reason is that whereas we say there are just 2 errors, in reality there are as many as there are copiers -- effectively an infinite number, about half of whom will make an error of +1cm, and about half of whome will an error of -1cm. Or you can say that the probabilities of  errors of -1cm and +1cm are 0.5, and 0.5.

5. In order to see the laws in action, and figure them out (with the aid of the `R` code below)

   + Begin with many simulated random **pairs** of measurements of (possibly imperfect copies) of the meter bar. For example, you might specify `no.of.pairs = 10000`. 

    + Then, using the 2-point (-1cm, +1cm) error distribution shown in the first Figure, simulate this large number of pairs of measurements. There are almost as many ways to do this as there are `R` programmers. One way (looking ahead to when we want to generalize and simulate larger samples) would be to use a matrix, i.e. a 2-way array, with as many rows as there are pairs (sets), and as many columns as the number of measurements per sample (here 2, but adjustable as you go along). See below.

    + For each of these simulated samples of size $n$ = 2, calculate the sample sum and sample mean. The `apply` function is very helpful here: you tell it to apply the desired function (FUN) separately for each **row** of the matrix by specifying `MARGIN = 1`. (Specifying `MARGIN = 2` would give you a separate result for each column.)
    
    + Now (finally) calculate the spread of the sample sums and sample means. Do so suing both the standard deviation, and its square (the variance). The latter is not a natural quantity for non-statistians,
    
6. Repeat 5. for samples of size $n$ = 1, 3, 4, 5, 6, 7 , 8, 9, and 10. and plot the variances and sd's against $n.$ What laws do these plots suggest?

According to Stephen Stigler, a historian of statistics, understanding of this law is one of the things that separates statisticians from mathematicains and computer scientists. Indeed, it is the second of what he calls the Seven Pillars od Statistics:

>The first recognition that the variation of sums did not increase proportionately with the number of independent terms added (and that the standard deviation of means did not decrease inversely with the number of terms) dates to the 1700s. This novel insight, that information on accuracy did not accumulate linearly with added data, came in the 1720s to Abraham de Moivre as he began to seek a way to accurately compute binomial probabilities with a large number of trials. In 1733 this would lead to his famous result, which we now call the normal approximation to the binomial, but by 1730 he already noticed that a crucial aspect of the distribution was tied to ... [Stigler The Seven Pillars of Statistical Wisdom. Chapter 2. Its Measurement and Rate of Change.]

His story of The Trial of the Pyx dramatically illustrates the early and continued blindness to the correct form of the laws. But he doesn't think that Newtom, who was Master of the Mint for many years, took advantage of this blindness to become rich.  

    

```{r,eval=T, echo=TRUE, fig.align="center", fig.height=9, fig.width=9, warning=FALSE, message=F}

ERRORS = c(-1,1)

no.of.pairs = 10000 
means.samples.of.size.2 = rep(NA,no.of.pairs)

measurements = matrix(100+sample(ERRORS,size = 2*no.of.pairs, replace = TRUE),
                      nrow = no.of.pairs, ncol=2)
str(measurements)
head(measurements,4)
tail(measurements,4)

sums.samples.of.size.2 = apply(measurements,MARGIN=1,FUN=sum)
str(sums.samples.of.size.2)
means.samples.of.size.2 = apply(measurements,MARGIN=1,FUN=mean)

tail(sums.samples.of.size.2,4)
tail(means.samples.of.size.2,4)

table(sums.samples.of.size.2)
round( mean(sums.samples.of.size.2), 2)
round( var(sums.samples.of.size.2), 2)
round( sd(sums.samples.of.size.2) , 2)


table(means.samples.of.size.2)
round( mean(means.samples.of.size.2), 2)
round( var(means.samples.of.size.2), 2)
round( sd(means.samples.of.size.2) , 2)

  
```


Elevators

## Biological variation

References

David Hughes. The mean density of the Earth.
Journal of the British Astronomical Association, Vol. 116, No. 1, p.21. 2006
