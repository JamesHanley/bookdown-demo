<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Random Variables/Variation | Introduction to Statistical Analysis: a regression-from-the-outset approach</title>
  <meta name="description" content="A regression-from-the-outset based approach" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Random Variables/Variation | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A regression-from-the-outset based approach" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Random Variables/Variation | Introduction to Statistical Analysis: a regression-from-the-outset approach" />
  
  <meta name="twitter:description" content="A regression-from-the-outset based approach" />
  

<meta name="author" content="Sahir, Shirin and Jim" />


<meta name="date" content="2020-06-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability.html"/>
<link rel="next" href="distributions.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">In Planning Stage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#target"><i class="fa fa-check"></i><b>0.1</b> Target</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#topicstextbooks"><i class="fa fa-check"></i><b>0.2</b> Topics/textbooks</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#regression-from-the-outset"><i class="fa fa-check"></i><b>0.3</b> Regression from the outset</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#parameters-first-data-later"><i class="fa fa-check"></i><b>0.4</b> Parameters first, data later</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#lets-switch-to-y-bar-and-drop-x-bar."><i class="fa fa-check"></i><b>0.5</b> Let’s switch to “y-bar”, and drop “x-bar”.</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#computing-from-the-outset"><i class="fa fa-check"></i><b>0.6</b> Computing from the outset</a></li>
<li class="chapter" data-level="0.7" data-path="index.html"><a href="index.html#appendix"><i class="fa fa-check"></i><b>0.7</b> Appendix:</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#goals"><i class="fa fa-check"></i><b>1.1</b> Goals</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#structure"><i class="fa fa-check"></i><b>1.2</b> Structure</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#attitudes-etc."><i class="fa fa-check"></i><b>1.3</b> Attitudes, etc….</a></li>
</ul></li>
<li class="part"><span><b>I Part I</b></span></li>
<li class="chapter" data-level="2" data-path="paras.html"><a href="paras.html"><i class="fa fa-check"></i><b>2</b> Statistical Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="paras.html"><a href="paras.html#parameters"><i class="fa fa-check"></i><b>2.1</b> Parameters</a></li>
<li class="chapter" data-level="2.2" data-path="paras.html"><a href="paras.html#parameter-contrasts"><i class="fa fa-check"></i><b>2.2</b> Parameter Contrasts</a><ul>
<li class="chapter" data-level="2.2.1" data-path="paras.html"><a href="paras.html#parameter-relations-in-numbers-and-words"><i class="fa fa-check"></i><b>2.2.1</b> Parameter relations in numbers and words</a></li>
<li class="chapter" data-level="2.2.2" data-path="paras.html"><a href="paras.html#parameter-relations-in-symbols-and-with-the-help-of-an-index-category-indicator"><i class="fa fa-check"></i><b>2.2.2</b> Parameter relations in symbols, and with the help of an index-category indicator</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="paras.html"><a href="paras.html#parameter-functions"><i class="fa fa-check"></i><b>2.3</b> Parameter functions</a></li>
<li class="chapter" data-level="2.4" data-path="paras.html"><a href="paras.html#phraseology-to-avoid"><i class="fa fa-check"></i><b>2.4</b> Phraseology to avoid</a></li>
<li class="chapter" data-level="2.5" data-path="paras.html"><a href="paras.html#summary"><i class="fa fa-check"></i><b>2.5</b> SUMMARY</a></li>
<li class="chapter" data-level="2.6" data-path="paras.html"><a href="paras.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="paras.html"><a href="paras.html#references"><i class="fa fa-check"></i><b>2.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html#the-bayesian-approach"><i class="fa fa-check"></i><b>3.1</b> The Bayesian Approach</a><ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#example-parameter-is-2-valued-yes-or-no"><i class="fa fa-check"></i><b>3.1.1</b> Example: parameter is 2-valued: yes or no</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference.html"><a href="inference.html#example-parameter-is-a-proportion"><i class="fa fa-check"></i><b>3.1.2</b> Example: parameter is a proportion</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference.html"><a href="inference.html#examples-parameter-is-a-personal-number-or-population-mean"><i class="fa fa-check"></i><b>3.1.3</b> Examples: parameter is a personal number or population mean</a></li>
<li class="chapter" data-level="3.1.4" data-path="inference.html"><a href="inference.html#the-bayesian-bottom-line"><i class="fa fa-check"></i><b>3.1.4</b> The Bayesian Bottom Line</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inference.html"><a href="inference.html#frequentist-approach"><i class="fa fa-check"></i><b>3.2</b> Frequentist approach</a><ul>
<li class="chapter" data-level="3.2.1" data-path="inference.html"><a href="inference.html#frequentist-test-of-a-null-hypothesis"><i class="fa fa-check"></i><b>3.2.1</b> (Frequentist) Test of a Null Hypothesis</a></li>
<li class="chapter" data-level="3.2.2" data-path="inference.html"><a href="inference.html#ingredients-and-methods-of-procedure-in-a-statistical-test"><i class="fa fa-check"></i><b>3.2.2</b> Ingredients and methods of procedure in a statistical test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="inference.html"><a href="inference.html#does-the-approach-matter"><i class="fa fa-check"></i><b>3.3</b> Does the approach matter?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Parameter Intervals</a><ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> ‘100% confidence’ intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#more-nuanced-intervals"><i class="fa fa-check"></i><b>4.2</b> More-nuanced intervals</a></li>
<li class="chapter" data-level="4.3" data-path="CI.html"><a href="CI.html#summary-1"><i class="fa fa-check"></i><b>4.3</b> SUMMARY</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="paraMu.html"><a href="paraMu.html"><i class="fa fa-check"></i><b>5</b> The ‘mean’ parameter <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="paraMu.html"><a href="paraMu.html#two-genres"><i class="fa fa-check"></i><b>5.1</b> Two genres</a></li>
<li class="chapter" data-level="5.2" data-path="paraMu.html"><a href="paraMu.html#fitting-these-to-data-estimating-them-from-data"><i class="fa fa-check"></i><b>5.2</b> Fitting these to data / Estimating them from data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="paraPi.html"><a href="paraPi.html"><i class="fa fa-check"></i><b>6</b> The (proportion) parameter</a><ul>
<li class="chapter" data-level="6.1" data-path="paraPi.html"><a href="paraPi.html#example-one"><i class="fa fa-check"></i><b>6.1</b> Example one</a></li>
<li class="chapter" data-level="6.2" data-path="paraPi.html"><a href="paraPi.html#example-two"><i class="fa fa-check"></i><b>6.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="paraLambda.html"><a href="paraLambda.html"><i class="fa fa-check"></i><b>7</b> The (event rate) parameter</a><ul>
<li class="chapter" data-level="7.1" data-path="paraLambda.html"><a href="paraLambda.html#etc"><i class="fa fa-check"></i><b>7.1</b> Etc</a></li>
<li class="chapter" data-level="7.2" data-path="paraLambda.html"><a href="paraLambda.html#etc-1"><i class="fa fa-check"></i><b>7.2</b> ETC</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html"><i class="fa fa-check"></i><b>8</b> Contrast: 2 mean parameters</a><ul>
<li class="chapter" data-level="8.1" data-path="contrast2Muparas.html"><a href="contrast2Muparas.html#estimand-estimator-estimate"><i class="fa fa-check"></i><b>8.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html"><i class="fa fa-check"></i><b>9</b> Contrast: 2 proportion parameters</a><ul>
<li class="chapter" data-level="9.1" data-path="contrast2Piparas.html"><a href="contrast2Piparas.html#estimand-estimator-estimate-1"><i class="fa fa-check"></i><b>9.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html"><i class="fa fa-check"></i><b>10</b> Contrast: 2 speed parameters</a><ul>
<li class="chapter" data-level="10.1" data-path="contrast2Lambdaparas.html"><a href="contrast2Lambdaparas.html#estimand-estimator-estimate-2"><i class="fa fa-check"></i><b>10.1</b> Estimand, estimator, estimate</a></li>
</ul></li>
<li class="part"><span><b>II Part II</b></span></li>
<li class="chapter" data-level="11" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>11</b> Probability</a><ul>
<li class="chapter" data-level="11.1" data-path="probability.html"><a href="probability.html#objectives"><i class="fa fa-check"></i><b>11.1</b> Objectives</a></li>
<li class="chapter" data-level="11.2" data-path="probability.html"><a href="probability.html#probability-scales"><i class="fa fa-check"></i><b>11.2</b> Probability Scales</a></li>
<li class="chapter" data-level="11.3" data-path="probability.html"><a href="probability.html#basic-rules-for-probability-calculations"><i class="fa fa-check"></i><b>11.3</b> Basic rules for probability calculations</a></li>
<li class="chapter" data-level="11.4" data-path="probability.html"><a href="probability.html#conditional-probabilities-and-independence"><i class="fa fa-check"></i><b>11.4</b> Conditional probabilities, and (in)dependence</a></li>
<li class="chapter" data-level="11.5" data-path="probability.html"><a href="probability.html#changing-the-conditioning-the-direction-matters"><i class="fa fa-check"></i><b>11.5</b> Changing the Conditioning: the direction matters</a></li>
<li class="chapter" data-level="11.6" data-path="probability.html"><a href="probability.html#summary-slides"><i class="fa fa-check"></i><b>11.6</b> Summary Slides</a></li>
<li class="chapter" data-level="11.7" data-path="probability.html"><a href="probability.html#exercises-1"><i class="fa fa-check"></i><b>11.7</b> Exercises:</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="randomVariables.html"><a href="randomVariables.html"><i class="fa fa-check"></i><b>12</b> Random Variables/Variation</a><ul>
<li class="chapter" data-level="12.1" data-path="randomVariables.html"><a href="randomVariables.html#objectives-1"><i class="fa fa-check"></i><b>12.1</b> Objectives</a></li>
<li class="chapter" data-level="12.2" data-path="randomVariables.html"><a href="randomVariables.html#random-variables"><i class="fa fa-check"></i><b>12.2</b> Random Variables</a></li>
<li class="chapter" data-level="12.3" data-path="randomVariables.html"><a href="randomVariables.html#expectation-mean-of-a-random-variable"><i class="fa fa-check"></i><b>12.3</b> Expectation (mean) of a Random Variable</a></li>
<li class="chapter" data-level="12.4" data-path="randomVariables.html"><a href="randomVariables.html#expected-value-of-a-function-of-a-random-variable"><i class="fa fa-check"></i><b>12.4</b> Expected value of a FUNCTION of a random variable</a></li>
<li class="chapter" data-level="12.5" data-path="randomVariables.html"><a href="randomVariables.html#variance-and-thus-sd-of-a-random-variable"><i class="fa fa-check"></i><b>12.5</b> Variance (and thus, SD) of a random variable</a><ul>
<li class="chapter" data-level="12.5.1" data-path="randomVariables.html"><a href="randomVariables.html#definitions"><i class="fa fa-check"></i><b>12.5.1</b> Definitions</a></li>
<li class="chapter" data-level="12.5.2" data-path="randomVariables.html"><a href="randomVariables.html#some-good-reasons-for-using-variance-which-averages-the-squares-of-the-deviations-from-the-mean."><i class="fa fa-check"></i><b>12.5.2</b> Some (good) reasons for using variance, which averages the squares of the deviations from the mean.</a></li>
<li class="chapter" data-level="12.5.3" data-path="randomVariables.html"><a href="randomVariables.html#but-for-end-users-today-."><i class="fa fa-check"></i><b>12.5.3</b> But, for end-users today ….</a></li>
<li class="chapter" data-level="12.5.4" data-path="randomVariables.html"><a href="randomVariables.html#example-of-variance-calculation-using-one-pass-formula"><i class="fa fa-check"></i><b>12.5.4</b> Example of Variance-calculation using one-pass formula</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="randomVariables.html"><a href="randomVariables.html#variance-and-sd-of-a-function-of-a-random-variable"><i class="fa fa-check"></i><b>12.6</b> Variance and SD of a FUNCTION of a random variable</a></li>
<li class="chapter" data-level="12.7" data-path="randomVariables.html"><a href="randomVariables.html#sumsmeansdifferences-of-rvs"><i class="fa fa-check"></i><b>12.7</b> Sums/means/differences of RVs</a><ul>
<li class="chapter" data-level="12.7.1" data-path="randomVariables.html"><a href="randomVariables.html#a-sum-of-2-or-n"><i class="fa fa-check"></i><b>12.7.1</b> A sum (of 2 or <span class="math inline">\(n\)</span>)</a></li>
<li class="chapter" data-level="12.7.2" data-path="randomVariables.html"><a href="randomVariables.html#measurement-errors"><i class="fa fa-check"></i><b>12.7.2</b> Measurement Errors</a></li>
<li class="chapter" data-level="12.7.3" data-path="randomVariables.html"><a href="randomVariables.html#mean-of-2-or-n-rvs"><i class="fa fa-check"></i><b>12.7.3</b> Mean (of 2 or <span class="math inline">\(n\)</span> RVs)</a></li>
<li class="chapter" data-level="12.7.4" data-path="randomVariables.html"><a href="randomVariables.html#difference-of-2-rvs"><i class="fa fa-check"></i><b>12.7.4</b> Difference of 2 RVs</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="randomVariables.html"><a href="randomVariables.html#linear-combinations-of-rvs-regression-slopes"><i class="fa fa-check"></i><b>12.8</b> Linear combinations of RVs (regression slopes)</a></li>
<li class="chapter" data-level="12.9" data-path="randomVariables.html"><a href="randomVariables.html#exercises-2"><i class="fa fa-check"></i><b>12.9</b> Exercises</a></li>
<li class="chapter" data-level="12.10" data-path="randomVariables.html"><a href="randomVariables.html#summary-slides-1"><i class="fa fa-check"></i><b>12.10</b> Summary Slides</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>13</b> Distributions</a><ul>
<li class="chapter" data-level="13.1" data-path="distributions.html"><a href="distributions.html#objectives-2"><i class="fa fa-check"></i><b>13.1</b> Objectives</a></li>
<li class="chapter" data-level="13.2" data-path="distributions.html"><a href="distributions.html#named-distributions"><i class="fa fa-check"></i><b>13.2</b> Named Distributions</a><ul>
<li class="chapter" data-level="13.2.1" data-path="distributions.html"><a href="distributions.html#bernoulli"><i class="fa fa-check"></i><b>13.2.1</b> Bernoulli</a></li>
<li class="chapter" data-level="13.2.2" data-path="distributions.html"><a href="distributions.html#binomial"><i class="fa fa-check"></i><b>13.2.2</b> Binomial</a></li>
<li class="chapter" data-level="13.2.3" data-path="distributions.html"><a href="distributions.html#poisson"><i class="fa fa-check"></i><b>13.2.3</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="distributions.html"><a href="distributions.html#exercises-3"><i class="fa fa-check"></i><b>13.3</b> Exercises</a><ul>
<li class="chapter" data-level="13.3.1" data-path="distributions.html"><a href="distributions.html#clusters-of-miscarriages-based-on-article-by-l-abenhaim"><i class="fa fa-check"></i><b>13.3.1</b> Clusters of Miscarriages [based on article by L Abenhaim]</a></li>
<li class="chapter" data-level="13.3.2" data-path="distributions.html"><a href="distributions.html#prone-ness-to-miscarriages"><i class="fa fa-check"></i><b>13.3.2</b> ‘Prone-ness’ to Miscarriages ?</a></li>
<li class="chapter" data-level="13.3.3" data-path="distributions.html"><a href="distributions.html#automated-chemistries-from-ingelfinger-et-al"><i class="fa fa-check"></i><b>13.3.3</b> Automated Chemistries (from Ingelfinger et al)</a></li>
<li class="chapter" data-level="13.3.4" data-path="distributions.html"><a href="distributions.html#binomial-or-opportunistic-capitalization-on-chance-multiple-looks-at-data"><i class="fa fa-check"></i><b>13.3.4</b> Binomial or Opportunistic? (Capitalization on chance… multiple looks at data)</a></li>
<li class="chapter" data-level="13.3.5" data-path="distributions.html"><a href="distributions.html#can-one-influence-the-sex-of-a-baby"><i class="fa fa-check"></i><b>13.3.5</b> Can one influence the sex of a baby?</a></li>
<li class="chapter" data-level="13.3.6" data-path="distributions.html"><a href="distributions.html#its-the-3rd-week-of-the-course-it-must-be-binomial"><i class="fa fa-check"></i><b>13.3.6</b> It’s the 3rd week of the course: it must be Binomial</a></li>
<li class="chapter" data-level="13.3.7" data-path="distributions.html"><a href="distributions.html#tests-of-intuition"><i class="fa fa-check"></i><b>13.3.7</b> Tests of intuition</a></li>
<li class="chapter" data-level="13.3.8" data-path="distributions.html"><a href="distributions.html#ci-for-proportion-when-observe-0n-or-nn"><i class="fa fa-check"></i><b>13.3.8</b> CI for proportion when observe 0/n or n/n</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Part III</b></span></li>
<li class="chapter" data-level="14" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>14</b> Mathematics</a><ul>
<li class="chapter" data-level="14.1" data-path="math.html"><a href="math.html#notation"><i class="fa fa-check"></i><b>14.1</b> Notation</a></li>
<li class="chapter" data-level="14.2" data-path="math.html"><a href="math.html#powers-logarithms-and-antilogarithms"><i class="fa fa-check"></i><b>14.2</b> Powers, Logarithms and Anti–logarithms</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="computing01.html"><a href="computing01.html"><i class="fa fa-check"></i><b>15</b> Computing Session 1</a><ul>
<li class="chapter" data-level="15.1" data-path="computing01.html"><a href="computing01.html#objectives-3"><i class="fa fa-check"></i><b>15.1</b> Objectives</a></li>
<li class="chapter" data-level="15.2" data-path="computing01.html"><a href="computing01.html#background-to-two-datasets"><i class="fa fa-check"></i><b>15.2</b> Background to two datasets</a><ul>
<li class="chapter" data-level="15.2.1" data-path="computing01.html"><a href="computing01.html#climate"><i class="fa fa-check"></i><b>15.2.1</b> Climate</a></li>
<li class="chapter" data-level="15.2.2" data-path="computing01.html"><a href="computing01.html#ages-of-cars"><i class="fa fa-check"></i><b>15.2.2</b> Ages of Cars</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="computing01.html"><a href="computing01.html#statisticalcomputing-tasks"><i class="fa fa-check"></i><b>15.3</b> Statistical/Computing Tasks</a><ul>
<li class="chapter" data-level="15.3.1" data-path="computing01.html"><a href="computing01.html#guesses-re-date-of-ice-breakup"><i class="fa fa-check"></i><b>15.3.1</b> Guesses re Date of Ice Breakup</a></li>
<li class="chapter" data-level="15.3.2" data-path="computing01.html"><a href="computing01.html#how-old-are-uk-cars"><i class="fa fa-check"></i><b>15.3.2</b> How old are UK cars?</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="computing01.html"><a href="computing01.html#the-p-and-q-functions-an-orientation"><i class="fa fa-check"></i><b>15.4</b> The p and q functions: an orientation</a></li>
<li class="chapter" data-level="15.5" data-path="computing01.html"><a href="computing01.html#shapes-of-distributions"><i class="fa fa-check"></i><b>15.5</b> Shapes of Distributions</a></li>
<li class="chapter" data-level="15.6" data-path="computing01.html"><a href="computing01.html#exercises-4"><i class="fa fa-check"></i><b>15.6</b> Exercises</a><ul>
<li class="chapter" data-level="15.6.1" data-path="computing01.html"><a href="computing01.html#guesses-in-nenana-ice-classic"><i class="fa fa-check"></i><b>15.6.1</b> Guesses in Nenana Ice Classic</a></li>
<li class="chapter" data-level="15.6.2" data-path="computing01.html"><a href="computing01.html#exercise-ages-of-uk-cars"><i class="fa fa-check"></i><b>15.6.2</b> Exercise: Ages of UK Cars</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="computing01.html"><a href="computing01.html#summary-2"><i class="fa fa-check"></i><b>15.7</b> SUMMARY</a><ul>
<li class="chapter" data-level="15.7.1" data-path="computing01.html"><a href="computing01.html#computing"><i class="fa fa-check"></i><b>15.7.1</b> Computing</a></li>
<li class="chapter" data-level="15.7.2" data-path="computing01.html"><a href="computing01.html#statistical-concepts-and-principles"><i class="fa fa-check"></i><b>15.7.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="computing02.html"><a href="computing02.html"><i class="fa fa-check"></i><b>16</b> Computing: Session No. 2</a><ul>
<li class="chapter" data-level="16.1" data-path="computing02.html"><a href="computing02.html#objectives-4"><i class="fa fa-check"></i><b>16.1</b> Objectives</a></li>
<li class="chapter" data-level="16.2" data-path="computing02.html"><a href="computing02.html#scientific-background"><i class="fa fa-check"></i><b>16.2</b> Scientific background</a></li>
<li class="chapter" data-level="16.3" data-path="computing02.html"><a href="computing02.html#random-variation"><i class="fa fa-check"></i><b>16.3</b> Random Variation</a><ul>
<li class="chapter" data-level="16.3.1" data-path="computing02.html"><a href="computing02.html#measurement-errors-1"><i class="fa fa-check"></i><b>16.3.1</b> Measurement errors</a></li>
<li class="chapter" data-level="16.3.2" data-path="computing02.html"><a href="computing02.html#biological-variation"><i class="fa fa-check"></i><b>16.3.2</b> Biological variation</a></li>
<li class="chapter" data-level="16.3.3" data-path="computing02.html"><a href="computing02.html#example-2"><i class="fa fa-check"></i><b>16.3.3</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="computing02.html"><a href="computing02.html#when-these-laws-dont-apply"><i class="fa fa-check"></i><b>16.4</b> When these Laws don’t apply</a></li>
<li class="chapter" data-level="16.5" data-path="computing02.html"><a href="computing02.html#summary-3"><i class="fa fa-check"></i><b>16.5</b> SUMMARY</a><ul>
<li class="chapter" data-level="16.5.1" data-path="computing02.html"><a href="computing02.html#computing-1"><i class="fa fa-check"></i><b>16.5.1</b> Computing</a></li>
<li class="chapter" data-level="16.5.2" data-path="computing02.html"><a href="computing02.html#statistical-concepts-and-principles-1"><i class="fa fa-check"></i><b>16.5.2</b> Statistical Concepts and Principles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="computing03.html"><a href="computing03.html"><i class="fa fa-check"></i><b>17</b> Computing: Session No. 3</a><ul>
<li class="chapter" data-level="17.1" data-path="computing03.html"><a href="computing03.html#objectives-5"><i class="fa fa-check"></i><b>17.1</b> Objectives</a></li>
<li class="chapter" data-level="17.2" data-path="computing03.html"><a href="computing03.html#exercises-7"><i class="fa fa-check"></i><b>17.2</b> Exercises</a><ul>
<li class="chapter" data-level="17.2.1" data-path="computing03.html"><a href="computing03.html#pooled-blood"><i class="fa fa-check"></i><b>17.2.1</b> Pooled Blood</a></li>
<li class="chapter" data-level="17.2.2" data-path="computing03.html"><a href="computing03.html#life-tables-1990"><i class="fa fa-check"></i><b>17.2.2</b> Life Tables [1990]</a></li>
<li class="chapter" data-level="17.2.3" data-path="computing03.html"><a href="computing03.html#life-tables-2018"><i class="fa fa-check"></i><b>17.2.3</b> Life Tables [2018]</a></li>
<li class="chapter" data-level="17.2.4" data-path="computing03.html"><a href="computing03.html#a-simplified-epidemic"><i class="fa fa-check"></i><b>17.2.4</b> A simplified epidemic</a></li>
<li class="chapter" data-level="17.2.5" data-path="computing03.html"><a href="computing03.html#screening-for-hiv"><i class="fa fa-check"></i><b>17.2.5</b> Screening for HIV</a></li>
<li class="chapter" data-level="17.2.6" data-path="computing03.html"><a href="computing03.html#duplicate-birthdays"><i class="fa fa-check"></i><b>17.2.6</b> Duplicate Birthdays</a></li>
<li class="chapter" data-level="17.2.7" data-path="computing03.html"><a href="computing03.html#chevalier-de-méré"><i class="fa fa-check"></i><b>17.2.7</b> Chevalier de Méré</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="computing03.html"><a href="computing03.html#other-exercises-under-construction"><i class="fa fa-check"></i><b>17.3</b> Other Exercises (under construction)</a><ul>
<li class="chapter" data-level="17.3.1" data-path="computing03.html"><a href="computing03.html#hiv-transmission"><i class="fa fa-check"></i><b>17.3.1</b> HIV transmission</a></li>
<li class="chapter" data-level="17.3.2" data-path="computing03.html"><a href="computing03.html#the-2018-book-of-guesses"><i class="fa fa-check"></i><b>17.3.2</b> The 2018 Book of Guesses</a></li>
<li class="chapter" data-level="17.3.3" data-path="computing03.html"><a href="computing03.html#trends-over-the-last-100-years"><i class="fa fa-check"></i><b>17.3.3</b> Trends over the last 100 years</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="computing04.html"><a href="computing04.html"><i class="fa fa-check"></i><b>18</b> Computing: Session No. 4</a><ul>
<li class="chapter" data-level="18.1" data-path="computing04.html"><a href="computing04.html#objectives-6"><i class="fa fa-check"></i><b>18.1</b> Objectives</a></li>
<li class="chapter" data-level="18.2" data-path="computing04.html"><a href="computing04.html#exercises-8"><i class="fa fa-check"></i><b>18.2</b> Exercises</a><ul>
<li class="chapter" data-level="18.2.1" data-path="computing04.html"><a href="computing04.html#ages-of-uk-cars"><i class="fa fa-check"></i><b>18.2.1</b> Ages of UK cars</a></li>
<li class="chapter" data-level="18.2.2" data-path="computing04.html"><a href="computing04.html#lengths-of-babies-names"><i class="fa fa-check"></i><b>18.2.2</b> Lengths of Babies’ Names</a></li>
<li class="chapter" data-level="18.2.3" data-path="computing04.html"><a href="computing04.html#life-tables-2018-1"><i class="fa fa-check"></i><b>18.2.3</b> Life Tables [2018]</a></li>
<li class="chapter" data-level="18.2.4" data-path="computing04.html"><a href="computing04.html#variable-length-parallel-parking-spaces"><i class="fa fa-check"></i><b>18.2.4</b> Variable-length (parallel) parking spaces</a></li>
<li class="chapter" data-level="18.2.5" data-path="computing04.html"><a href="computing04.html#galtons-data-on-family-heights"><i class="fa fa-check"></i><b>18.2.5</b> Galton’s data on family heights</a></li>
<li class="chapter" data-level="18.2.6" data-path="computing04.html"><a href="computing04.html#height-differences-of-random-m-f-pairs"><i class="fa fa-check"></i><b>18.2.6</b> Height differences of random M-F pairs</a></li>
<li class="chapter" data-level="18.2.7" data-path="computing04.html"><a href="computing04.html#same-sex-or-opposite-sex"><i class="fa fa-check"></i><b>18.2.7</b> Same-sex or opposite-sex?</a></li>
<li class="chapter" data-level="18.2.8" data-path="computing04.html"><a href="computing04.html#box-minus-bowl"><i class="fa fa-check"></i><b>18.2.8</b> Box minus bowl</a></li>
<li class="chapter" data-level="18.2.9" data-path="computing04.html"><a href="computing04.html#car-parking-fixed-and-variable"><i class="fa fa-check"></i><b>18.2.9</b> car parking fixed and variable</a></li>
<li class="chapter" data-level="18.2.10" data-path="computing04.html"><a href="computing04.html#correcting-length-biased-sampling"><i class="fa fa-check"></i><b>18.2.10</b> Correcting length-biased sampling</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="computing04.html"><a href="computing04.html#other-exercises-under-construction-1"><i class="fa fa-check"></i><b>18.3</b> Other Exercises (under construction)</a><ul>
<li class="chapter" data-level="18.3.1" data-path="computing04.html"><a href="computing04.html#the-2018-book-of-guesses-1"><i class="fa fa-check"></i><b>18.3.1</b> The 2018 Book of Guesses</a></li>
<li class="chapter" data-level="18.3.2" data-path="computing04.html"><a href="computing04.html#trends-over-the-last-100-years-1"><i class="fa fa-check"></i><b>18.3.2</b> Trends over the last 100 years</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="dalite.html"><a href="dalite.html"><i class="fa fa-check"></i><b>19</b> DALITE</a><ul>
<li class="chapter" data-level="19.1" data-path="dalite.html"><a href="dalite.html#aim"><i class="fa fa-check"></i><b>19.1</b> Aim</a></li>
<li class="chapter" data-level="19.2" data-path="dalite.html"><a href="dalite.html#how-it-works"><i class="fa fa-check"></i><b>19.2</b> How it works</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Analysis: a regression-from-the-outset approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="randomVariables" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Random Variables/Variation</h1>
<div id="objectives-1" class="section level2">
<h2><span class="header-section-number">12.1</span> Objectives</h2>
<p>This <strong>central</strong> chapter addresses a <strong>fundamental concept</strong>, namely the <strong>variance of a random variable</strong>. It gives the laws governing the variance of a sum of 2, or (especially) <span class="math inline">\(n\)</span> random variables – and even more importantly – the laws governing <strong>the variance of a difference of two random variables.</strong> The latter is central, not just to simple contrasts involving just 2 sample means or proportions, but also in the much wider world of regression, since the variance (sampling variability) of any regression slope can be viewed as the <strong>variance of a linear combination of</strong> random ‘errors’, or random deviations, or <strong>random variables</strong>. So, if there is one ‘master’ formula to pay attention to and to ‘own’, it is the one for the variance of a linear combination of random variables. All others are special cases of this.</p>
<p>So, the <strong>specific objectives</strong> are to truly understand</p>
<ul>
<li><p>the concept of a random variable.</p></li>
<li><p>the concept of the (expectation and) variance of a random variable.</p></li>
<li><p>why it is that, when dealing with the sum of two or more independent random variables, it is not their standard deviations that sum (add), but rather their variances.</p></li>
<li><p>likewise, when dealing with the <strong>difference</strong> of two independent random variables, or some <strong>linear combination</strong> of <span class="math inline">\(n\)</span> independent random variables involving positive and negative weights, why it is that <strong>the component variances add</strong>, and <strong>with what weights</strong>.</p></li>
</ul>
</div>
<div id="random-variables" class="section level2">
<h2><span class="header-section-number">12.2</span> Random Variables</h2>
<p><strong>Textbook definitions of RANDOM VARIABLE</strong></p>
<p>(Note: JH has changed the usual <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>)</p>
<blockquote>
<p>A variable (<span class="math inline">\(Y\)</span>) whose value is a number determined by the outcome of an experiment</p>
</blockquote>
<blockquote>
<p>A variable (<span class="math inline">\(Y\)</span>) whose value is a numerical outcome of a random phenomenon</p>
</blockquote>
<p><strong>Textbook definitions: DISCRETE Random Variable</strong></p>
<blockquote>
<p>A random variable that assumes only a finite (or countably infinite) number of distinct values</p>
</blockquote>
<blockquote>
<p>Discrete random variables have “a finite or countably infinite number of possible values, each with positive or zero probability.</p>
</blockquote>
<blockquote>
<p>A discrete random variable <span class="math inline">\(Y\)</span> has a finite number of possible values</p>
</blockquote>
<p>Random Variable: DISCRETE or CONTINUOUS ?</p>
<ul>
<li><em>How long</em> you have to wait for bus / elevator / surgery/ download to complete</li>
<li>the <em>blood group</em> of n = 1 randomly selected person</li>
<li><em>how many tries</em> before pass a course</li>
<li><em>how many</em> of n = 20 randomly selected persons will return questionnaire in pilot study</li>
<li><em>length of song</em> on a CD</li>
<li><em>mean cholesterol level</em> in sample of n = 30 randomly selected persons</li>
<li><em>how hot</em> it is going to be today</li>
<li><em>how much</em> snow we will get next winter</li>
<li><em>time</em> someone called (on answering machine)</li>
<li><em>value of test-statistic</em> if 2 populations sampled from have the same mean</li>
<li><em>how much</em> ice McDonalds puts in soft drink</li>
<li><em>how many</em> calories in hamburger</li>
<li><em>how many</em> numbers you get correct in 6/49?</li>
<li><em>where</em> roulette wheel stops</li>
<li><em>how many</em> “wrong number” calls received</li>
<li><em>how many</em> keys you have to try before get the right one</li>
<li><em>how much</em> water consumed by 100 homes</li>
</ul>
<p><strong>PROBABILITY DISTRIBUTION <span class="math inline">\(p(y)\)</span> associated with (Discrete) Random Variable <span class="math inline">\(Y\)</span></strong></p>
<p>The ordered pairs {y, Probability(<span class="math inline">\(Y\)</span> = <span class="math inline">\(y\)</span>) } , where <span class="math inline">\(y\)</span> ranges over the possible values of <span class="math inline">\(Y\)</span></p>
<ul>
<li><p>Probability(<span class="math inline">\(Y\)</span> = <span class="math inline">\(y\)</span>) is often shortened to Prob(<span class="math inline">\(Y\)</span> = <span class="math inline">\(y\)</span>) or P(<span class="math inline">\(Y\)</span> = <span class="math inline">\(y\)</span>)</p></li>
<li><p>Can display distribution as formula, table, graph, etc.</p></li>
</ul>
<p>EXAMPLES</p>
<p>(Note: the only reason we start with lotteries is that we can give the exact, mathematically-known, probabilities of each possible result. In most of the other examples, the probabilities will be context-specific, and seldom knowable with full exactness.)</p>
<ul>
<li>Y = Winnings on a $1 wager on la Quotidienne 3 (same distribution for exact order and any order)</li>
</ul>
<p><code>....... ($)</code><br />
<code>........  y .. Probability(Y = y)</code></p>
<p><code>........ 0 ............... 0.999</code><br />
<code>...... 450 ............... 0.001</code><br />
<code>.......................... =====</code><br />
<code>...... AL................. 1.000</code></p>
<ul>
<li>Randomly select n = 1 person<br />
Y = the person’s Blood Group</li>
</ul>
<p><code>.... y   Probability(Y = y)</code></p>
<p><code>.... A .............. p(A)</code><br />
<code>.... B .............. p(B)</code><br />
<code>.... AB ............ p(AB)</code><br />
<code>.... O .............. p(O)</code><br />
<code>.................... =====</code><br />
<code>....ALL ............ 1.000</code></p>
<ul>
<li>In a 1911 census, of all households with at least 1 child aged 12 or under, the proportions of households that had 1, 2, … children. [We ignore for now any errors in recording ages]</li>
</ul>
<pre><code>##      n.children proportion.of.households
## [1,]          1                    0.335
## [2,]          2                    0.245
## [3,]          3                    0.180
## [4,]          4                    0.126
## [5,]          5                    0.072
## [6,]          6                    0.031
## [7,]          7                    0.009
## [8,]          8                    0.002</code></pre>
<ul>
<li>Proportions of (currently) married women aged 45 and older who, in a 1911 census, reported this “number of children born alive to the present marriage”</li>
</ul>
<pre><code>##       n.children proportion.of.women
##  [1,]          0            0.243878
##  [2,]          1            0.047704
##  [3,]          2            0.058265
##  [4,]          3            0.067908
##  [5,]          4            0.072704
##  [6,]          5            0.078010
##  [7,]          6            0.077653
##  [8,]          7            0.074133
##  [9,]          8            0.072245
## [10,]          9            0.061939
## [11,]         10            0.055357
## [12,]         11            0.034031
## [13,]         12            0.026071
## [14,]         13            0.014031
## [15,]         14            0.007959
## [16,]         15            0.004337
## [17,]         16            0.001837
## [18,]         17            0.001020
## [19,]         18            0.000561
## [20,]         19            0.000306
## [21,]         20            0.000051</code></pre>
<ul>
<li>Choose a random word and count how many characters (letters) it contains<br />
Y = Number of characters in word</li>
</ul>
<p><code>.... y . Probability(Y = y)</code></p>
<p><code>.... 1 .............. p(1)</code><br />
<code>.... 2 .............. p(2)</code><br />
<code>.... etc. ........... etc.</code><br />
<code>.................... =====</code><br />
<code>... ALL ............ 1.000</code></p>
<ul>
<li>Choose a random hospital admission and count how many days it lasted</li>
</ul>
<p>Y = Length of Stay (LOS)</p>
<p><code>.... y ..... Probability(Y = y)</code></p>
<p><code>.... 1 day ............... p(1)</code><br />
<code>.... 2 days .............. p(2)</code><br />
<code>.... etc. ................ etc.</code><br />
<code>......................... =====</code><br />
<code>.... ALL ................ 1.000</code></p>
<p>Note 1: The probabilities (or if you prefer to think of them as proportions) add to 1. We use probabilities or fractions as relative frequencies (like a histogram with an infinite number of entries)</p>
<p>Note 2: typically, the random quantity is obtained from an aggregate of elements e.g. a sum, mean , proportion, regression slope</p>
<p>MORE EXAMPLES OF PROBABILITY DISTRIBUTIONS</p>
<p>(Some are on a continuous scale, but for convenience, the possible values are shown ‘binned’ into intervals)</p>
<ul>
<li>To pilot-test what the return rate of a mail survey is going to be, you mail the questionnaire to n = 20 randomly selected persons.<br />
Y = the number who will return questionnaire</li>
</ul>
<p><code>...... y  Probability(Y = y)</code><br />
<code>...... 0 ............. p(0)</code><br />
<code>...... 1 ............. p(1)</code><br />
<code>...... 2 ............. p(1)</code><br />
<code>....... .............. ....</code><br />
<code>..... 19 ............ p(19)</code><br />
<code>..... 20 ............ p(20)</code><br />
<code>..................... =====</code><br />
<code>..... ALL ........... 1.000</code></p>
<ul>
<li>People call out their birthdays:<br />
Y = when get 1st duplicate</li>
</ul>
<p><code>...... y  Probability(Y = y)</code></p>
<p><code>...... 2 ............. p(2)</code><br />
<code>...... 3 ............. p(3)</code><br />
<code>....... ...................</code><br />
<code>.... 366 ........... p(366)</code><br />
<code>.................... =====</code><br />
<code>..... ALL .......... 1.000</code></p>
<ul>
<li>Y = Winnings on a $1 wager on la Quotidienne 4 (same distribution for exact order and any order)</li>
</ul>
<p><code>....... ($)</code><br />
<code>........ y .. Probability(Y = y)</code></p>
<p><code>........ 0 ............... 0.999</code><br />
<code>..... 4500 ............... 0.001</code><br />
<code>.......................... =====</code><br />
<code>..... ALL ................ 1.000</code></p>
<ul>
<li>Y.bar = Mean cholesterol level in randomly selected sample of n = 30 persons</li>
</ul>
<p><code>.... y.bar .. Probability(y.bar would be...)</code></p>
<p><code>.. &lt;  4.5 mmol/L ............. p( . )</code><br />
<code>.... 4.5 - 4.6 .............. p( . )</code><br />
<code>.... 4.6 - 4.7 .............. p( . )</code><br />
<code>.... 4.7 - 4.8 .............. p( . )</code><br />
<code>.... 4.9 - 4.9 .............. p( . )</code><br />
<code>.... 4.9 - 5.0 .............. p( . )</code><br />
<code>.... 5.0 - 5.1 .............. p( . )</code><br />
<code>.... 5.1 - 5.2 .............. p( . )</code><br />
<code>.... etc</code><br />
<code>.... 5.6 - 5.7 .............. p( . )</code><br />
<code>.... 5.7 - 5.8 .............. p( . )</code><br />
<code>.... 5.8 - 5.9 .............. p( . )</code><br />
<code>.... 5.9 - 6.0 .............. p( . )</code><br />
<code>........ &gt; 6.0 .............. p( . )</code><br />
<code>............................. =====</code><br />
<code>..... ALL ................... 1.000</code></p>
<ul>
<li>Y = the value of the test- statistic if 2 populations sampled from had the same population mean</li>
</ul>
<p><code>.... test.statistic (Z) .. Probability(Z would be...)</code></p>
<p><code>.. &lt; -2 ........................... 0.028</code><br />
<code>.... -2 to -1 ..................... 0.136</code><br />
<code>.... -1 to 0 ...................... 0.341</code><br />
<code>..... 0 to +1 ..................... 0.341</code><br />
<code>.... +1 to +2 ..................... 0.341</code><br />
<code>.........&gt; +2 ..................... 0.028</code><br />
<code>................................... =====</code><br />
<code>..... ALL ......................... 1.000</code></p>
<ul>
<li>In 6/49 lottery, player selects 6 distinct numbers on a grid showing the numbers 1 to 49.<br />
49 otherwise identical balls, but numbered 1 to 49, are thoroughly mixed in an urn<br />
6 balls are drawn without replacement.<br />
.<br />
Y = how many of the balls drawn show numbers that match the numbers selected by player</li>
</ul>
<p><code>......... y .. Probability(Y = y)</code></p>
<p><code>......... 0 .......... 0.4359650</code><br />
<code>......... 1 .......... 0.4130195</code><br />
<code>......... 2 .......... 0.1323780</code><br />
<code>......... 3 .......... 0.0176504</code><br />
<code>......... 4 .......... 0.0009686</code><br />
<code>......... 5 .......... 0.0000184</code><br />
<code>......... 6 .......... 0.0000001</code><br />
<code>...................... =========</code><br />
<code>....... ALL .......... 1.0000000</code></p>
<blockquote>
<p><a href="http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/banco.pdf" class="uri">http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/banco.pdf</a></p>
</blockquote>
<blockquote>
<p><a href="http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/Keno(20-spot).pdf" class="uri">http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/Keno(20-spot).pdf</a></p>
</blockquote>
</div>
<div id="expectation-mean-of-a-random-variable" class="section level2">
<h2><span class="header-section-number">12.3</span> Expectation (mean) of a Random Variable</h2>
<p><strong>DEFINITION</strong></p>
<p>If <span class="math inline">\(Y\)</span> takes on the DISCRETE values</p>
<p>… <span class="math inline">\(y_1\)</span> with probability <span class="math inline">\(p_1\)</span>,<br />
… <span class="math inline">\(y_2\)</span> with probability <span class="math inline">\(p_2\)</span>,<br />
… <span class="math inline">\(y_3\)</span> with probability <span class="math inline">\(p_3\)</span>,<br />
… etc … <span class="math inline">\(y_k\)</span> with probability <span class="math inline">\(p_k\)</span>,</p>
<p>then the <strong>expected value of <span class="math inline">\(Y\)</span> (written ‘E(<span class="math inline">\(Y\)</span>)’ )</strong> is</p>
<p><span class="math display">\[E[Y] = y_1 \times p_1 + y_2 \times p_2 + \dots + y_k \times p_k = \sum y_i \times p_i.\]</span></p>
<p>E(<span class="math inline">\(Y\)</span>) is a mean that uses expected (i.e. unobservable or theoretical or long run) relative frequencies (<span class="math inline">\(p\)</span>’s). ( <span class="math inline">\(\bar{y}\)</span> uses observed relative frequencies. )</p>
<p>Can think of E(<span class="math inline">\(Y\)</span>) as ‘center of mass’ of <span class="math inline">\(p().\)</span></p>
<p>If <span class="math inline">\(Y\)</span> takes on the CONTINUOUS values <span class="math inline">\(y\)</span> - <span class="math inline">\(\delta y/2\)</span> to <span class="math inline">\(y\)</span> - <span class="math inline">\(\delta y/2\)</span> with probability <span class="math inline">\(p = f(y) \delta y\)</span>, then <span class="math display">\[ E[Y] = \int f(y) dy.\]</span></p>
<p><strong>RELEVANCE</strong> of Expectation of a Random Variable</p>
<ul>
<li><p>IT ACTS AS A MEAN FOR A VARIABLE THAT HAS A (CONCEPTUAL) REPETITION OR AN INFINITE N</p></li>
<li><p>THE EXPECTED VALUE OF A RANDOM VARIABLE Y WILL USUALLY BE IN TERMS OF POPULATION PARAMETERS.<br />
.<br />
A STATISTIC WITH EXPECTED VALUE <span class="math inline">\(\theta\)</span> IS AN ‘UNBIASED ESTIMATOR’ OF <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>example 1:<br />
Y = Proportion of ‘YES’ in sample on <span class="math inline">\(n\)</span><br />
E(<span class="math inline">\(Y\)</span>) = <span class="math inline">\(\pi\)</span> = PROPORTION of YES’ in POPULATION<br />
Then <span class="math inline">\(\hat{\pi}\)</span> = <span class="math inline">\(Y,\)</span> and Y is an unbiased estimator of <span class="math inline">\(\pi.\)</span></p></li>
<li><p>example 2:<br />
If we use a divisor of <span class="math inline">\(n - 1\)</span> to calculate the sample variance <span class="math inline">\(s^2 = \frac{\sum(y-\bar{y})^2}{n-1},\)</span> then<br />
E(<span class="math inline">\(s^2) = \sigma^2\)</span>, so <span class="math inline">\(s^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. ( <span class="math inline">\(\widehat{\sigma^2}\)</span> stands for ‘estimate/estimator of’ <span class="math inline">\(\sigma^2.\)</span>)<br />
If we use a divisor of <span class="math inline">\(n\)</span>, then<br />
<span class="math display">\[E\bigg(\frac{\sum(y-\bar{y})^2}{n}\bigg) = \frac{n-1}{n} \times \sigma^2.\]</span><br />
This estimator produces estiamtes that, on average, are too small.</p></li>
</ul>
<p><strong>EXAMPLES</strong> of EXPECTED VALUE of Random Variable</p>
<ul>
<li>Y = Winnings on a $1 wager on la Quotidienne 3</li>
</ul>
<p><code>..... y($) ... p(y) ...... y x p(y)</code></p>
<p><code>........ 0 .. 0.999 ....... $0.00</code><br />
<code>...... 450 ...0.001 ....... $0.45</code><br />
<code>.......................... =====</code><br />
<code>...... ..............SUM .. $0.45</code></p>
<p>Some may find it easier to think of averaging the winnings of 1 person who won 450 dollars and 999 persons who won 0 dollars.</p>
<ul>
<li><p>Keno: Y = Winnings on a $3 wager<br />
.<br />
E(Winnings) = $2.12 (&gt; 70%) <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/Keno(20-spot).pdf">see here.</a></p></li>
<li><p>Banco: Winnings on a $1 wager<br />
.<br />
42% ≤ E(Winnings) ≤ 55%, depending on how many numbers played <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/banco.pdf">see here.</a></p></li>
<li><p><strong>Longevity of a fictitious birth cohort</strong> if they were to experience age-specific death rates observed in Quebec in 1990 [see exercises in 3rd computing session]</p></li>
<li><p>Y = Length of life = age at death.<br />
Y = Age at death (or Longevity or length of Life, or lifetime, if you prefer to be positive) . . E(Y) = E(Lifetime) = ‘<strong>LIFE EXPECTANCY AT BIRTH</strong>’<br />
.<br />
Assume for sake of illustration that deaths in a decade are all at midpoint of interval (calculations done using one-year-wide age-bins rather than one-decade-wide would be more exact)</p></li>
</ul>
<p><code>age = mid-point of decade (for simplicity)</code><br />
<code>p = proportion dying in decade</code><br />
<code>....................MALES .............. FEMALES</code></p>
<p><code>decade .... age ..... p .... age x p ...... p ...... age x p</code><br />
.<br />
<code>00-10 ...... 05 .... 0.010 ... 0.050 ..... 0.080 ..... 0.040</code><br />
<code>10-20 ...... 15 .... 0.006 ... 0.089 ..... 0.002 ..... 0.030</code><br />
<code>20-30 ...... 25 .... 0.012 ... 0.295 ..... 0.004 ......y.yyy</code><br />
<code>30-40 ...... 35 .... 0.016 ... 0.544 ..... 0.007 ......y.yyy</code><br />
<code>40-50 ...... 45 .... 0.030 ... 1.335 ..... 0.017 ......y.yyy</code><br />
<code>50-60 ...... 55 .... 0.074 ... 4.079 ..... 0.040 ......y.yyy</code><br />
<code>60-70 ...... 65 .... 0.180 .. 11.697 ..... 0.096 ......y.yyy</code><br />
<code>70-80 ...... 75 .... 0.301 .. 22.610 ..... 0.214 ......y.yyy</code><br />
<code>80-90 .. ... 85 .... 0.279 .. 23.680 ..... 0.358 ......y.yyy</code><br />
<code>90-100 ..... 95 .... 0.093 ... 8.822 ..... 0.254 .....24.136</code><br />
<code>.................... =.=== .. ==.=== ..... =.=== .... ==.===</code><br />
<code>.ALL ............... 1.000 .. 73.2 ....... 1.000 .... xx.x</code></p>
<ul>
<li>Mean no. of children 12 years and under in households with at least 1 such child:</li>
</ul>
<pre><code>##      n.children proportion.of.households product
## [1,]          1                    0.335   0.335
## [2,]          2                    0.245   0.490
## [3,]          3                    0.180   0.540
## [4,]          4                    0.126   0.504
## [5,]          5                    0.072   0.360
## [6,]          6                    0.031   0.186
## [7,]          7                    0.009   0.063
## [8,]          8                    0.002   0.016</code></pre>
<pre><code>## . Sum(products) = Expected Value = Mean =  2.5</code></pre>
<ul>
<li>If you want to start <strong>your own Insurance Company</strong> Y = Payout (from -99,750 to +$1250)</li>
</ul>
<p>E(Payout on single policy) &gt; 0 BUT… Variance(Payout) is VERY LARGE</p>
<p>see MM3 p 341</p>
<ul>
<li>Distance Where to wait if 3 unequally spaced elevators ? prob(it’s #1) = p(it’s #2)=p(it’s #3) = 1/3</li>
</ul>
</div>
<div id="expected-value-of-a-function-of-a-random-variable" class="section level2">
<h2><span class="header-section-number">12.4</span> Expected value of a FUNCTION of a random variable</h2>
<ul>
<li>If <span class="math inline">\(Y \ \sim \ p(y)\)</span>, i.e., <span class="math inline">\(Y\)</span> has probability distribution p(<span class="math inline">\(y\)</span>), and if <span class="math inline">\(g(Y)\)</span> is some real-valued function of <span class="math inline">\(Y\)</span>, then <span class="math display">\[E[ g(Y) ] = \sum g(y)  \times p(y),\]</span> i.e. it is a weighted mean of the <span class="math inline">\(g(y)\)</span>’s, with <span class="math inline">\(p(y)\)</span>’s as weights.</li>
</ul>
<p><strong>NOTE</strong> In some instances, the expectation of <span class="math inline">\(g(Y)\)</span> is <span class="math inline">\(g(E[Y])\)</span>, while in others it is more complex. Can you figure out when it is/is not in each of the following instances?</p>
<ul>
<li><p><strong>Examples</strong></p></li>
<li><p><span class="math inline">\(Y\)</span> = Noon Temperature (C) in Montreal on a randomly selected day of the year;<br />
<span class="math inline">\(g(Y)\)</span> = Temperature (F) = 32 + (9/5) <span class="math inline">\(Y\)</span></p></li>
<li><p><span class="math inline">\(Y\)</span> = Weight in Kg (or Height in cm) of a randomly selected person;<br />
g(Y) = Weight in Kg (or Height in inches)</p></li>
<li><p><span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are two random variables that might or might not be related;<br />
.<br />
if <span class="math inline">\(g(Y_1, Y_2) = Y_1 + Y_2,\)</span> then <span class="math inline">\(E[g(Y_1, Y_2)] = E[Y_1] + E[Y_2].\)</span><br />
.<br />
if <span class="math inline">\(g(Y_1, Y_2) = \frac{Y_1 + Y_2}{2},\)</span> then <span class="math inline">\(E[g(Y_1, Y_2)] = \frac{E[Y_1] + E[Y_2]}{2},\)</span><br />
.<br />
and, by analogy, for a sum or mean of <span class="math inline">\(n\)</span> related or unrelated random variables.</p></li>
<li><p><span class="math inline">\(Y\)</span> = diameter of a randomly chosen sphere;<br />
<span class="math inline">\(g(Y)\)</span> = Volume of sphere = <span class="math inline">\(\frac{\pi}{6} Y^3.\)</span></p></li>
<li><p><span class="math inline">\(Y\)</span> = fuel consumption, in liters/100km, of a randomly selected make of car;<br />
<span class="math inline">\(g(Y)\)</span> = miles per gallon or Km per liter (reciprocal)</p></li>
<li><p><span class="math inline">\(Y\)</span> = which of 3 <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/median-elevator.pdf"><strong>unequally spaced elevators</strong></a> shows up next.<br />
Prob(it is #1) = prop(it is #2) = prob(it’s #3) = 1/3.<br />
.<br />
<span class="math inline">\(g(Y)\)</span> = Distance to elevator. How to mimimize E(distance)?<br />
.<br />
<span class="math inline">\(g(Y)\)</span> = <strong>Squared Distance</strong> to elevator. How to mimimize E(<span class="math inline">\(g(Y)\)</span>)?</p></li>
<li><p>Random Variable Y with Expectation or Mean <span class="math inline">\(\mu\)</span><br />
.<br />
<span class="math inline">\(g(Y) = (Y - \mu)^2\)</span>, the <strong>squared deviation from the mean</strong></p></li>
</ul>
<p><strong>The following is in bold to emphasize one of the most fundamental concepts in statistics, namely VARIANCE.</strong></p>
</div>
<div id="variance-and-thus-sd-of-a-random-variable" class="section level2">
<h2><span class="header-section-number">12.5</span> Variance (and thus, SD) of a random variable</h2>
<div id="definitions" class="section level3">
<h3><span class="header-section-number">12.5.1</span> Definitions</h3>
<ul>
<li><p>E[<span class="math inline">\((Y - \mu)^2\)</span>] is called the VARIANCE of the random variable <span class="math inline">\(Y\)</span>. It is usually shortened to Var(<span class="math inline">\(Y\)</span>) or even to V(<span class="math inline">\(Y\)</span>).</p></li>
<li><p><strong>It, and its positive square root, called the <em>standard deviation</em> of <span class="math inline">\(Y\)</span>, or SD(<span class="math inline">\(Y\)</span>), are two of the most commonly used measures of variability or spread or uncertainty.</strong></p></li>
<li><p>Computationally, Variance(<span class="math inline">\(Y\)</span>) = E[ <span class="math inline">\((Y - \mu)^2\)</span> ] = <span class="math inline">\(\sum(y - \mu)^2 \times f(y),\)</span> or <strong>Mean Squared Deviation</strong>, and</p></li>
<li><p>Standard Deviation, SD(<span class="math inline">\(Y) = \sqrt{(Var(Y)}= \sqrt{E[ (Y — \mu)^2]},\)</span> or <strong>Root Mean Squared Deviation</strong></p></li>
<li><p>In <strong>French</strong>, the Standard Deviation is called <a href="https://fr.wikipedia.org/wiki/Écart_type"><strong>écart type</strong></a>.<br />
This <a href="http://www.french-linguistics.co.uk/dictionary/type.html">French-&gt;English dictionary</a> translates (the noun) <code>écart</code> as space, gap, distance between objects, interval, gap between dates, difference between numbers, opinions; à l’~ isolated, remote, out-of-the-way; à l’~ de well away from; ~ de conduite misdemeanour; ~ d’inflation inflation differential; ~ de langage strong language, bad language; ~ type standard deviation and<br />
(the adjective) <code>type</code> translates as typical, standard; lettre ~ standard letter; (Maths) écart ~ standard deviation. JH thinks this adjective better describes the meaning that ‘standard’ does. See here for the <strong>history of the term</strong> <a href="https://en.wikipedia.org/wiki/Standard_deviation#History"><code>standard</code> deviation</a>.</p></li>
</ul>
<p>Here, graphically and numerically illustrated, are three (of the many) ways to measure the variability of a random variable.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-50"></span>
<img src="statbook_files/figure-html/unnamed-chunk-50-1.png" alt="6 symmetrically distributed random variables, and 3  ways of measuring their spreads about a common mean. [After Mosteller, Rourke and Thomas. Probability with statistical applications 2nd Ed, p205] " width="864" />
<p class="caption">
Figure 12.1: 6 symmetrically distributed random variables, and 3 ways of measuring their spreads about a common mean. [After Mosteller, Rourke and Thomas. Probability with statistical applications 2nd Ed, p205]
</p>
</div>
<p>In practice, the mean absolute deviation is often quite close to the SD, and certainly easier to explain to explain to non-statisticians. However, when computig was by ahnd and laborious, it took two passes through the data to compute it, whereas, the SD could be computed in one.</p>
<ul>
<li><strong>Variance &amp; SD of number of children &lt;= 12 years</strong> in households with at least 1 such child:</li>
</ul>
<pre><code>##      n.children  devn. devn.squared proportion.of.households    product
## [1,]          1 -1.494     2.232036                    0.335 0.74773206
## [2,]          2 -0.494     0.244036                    0.245 0.05978882
## [3,]          3  0.506     0.256036                    0.180 0.04608648
## [4,]          4  1.506     2.268036                    0.126 0.28577254
## [5,]          5  2.506     6.280036                    0.072 0.45216259
## [6,]          6  3.506    12.292036                    0.031 0.38105312
## [7,]          7  4.506    20.304036                    0.009 0.18273632
## [8,]          8  5.506    30.316036                    0.002 0.06063207</code></pre>
<pre><code>## ....... Sum(products) = Mean of Squared Deviations = Variance = 2.22</code></pre>
<pre><code>## . Sq. Root of Mean of Squared Deviations = Standard Deviation = 1.5</code></pre>
<p><strong>Which is primary, Standard Deviation or Variance?</strong></p>
<ul>
<li>Although we first define variance and then take the square root to reach the SD, we should think of the SD as primary, at least for descriptive purposes (Mosteller et al. use the natural order “…standard deviation and variance (either of these measures determines the other because the variance is the square of the SD”… However, there are <em>good mathematical reasons</em> to work with variance.</li>
</ul>
</div>
<div id="some-good-reasons-for-using-variance-which-averages-the-squares-of-the-deviations-from-the-mean." class="section level3">
<h3><span class="header-section-number">12.5.2</span> Some (good) reasons for using variance, which averages the squares of the deviations from the mean.</h3>
<ul>
<li><p>ADDITIVITY<br />
The variance of the sum of two independent random variables is the sum of their variances, and even when the two variables are dependent the variability of their sum has a simple formula. SD;s dont add; their squares do. Or to quote the physicists, errors ‘<strong>Errors add in quadrature</strong>, like the lengths of the sides of Pythogoras’ traingle. It took mathematicains a long time to discover, this, and some of the blunders along the way are told in a fascinating chapter in this very readable book <a href="https://www-degruyter-com.proxy3.library.mcgill.ca/view/title/521193">The Seven Pillars of Statistical Wisdom</a>.</p></li>
<li><p>THE CENTRAL LIMIT THEOREM<br />
The limiting behavior of a random variable that is the sum of a large number of independent random variables depends on the variances of these random variables.</p></li>
<li><p>USEFUL RELATIONSHIP/SHORTCUT (especially for hand computation, ‘back when’, and in mathematical statistics, still today):<br />
<span class="math display">\[Variance(Y) = E(Y^2) - \mu^2.\]</span> <span class="math display">\[\textrm{Variance =  average  square  minus   squared average}.\]</span></p></li>
</ul>
</div>
<div id="but-for-end-users-today-." class="section level3">
<h3><span class="header-section-number">12.5.3</span> But, for end-users today ….</h3>
<p>Jerry Hill, who wrote <a href="https://www.collectionscanada.gc.ca/obj/s4/f2/dsk2/ftp02/NQ26855.pdf">this PhD thesis</a> ‘at the end of a career in medicine and epidemiology’ commuted from Ottawa and taught the 607 course a few times in the 1980s. In the course, he used to joke about the (many) mathematical statisticians who refer to a random variable having a Normal (Gaussian) distribution with standard deviation <span class="math inline">\(\sigma\)</span> as <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span>, i.e., who <strong>defined the spread of the distribution (any distribution, not just Gaussian ones) using its variance</strong>. [Note that the <code>dnorm</code>, <code>pnorm</code> <code>qnorm</code> and <code>rnorm</code> functions in <code>R</code> all use the argument <code>sd</code>, as do all applied textbooks, writing <span class="math inline">\(Y \sim N(\mu, \sigma)\)</span>]. Gerry would then go on to give a numerical example, using the commonly used metric the Total Fertility Rate (TFR) – or often simply ‘fertility rate’ – which (as per Goggle) `measures the <strong>average number of children per woman</strong>.‘The global average fertility rate is just below <strong>2.5 children per woman</strong> today’. He would them say, ‘Suppose the variation from country to country had a <strong>standard deviation of 1.2</strong>’. Then, reminding his class to use appropriate units when reporting statistical measures, he would ask that they give the appropriate units for the 2.5, the 1.2, and the <span class="math inline">\(1.2^2\)</span> = 1.44, forcing them to say the <strong>variance was 1.44 square children per square woman</strong>.</p>
</div>
<div id="example-of-variance-calculation-using-one-pass-formula" class="section level3">
<h3><span class="header-section-number">12.5.4</span> Example of Variance-calculation using one-pass formula</h3>
<p><strong>Number of children born alive</strong></p>
<p>Proportions of (currently) married women aged 45 and older who, in a 1911 census, reported this “number of children born alive to the present marriage”</p>
<pre><code>##      n.children squared.children proportion.of.women  product
## [1,]          0                0            0.243878 0.000000
## [2,]          1                1            0.047704 0.047704
## [3,]          2                4            0.058265 0.233060</code></pre>
<pre><code>##      n.children squared.children proportion.of.women  product
## [1,]         18              324            0.000561 0.181764
## [2,]         19              361            0.000306 0.110466
## [3,]         20              400            0.000051 0.020400</code></pre>
<pre><code>## Sum(product) = Mean Squared Value =  39.47 ; Mean Value =  4.83</code></pre>
<pre><code>## Variance =  39.47  minus the square of 4.83  =  16.11</code></pre>
<pre><code>## Thus, Standard Deviation = sqrt( 16.11 ) =  4.01</code></pre>
</div>
</div>
<div id="variance-and-sd-of-a-function-of-a-random-variable" class="section level2">
<h2><span class="header-section-number">12.6</span> Variance and SD of a FUNCTION of a random variable</h2>
<p>If we go back to some of the <strong>examples listed above</strong> we can reason out what the law must be</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> = Noon Temperature (C) in Mtl on a randomly selected day of the year;<br />
… <span class="math inline">\(g(Y)\)</span> = Temperature (F) = 32 + (9/5) <span class="math inline">\(Y\)</span><br />
If the SD was say 10 C, then surely the SD is 18 C. After all, Temperature is Temperature, so you are not – as some textbooks are inclined to say – ‘changing’ the fundamental variable, but rather changing the scale of the temperature variable. So, the SD scales up by 9/5, and so, being <strong>an average square</strong>, the <strong>variance scales up by <span class="math inline">\((9/5)^2\)</span>.</strong> If you are going the other way, from the larger F scale to the smaller C scale, the scalings are (5/9) for the SD, and <span class="math inline">\((5/9)^2\)</span> for the variance. More generally,<br />
<span class="math display">\[SD[constant  \times RV] = constant  \times SD[RV]\]</span><br />
<span class="math display">\[Var[constant  \times RV] = constant^2  \times Var[RV]\]</span> .<br />
This example also shows another law related to spread: shifting left or by a constant amount (eg. suppose the conversion was <span class="math inline">\(F = 10 + (9/5)C\)</span> instead of was <span class="math inline">\(F = 32 + (9/5)C,\)</span> it would not alter the spread. Thus,<br />
.<br />
<span class="math display">\[SD[ RV + constant ] = SD[RV]\]</span><br />
<span class="math display">\[Var[ RV + constant ] = Var[RV]\]</span></p></li>
<li><p><span class="math inline">\(Y\)</span> = Weight in Kg (or Height in cm) of a randomly selected person;<br />
… g(Y) = Weight in Kg (or Height in inches)<br />
.<br />
This involves just a scaling, with no shift. So if the SD were 10 Kg, it would be 22 lbs, and the variances in the 2 scales would be <span class="math inline">\(100 \ Kg^2\)</span> and <span class="math inline">\(484 \ lb^2\)</span>.</p></li>
<li><p><span class="math inline">\(Y\)</span> = <strong>Years of publication</strong> of all the books in the McGill Library, with Years measured from AD (Anno Dominini, ‘in the year of the Lord’).<br />
.<br />
The SD would be the same if we measured the Year from 1439 AD (<span class="math inline">\(Y&#39; = Y\)</span> - 1439) when Gutenberg was the first European to use movable type, or from 1492 AD when Christopher Columbus reached North America.<br />
.<br />
What if, instead, we calculated the <strong>age of each book in the year 2020</strong>, i.e. as <span class="math inline">\(Y&#39; = 2020 - Y\)</span>?<br />
.<br />
The <strong>scale would now be reversed</strong>. Instead of being at the extreme left, the older books would not be at the right hand of the scale, and vice versa. But the spread would still be the same, even though the shape of the new distribution would be the miorrow inage of the old one.<br />
.<br />
What if we measured <strong>age in decades</strong>, i.e., <span class="math inline">\(Y&#39; = \frac{2020 - Y}{10}\)</span> or centuries i.e., <span class="math inline">\(Y&#39; = \frac{2020 - Y}{100}\)</span>?<br />
.<br />
The SDs would be scaled down by 10 and by 100, and the variances by <span class="math inline">\(10^2\)</span> and <span class="math inline">\(100^2.\)</span></p></li>
<li><p><span class="math inline">\(Y\)</span> = Ocean Depth ot a randomly chosen location, measured in metres. If the <strong>‘origin’ is the ocean floor</strong>, all depths will be positive; if it is <strong>the surface of the ocean</strong>, they will all be negative. The <strong>spread, the SD and the Variance stay the same</strong>, but the <strong>shapes of the distributions are mirror inages</strong> of each other.</p></li>
<li><p><span class="math inline">\(Y\)</span> = diameter of a randomly chosen sphere;<br />
… <span class="math inline">\(g(Y)\)</span> = Volume of sphere = <span class="math inline">\(\frac{\pi}{6} Y^3.\)</span><br />
.<br />
This one is more complicated – as you might have guessed from just trying to compute the expectation. The fact that the scaling is different at different values of <span class="math inline">\(Y\)</span> complicated matters. There is an appriximate formula, that depends on the scaling at some ‘representative’ value of <span class="math inline">\(Y\)</span>, typically the mean or mode. For more on this, see the examples in this <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/jh_dt_tas_2006.pdf">expository piece</a>.</p></li>
<li><p><span class="math inline">\(Y\)</span> = fuel consumption, in liters/100km, of a randomly selected make of car;<br />
<span class="math inline">\(g(Y)\)</span> = miles per gallon or Km per liter (reciprocal)<br />
.<br />
There is no exact closed form, but the approximation works well because the values are well away from zero, and not too spread out – unless you allow Hummers!</p></li>
</ul>
</div>
<div id="sumsmeansdifferences-of-rvs" class="section level2">
<h2><span class="header-section-number">12.7</span> Sums/means/differences of RVs</h2>
<div id="a-sum-of-2-or-n" class="section level3">
<h3><span class="header-section-number">12.7.1</span> A sum (of 2 or <span class="math inline">\(n\)</span>)</h3>
<p>To keep it simple, and to allow us to see what is going on, Let’s consider two very simple random variables (RV’s), each taking just 2 values, and with equal probabilities. [You can check for yourself later that the same law applies to random variables that take on more than 2 values, and with uneven probability distributions.] The key is that the 2 variables are <em>independent</em> of each other.</p>
<p>The next panels show the two RV’s. <span class="math inline">\(RV_1\)</span>  (6  or  12)  and  <span class="math inline">\(RV_2\)</span>  (8  or  16)</p>
<p><span class="math display">\[Sum = RV_1 \ (6 \ or \ 12) \ + \ RV_2 \ (8 \ or \ 16) \ = \ 14 \ or \ 20 \ or \ 22 \ or \ 28.\]</span><br />
We will come back much later to the choices of the specific values for each of the random variables; for our purposes here, the main point is that both equally-likely values are an even number apart, so the SDs are integers, and all calculations involve integers. Note that an RV with equally likely values 1 and 7 (or 3 and 9, or -1 and + 5) has the same SD as the RV with equally likely values 6 and 12: it is the <em>distance between them</em> that determines the SD. Note also, that with half of the values at one extreme and hald at teh other, all values are exactly one SD from the mean. [If you want to give them some meaning so that you can relate to them, thing of them as the durations of 2 stages in an industrial process, or a service (such as processing online orders, and arrranging shipping and delivery, or waiting for a mammogram, and then a biopsy).]</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-54"></span>
<img src="statbook_files/figure-html/unnamed-chunk-54-1.png" alt="Variance of RV1" width="864" />
<p class="caption">
Figure 12.2: Variance of RV1
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-55"></span>
<img src="statbook_files/figure-html/unnamed-chunk-55-1.png" alt="Variance of RV2" width="864" />
<p class="caption">
Figure 12.3: Variance of RV2
</p>
</div>
<p>We now imagine taking a random value of <span class="math inline">\(RV_1\)</span> and a random value of <span class="math inline">\(RV_2\)</span> and summing them. There are 4 possible sums, and in this case they are all distinct (this isn’y always be the case, but the values of the two RV’s in <em>this example</em> were deliberately chosen to make them all distict, and to avoid grouping RV combinations with the same sum.) A probability tree (next panel) helps to see the 4 possibilities, but here we add an extra feature: we let the lengths of the branches denote the values of the RVs. (This ‘stacking’ of RV’s can have practical advantages too: when Francis Galton wanted to quickly and precisely compute the mean diameter of a large sample of peas, he lined them up in a long groove, and just measured the total width of the entire sample, then divided it by the <span class="math inline">\(n\)</span>.)</p>
<p>In our example, the 4 equally likely sums are 14, 20, 22 and 28, and their mean is <span class="math inline">\(\frac{14+20+22+28}{4} = 21.\)</span> That the mean (expected value) of the sum equals the sum of the 2 individual means or expected values, is hardly surprising [ – and it is even true if the 2 RV’s were not independent.] The 4 equally likely deviations from this 21 are -7, -1, +1, and +7, and so, <strong>from 1st principles</strong>, the <strong>Variance of the sum of the 2 RVs is</strong> <span class="math display">\[\frac{(-7)^2 + (-1)2 +  (+1)^2 +  (+7)^2}{4} = \frac{100}{4} = 25.\]</span><br />
Thus, the <strong>SD of the sum of the 2 RVs is</strong> <span class="math inline">\(\sqrt{25}\)</span> = 5.</p>
<p>Fortunately, we don’t have to go back to 1st principles <strong>to calculate the SD of the sum of 2 RVs – but we do NOT do so by adding the 2 SDs</strong>. IT IS THE VARIANCES THAT ADD!</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-57"></span>
<img src="statbook_files/figure-html/unnamed-chunk-57-1.png" alt="Variance (and thus SD) of the SUM of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the sum from its expectation is decomposed into its 2 components. This, (by the expansion rule for (a+b) squared that you learned in high school), each squared deviation becomes a sum of 2 squares, and a 'cross-product'. But the 4 cross-products cancel each other, and you are left with the sums of two squares, the original 3-squared and the  original 4-squared, i.e. the variances of RV1 and RV2." width="864" />
<p class="caption">
Figure 12.4: Variance (and thus SD) of the SUM of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the sum from its expectation is decomposed into its 2 components. This, (by the expansion rule for (a+b) squared that you learned in high school), each squared deviation becomes a sum of 2 squares, and a ‘cross-product’. But the 4 cross-products cancel each other, and you are left with the sums of two squares, the original 3-squared and the original 4-squared, i.e. the variances of RV1 and RV2.
</p>
</div>
<p>Whereas one numerical example doesn’t prove the ‘the variances of a sum of independent RVs is the sum of their variances’ rule, you can check out other more complicated examples with more values, and uneven distributions – or use simulation – and satisfy yourself that it is a general rule. Indeed, the formal mathematical statistics ‘proof’ uses the exact same method as that used in the panel, except that it replaces each number by a symbol! (By the way, when students from other disciplines who claim to have taken statistics courses ask JH for permission to enrol in the math-stat based course bios601, one of his standard tests is to give then this problem: ‘Let RV1 and RV2 be two random variables. From 1st principles, derive the formula for the variance of RV1+RV2’.)</p>
<p>So we can sum up the forgoing numerical example by saying:</p>
<p><span class="math display">\[ SD_1 = 3; SD_2 =4;\  but \ SD[Sum] \ \ne \ 3 + 4. \ \ \  SD[Sum] \ = \ \sqrt{3^2 + 4^2} = 5.\]</span><br />
<strong>BOTTOM LINE : 3 + 4 is not 7! it’s 5! </strong> Or, as physicists say, <strong>Uncertainties ADD IN QUADRATURE’</strong> (that’s the math word for the square root of the sum of squares). Now you can see why mathematical statisticians like to work with squared SD’s. And you can see why we chose the ‘nice’ variance values 3 and 4. Just like for the right-angled triangles in Pythagoras’ theorem, where the length of the hypotenuese is the square root of the squares of the elngths of the sides, so it is also with othogonal or independent random variables: the SD of their sum is the square root of the sum of the squared SD’s of the individual RVs.</p>
<p><strong>And of course, since the theorem works for the sum of 2 independent RV’s, it also works for the sum of 3, and for the sum of <span class="math inline">\(n\)</span> such RVs.</strong></p>
</div>
<div id="measurement-errors" class="section level3">
<h3><span class="header-section-number">12.7.2</span> Measurement Errors</h3>
<p>These are an important issue in the non-experimental – and even the experimental – sciences. The simplest case (called the <strong>‘classical’ error model</strong>) is where the errors are independent of the true value, so that the <strong>variance in the observed (error-containing) values is the sum of the variance of the ‘true’ (errorless) values, and the variance of the errors.</strong></p>
<p>Even though many people think that random measurement errors <strong>cancel out</strong>, especially in large datasets, <strong>they do not</strong>. Even when they affect the <span class="math inline">\(Y\)</span> on the left side side of a regression model, they add ‘noise’ to the slope estimates. But when they affect an <span class="math inline">\(X\)</span> on the right hand side of a regression model, or even a correlation, their effects are more insidious. See for example, pages 19-21 of <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/hw_measurement2019.pdf#page=19">these Notes</a> and exercises 6, 8, 9, 18 and 21 that follow.</p>
<p>The other measurement error model, called ‘Berkson’ error, described <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Surveys/EffectsXerrorsNotesFromALM.pdf">here:</a>, is less common, and has less nasty effects.</p>
<p>The following diagram shows the classical error model, and one of the important metrics to measure the extent of the error, namely the <strong>intra-class correlation coefficient</strong>. The ‘ICC’ is relevant no matter whether the variable is on the left or right had side of the regression model. Even though we named the random variable <span class="math inline">\(Y\)</span>, it does not mean that measurement issues apply only to <span class="math inline">\(Y\)</span> variables. In fact, errors in <span class="math inline">\(X\)</span> variables have nastier effects. We chose the name <span class="math inline">\(Y\)</span> because we don’t treat an errorless <span class="math inline">\(X\)</span> in a regression model as a random variable, and we are seldom interested in inferences regarding it!</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-58"></span>
<img src="statbook_files/figure-html/unnamed-chunk-58-1.png" alt="Random Measurement Error (E) added to a Random Variable. On the left is the distribution of a Random Variable Y, with each stickfigure representing a very large number of individuals. On the right is the distribution of the Random Variable Y', where Y' = Y + E, and E is independent of Y, and has a 2-point distribution, namely -0.5 and +0.5, with equal probabilities. (Here, the provenance/origin of each Y' value is shown by its colour, but in practice we would not have that luxury of knowing what the 'true' [errorless] value was). This is the variation we get to observe/measure. Of the variance of 1.93, some  1.68 of it is 'real'/'genuine', and the remainder, 0.25 is measurement error. The genuine variance of interest, 1.68, expressed as a proportion of the observable variance 1.93, namely 1.68/(1.68 + 0.25) = 0.87 or 87%. The proportion that is real is called the INTRA-CLASS CORRELATION (ICC) or intra-class correlation coefficient, and is an important indicator of the quality of the measurement of Y." width="864" />
<p class="caption">
Figure 12.5: Random Measurement Error (E) added to a Random Variable. On the left is the distribution of a Random Variable Y, with each stickfigure representing a very large number of individuals. On the right is the distribution of the Random Variable Y’, where Y’ = Y + E, and E is independent of Y, and has a 2-point distribution, namely -0.5 and +0.5, with equal probabilities. (Here, the provenance/origin of each Y’ value is shown by its colour, but in practice we would not have that luxury of knowing what the ‘true’ [errorless] value was). This is the variation we get to observe/measure. Of the variance of 1.93, some 1.68 of it is ‘real’/‘genuine’, and the remainder, 0.25 is measurement error. The genuine variance of interest, 1.68, expressed as a proportion of the observable variance 1.93, namely 1.68/(1.68 + 0.25) = 0.87 or 87%. The proportion that is real is called the INTRA-CLASS CORRELATION (ICC) or intra-class correlation coefficient, and is an important indicator of the quality of the measurement of Y.
</p>
</div>
<p><strong>The concept of an ICC depends on the law that ‘variances add.’</strong></p>
</div>
<div id="mean-of-2-or-n-rvs" class="section level3">
<h3><span class="header-section-number">12.7.3</span> Mean (of 2 or <span class="math inline">\(n\)</span> RVs)</h3>
<p>AND, if we know how to compute the SD of a SUM of <span class="math inline">\(n\)</span> independent RV’s, we then automatically know how to compute the SD of a MEAN of <span class="math inline">\(n\)</span> independent RV’s. Remember back to an assigment on the SD of a set of temperatures measured in the larger degrees F scale: then the SD of the same set of temperatures measured in the smaller degrees C scale is just 5/9ths of the one in the F scale.</p>
<p>Going from a <strong>sum</strong> of <span class="math inline">\(n\)</span> RVs to a <strong>mean</strong> of <span class="math inline">\(n\)</span> RVs involves going to a scale that is (1/n)-th the spread of the sum scale.</p>
<p>So, in the above example, with <span class="math inline">\(n\)</span> = 2, the SD of the mean of the 2 RVs is (1/2) the SD of the sum, i.e.,</p>
<p><span class="math display">\[ SD\bigg(\frac{RV_1 +RV_2}{2}\bigg) = \frac{SD(RV_1+RV_2)}{2} = \frac{5}{2}.\]</span></p>
<p><strong>SPECIAL CASE</strong> (quite common) where <strong>SDs are identical</strong>:</p>
<p>Up to now, to keep things general, we used <span class="math inline">\(n\)</span> non-identical – but independent – random variables. If we consider the Variance and the sum of <span class="math inline">\(n\)</span> IDENTICAL – and independent – random variables, so the the <span class="math inline">\(n\)</span> Variances (each abbreviated to Var) are all equal, the laws simplify</p>
<p>First, since the variances add, we have that</p>
<p><span class="math display">\[ Variance(RV_1 + RV_2 + \dots + RV_n) = Var_1 + Var_2 + \dots + Var_n = n \times \ each \ Var.\]</span></p>
<p>and so, taking square roots,</p>
<p><span class="math display">\[ SD( \ RV_1 + RV_2 + \dots + RV_n \ ) = \sqrt{ \ n \times \ each \ Var} = \sqrt{n} \ \times \ each \ SD\]</span></p>
<p>When we go from <strong>sum</strong> of <span class="math inline">\(n\)</span> IDENTICAL independent RVs to a <strong>mean</strong> of <span class="math inline">\(n\)</span> IDENTICAL RVs, we go to a scale that is (1/n)-th the spread of the sum scale. So, again, just as when we went from the larger degrees F scale to the smaller degrees C scale, we have</p>
<p><span class="math display">\[ SD\bigg(\frac{RV_1 + RV_2 + \dots + RV_n}{n}\bigg) = \frac{\sqrt{n} \ \times \ each \ SD}{n} = \frac{common \ SD}{\sqrt{n}} .\]</span> Sometimes, we will need to work with variances. When we do, we use this law:</p>
<p>*<span class="math display">\[ VAR\bigg(\frac{RV_1 + RV_2 + \dots + RV_n}{n}\bigg) = \frac{common \ VAR}{n} .\]</span></p>
<p><strong>EXAMPLE</strong> Lengths of words in a book (“book A”)</p>
<p>The number of dashes in of each row in the first panel is the number of letters in a randomly selected word from the book: dashes are for better visibility. The words (rows) are sorted by length, form shortest to longest. One cannot judge the full distribution just from this limited set, but (even though shape doesn’t matter much in the big scheme of things) you get a sense of its shape. In the entire book, the mean word length is about 4.5 letters, and the SD is 2.4 letters. (The fact that the distance between the minimum word length (1 letter) and the mean word length is less than 2 SDs hints that the full distribution has a longe right tail.)</p>
<p>In each row in the coloured panels, some 4 (or 9) randomly sampled words are (like Galton’s peas) pushed right up against each other, without spaces, and shown in a mono-spaced font, so that where the ‘line’ ends indicates the total number of letters in the 4 (9) words. Since this small number of rows (possibilies) is too small to give a good sense of the sampling distribution, the smooth purple histograms were calculated exactly.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-59"></span>
<img src="statbook_files/figure-html/unnamed-chunk-59-1.png" alt="Illustrations of SD's of Sums and Means of n = 1, 4 and 9 independent and identically distributed random variables. Each RV is the length of a randomly selected word from a certain book. [Below, we will compare the mean word length in this book with the mean in abook by a competitor]. The distributions in purple were computed theoretically, using convolutions. Each row shows 1 'realization' of each of the  n random variables, with each word in a different color. The rows are sorted according to the values of the total [or mean] numbers of letters (chars) in the sample of n words. In the panels where n is 4 or 9, the leftmost n-1 characters of the n concatenated words are cropped, but the total/mean length  is correct. The top panel lists the 'per word' variation of all of the words in the book, and its SD, sometimes called the 'unit' variability. You can also think of the length of each unit as the mean of a sample of size n = 1. The second panel shows that to reduce the sampling variation (the SD) of the mean by half, one needs to quadruple the n. The third panel shows that to reduce the sampling variation (the SD) to 1/3, one needs to multiply the n by 9. Note, in passing, that at $n$ = 9, via the Central Limit Theorem, and the fact that the original distribution is 'CLT friendly' (the mode is not at either extreme, and the tails don't extend indefinitely), the shape of the sampling distribution is already close to Gaussian." width="864" />
<p class="caption">
Figure 12.6: Illustrations of SD’s of Sums and Means of n = 1, 4 and 9 independent and identically distributed random variables. Each RV is the length of a randomly selected word from a certain book. [Below, we will compare the mean word length in this book with the mean in abook by a competitor]. The distributions in purple were computed theoretically, using convolutions. Each row shows 1 ‘realization’ of each of the n random variables, with each word in a different color. The rows are sorted according to the values of the total [or mean] numbers of letters (chars) in the sample of n words. In the panels where n is 4 or 9, the leftmost n-1 characters of the n concatenated words are cropped, but the total/mean length is correct. The top panel lists the ‘per word’ variation of all of the words in the book, and its SD, sometimes called the ‘unit’ variability. You can also think of the length of each unit as the mean of a sample of size n = 1. The second panel shows that to reduce the sampling variation (the SD) of the mean by half, one needs to quadruple the n. The third panel shows that to reduce the sampling variation (the SD) to 1/3, one needs to multiply the n by 9. Note, in passing, that at <span class="math inline">\(n\)</span> = 9, via the Central Limit Theorem, and the fact that the original distribution is ‘CLT friendly’ (the mode is not at either extreme, and the tails don’t extend indefinitely), the shape of the sampling distribution is already close to Gaussian.
</p>
</div>
<p>As you would have expected, the purple <strong>sampling distributions narrow with increasing sample size</strong>, but the narrowing is <strong>not by a factor of <span class="math inline">\(n\)</span>,</strong> but by a <strong>factor of <span class="math inline">\(\sqrt{n}\)</span>.</strong> All the billions of <em>possible means</em> of samples of size <span class="math inline">\(n\)</span> = 4 <em>would have</em> a SD of 2.4/<span class="math inline">\(\sqrt{4}\)</span> = 2.4/2 = 1.2. The <em>possible means</em> of samples of size <span class="math inline">\(n\)</span> = 9 <em>would have</em> a SD of 2.4/<span class="math inline">\(\sqrt{9}\)</span> = 2.4/3 = 0.8. <strong>Note the careful choice of the words</strong> in <em>italics</em>: in reality, you will only observe <strong>one</strong> of the billions of possibilities, so the sampling distribution is <em>imaginary</em> and thus the SD is also <em>imaginary</em> and so the SD of the <strong>conceptual</strong> sampling distribution is (<em>would be</em>) an <em>imaginary</em> 1.2 or 0.8. The only reason we are able to show the purple distributions is because of the laws of statistics, applied to all the words in the book, so we know the mean and the unit SD.</p>
</div>
<div id="difference-of-2-rvs" class="section level3">
<h3><span class="header-section-number">12.7.4</span> Difference of 2 RVs</h3>
<p>To keep it simple, let’s consider the two very simple random variables (RV’s), each taking just 2 values, and with equal probabilities, and independent of each other. But suppose now that we are interested in their difference</p>
<p><span class="math display">\[Difference = RV_1 \ (6 \ or \ 12) \ - \ RV_2 \ (8 \ or \ 16) \ = \ -10 \ or \ -4 \ or \ -2 \ or \ +4.\]</span></p>
<p>Now, imagine taking a random value of <span class="math inline">\(RV_1\)</span> and subtracting from it a random value of <span class="math inline">\(RV_2\)</span>. There are 4 possible differences, and in this deliberately constricted example, they are all distinct. A probability tree (next panel) helps to see the 4 possibilities, and the lengths of the branches denote the values of the RVs.</p>
<p>The 4 equally likely differences are -10, -4, -2 and +4, and so their mean is <span class="math inline">\(\frac{-10 -4 -2 +4}{4} = -3.\)</span> That the mean (expected value) of the difference equals the difference of the 2 individual means or expected values, is hardly surprising [ – and it is even true if the 2 RV’s were not independent.] The 4 equally likely deviations from this -3 are -7, -1, +1, and +7, and so, <strong>from 1st principles</strong>, the <strong>Variance of the difference of the 2 RVs is</strong> <span class="math display">\[\frac{(-7)^2 + (-1)2 +  (+1)^2 +  (+7)^2}{4} = \frac{100}{4} = 25.\]</span><br />
Thus, the <strong>SD of the difference of the 2 RVs is</strong> <span class="math inline">\(\sqrt{25}\)</span> = 5.</p>
<p>Fortunately, we don’t have to go back to 1st principles <strong>to calculate the SD of the differences of 2 RVs – but we do NOT do so by adding the 2 SDs</strong>. IT IS THE VARIANCES THAT ADD!</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-62"></span>
<img src="statbook_files/figure-html/unnamed-chunk-62-1.png" alt="Variance (and thus SD) of the DIFFERENCE, RV1 - RV2, of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the difference from its expectation is decomposed into its 2 components. Each each squared deviation becomes a sum of 2 squares, and a 'cross-product'. But the 4 cross-products cancel each other, and you are left with the SUMS of two squares, the original 3-squared and the  original 4-squared, i.e. the variances of RV1 and RV2." width="864" />
<p class="caption">
Figure 12.7: Variance (and thus SD) of the DIFFERENCE, RV1 - RV2, of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the difference from its expectation is decomposed into its 2 components. Each each squared deviation becomes a sum of 2 squares, and a ‘cross-product’. But the 4 cross-products cancel each other, and you are left with the SUMS of two squares, the original 3-squared and the original 4-squared, i.e. the variances of RV1 and RV2.
</p>
</div>
<p>Again one numerical example doesn’t prove the ‘the variances of a difference of two independent RVs is the sum of their variances’ rule, but you can check out other more complicated examples with more values, and uneven distributions – or use simulation – and satisfy yourself that it is a general rule.</p>
<p>So we can sum up the forgoing numerical example by saying:</p>
<p><span class="math display">\[ SD_1 = 3; SD_2 =4;\  but \ SD[Difference] \ \ne \ 3 + 4. \ \ \  SD[Difference] \ = \ \sqrt{3^2 + 4^2} = 5.\]</span><br />
It turns out that, from what we already knew about the sum of 2 independent RVs, we have anticipated this law. We didn’t need to go through all the formulae from scratch again. The reason had to do with the ‘mirror image’ distributions, such as the depths of the ocean, we saw above. The spread (SD, or variance) is the same, whether one writes/reads/computes from left to right, or right to left! In other words, the variance of the random variable <span class="math inline">\(-RV_2\)</span> is the same as that of the random variable <span class="math inline">\(RV_2.\)</span> So, by writing <span class="math inline">\(RV_1 - RV_2\)</span> as a sum, and using the law for the variance of a sum, we arrive at <span class="math display">\[Var[RV_1 - RV_2] = Var[RV_1 + (-RV_2)] = Var[RV_1] + Var[(-RV_2)] = Var[RV_1] + Var[RV_2].\]</span></p>
<p><strong>EXAMPLE</strong> Difference in mean length of words in books A (in blue) and B (in red)</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-63"></span>
<img src="statbook_files/figure-html/unnamed-chunk-63-1.png" alt="Differences in  mean lengths of  n randomly selected words from each of 2 books. The (sampling) distributions were computed theoretically, using convolutions." width="864" />
<p class="caption">
Figure 12.8: Differences in mean lengths of n randomly selected words from each of 2 books. The (sampling) distributions were computed theoretically, using convolutions.
</p>
</div>
</div>
</div>
<div id="linear-combinations-of-rvs-regression-slopes" class="section level2">
<h2><span class="header-section-number">12.8</span> Linear combinations of RVs (regression slopes)</h2>
<p>In <em>non-experimental</em> research especially, the focus is typically on a fitted regression slope/coefficient, rather that on the simple difference <span class="math inline">\(\bar{y}_1\)</span> - <span class="math inline">\(\bar{y}_0\)</span> between the means of the <span class="math inline">\(y\)</span>s observed at each of two investigator-chosen values (<span class="math inline">\(X=1\)</span> and <span class="math inline">\(X=0\)</span>) of the determinant (<span class="math inline">\(X\)</span>) being studied.</p>
<p>Even if the estimator does not have a closed form, the fitted slope(s)/coefficient(s) are linear combinations of the <span class="math inline">\(y\)</span>’s and the <span class="math inline">\(x\)</span>’s. Thus, since each of the <span class="math inline">\(n\)</span> <span class="math inline">\(y\)</span>’s contains a random element, the slope (<span class="math inline">\(\hat{\beta}\)</span>) is an <span class="math inline">\(x\)</span>-based linear combination of <span class="math inline">\(n\)</span> random variables.</p>
<p>Thus one can view all variances (and thus all standard errors) in a unified way, and not have to learn separate laws for separate chapters. To see how this unified view avoids the typical ‘silo’ approach to statistical tecnniques, see <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/UniversalSampleSize.pdf">Sample Size, Precision and Power Calculations: A Unified Approach</a>. [Software developers who thrive on separate ‘niche’ markets are threatened by this parsimonious approach, just as are those who write 800-page textbooks with separate chapters for t-tests,l proportions, regression, multiple regression, logiostic regression, Cox regression, survival analysis,etc.]</p>
<p>In the past, when first introduced to simple linear regression, it was common to learn the estimator formula and the Variance formula by heart, and use them to compute the fitted slope and the the standard error for a fitted slope by hand, <span class="math display">\[\hat{\beta} = \frac{\sum (y-\bar{y})(x-\bar{x})}{ \sum (x-\bar{x})^2} \ ; \ \ Var[ \hat{\beta} ] = \frac{\sigma_e^2}{ \sum (x-\bar{x})^2} \ ; \ SE[ \hat{\beta} ] \ = \sqrt{Var[ \hat{\beta} ]} \ .\]</span> In the variance formula, <span class="math inline">\(\sigma_e^2\)</span> is the variance in the ‘errors’ in the <span class="math inline">\(y\)</span>’s. In practice, we have to estimate this quantirty, but in our example, for disdactic purposes, we will pretend to ‘know’ its value.</p>
<p>Sadly, <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/SimpleMultipleLinearRegressionSampleSize.pdf">these formidable formulaa hide what is going on</a>.</p>
<p>To truly understand what is going on, lets consider a <strong>very simple example</strong> where we can <em>see</em> what is happening. A student from a country that uses the Fahrenheit (F) system moves to Montreal, and wishes to know how to translate the outside temperature, expressed as the number of degrees Celsius (C) and shown in the Metro, and heard on radio stations, back into the F scale (s)he is familiar with. The student knows that the conversion is of the form F = <span class="math inline">\(\alpha\)</span> + <span class="math inline">\(\beta \times C\)</span> but, rather than look them up on Google, decides to estimate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> from pairs of (C,F) readings, taking the C readings directly from the Metro screen, and the F ones from his/her own portable thermometer. Suppose that the C readings are displayed to 1 decimal place, but that the F thermometer only displays the F to the nearest integer.</p>
<p>Now, knowing that one it just takes 2 data points to draw a line, the student takes F measurements on two occasions, once when the displayed temperature is 12.5 C and one when it is 17.5 C. (The student didn’t know that when the real temperature was very close to xx.5 F, it had a 50% chance of being rounded up to xx+1 F, thereby creating an error of +0.5 F, and a 50% chance of being rounded down to xx-1 F, and producing an error of -0.5 F. Thus, the variance of each error is <span class="math inline">\((-0.5)^2 \times (1/2)\)</span> + <span class="math inline">\((+0.5)^2 \times (1/2)\)</span> = <span class="math inline">\(0.5^2,\)</span> and the SD is 0.5. (In the computer exercises, we will treat a broader type of random errors in the F readings).</p>
<p>Given that these two C settings correspond to 54.5 F and 63.5 F, but that the true temperature may be slightly on one side or the other of thse two values, what are the possible <span class="math inline">\(\beta\)</span> estimates? And, how variable would they be?</p>
<p>The 4 possibilities, shown as the slopes of the 4 fitted lines shown in black below, are (64-55)/5, (64-54)/5, (63-55)/5, and (63-54)/5, or, when sorted, 1.6, 1.8, 1.8 and 2.0 degrees F per degree C, each with probability 1/4, so that the variance of the equally likely slopes is <span class="math display">\[Var[ \hat{\beta} ] =  \frac{(1.6 - 1.8)^2 + (1.8 - 1.8)^2  + (1.8 - 1.8)^2 + (2.0 - 1.8)^2}{4} = \frac{1}{50} \ =  \ 0.02,\]</span> and the SE is <span class="math inline">\(\sqrt{0.02} = 0.14\)</span> degrees F per degree C.</p>
<p>Also shown in the diagram is the <strong>‘anatomy’ of the ‘random slope’</strong>. The possible slopes are displayed as a single expression in which the 2 random elements (i.e the 2 random variables, or the two ‘errors’ e<span class="math inline">\(_1\)</span> and e<span class="math inline">\(_2\)</span>) are isolated. The random slope is in the form of a constant (9/5) plus another constant (1/5) times the difference of two independent random errors. Applying all of the variance rules above, we have that <span class="math display">\[Var[random  \ slope] = (1/5)^2 \times ( Var[e_1] + Var[e_2])  \ = \ \frac{1}{50} \ .\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-64"></span>
<img src="statbook_files/figure-html/unnamed-chunk-64-1.png" alt="The 4  lines (in black) fitted to the 4 possible and equally likely pairs of data points (The  &lt;unknown to us&gt; 'true' values are shown in blue). By algebraicly isolating the contributions of the 2 random errors in F to the variation in the 4 slopes, the variance of the (random) slopes is easily computed using the laws for the variance of a combination of 2 independent random variables." width="864" />
<p class="caption">
Figure 12.9: The 4 lines (in black) fitted to the 4 possible and equally likely pairs of data points (The <unknown to us> ‘true’ values are shown in blue). By algebraicly isolating the contributions of the 2 random errors in F to the variation in the 4 slopes, the variance of the (random) slopes is easily computed using the laws for the variance of a combination of 2 independent random variables.
</p>
</div>
<p>The important point of this simple regression example is that even though in practice there would be many more data points, the principle/law used to calculate the sampling variation of a slope based on any number of datapoints remains the same: the fitted slope is still an <span class="math inline">\(x\)</span>-based linear combination of <span class="math inline">\(y\)</span>’s, (in this case, a closed form combination) and thus an <span class="math inline">\(x\)</span>-based linear combination of random errors – or more broadly of random deviations from the <span class="math inline">\(x\)</span>-conditional means of the ‘<span class="math inline">\(Y\)</span>’ variables. We will return later to all of the factors that influence the narrowness/width of sampling distributions generally, but you can maybe already see from the ‘algebraicly isolated’ representation of the slope that the more datapoints – and the wider apart they are on the <span class="math inline">\(x\)</span> axis and the smaller the magnitudes of the errors — the more reproducible the slope. The influence of this latter factors is less evident in the textbook formula for the Variance and the SE. This little exercise (next) should help you figure out how the factors come into it. <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/UniversalSampleSize.pdf">This piece</a> also focuses on these isssues in a transparent way. See the exercise on this.</p>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">12.9</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li>Refer to the <strong>fictitious cohort</strong> (shown above), constructed from the 1990 Quebec mortality rates.</li>
</ol>
<ul>
<li>Use 1st principles (together with <code>R</code>) to calculate the standard deviation of the longevity of the male cohort. Do so in two ways, using (a) the definition (b) the ‘shortcut’.</li>
<li>For <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/Hanley_Article_Galton_Data.pdf#page=7">human ‘computers’</a> back in the days before there were ‘electronic’ computers, what is the advantage of the shortcut?</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Suppose you get into the <strong>life insurance</strong> business in a small way, just taking on one client. The client pays you a premium of $100 at the beginning of each year for 5 years. If the client dies within the next 5 years, you will pay client’s estate $20,000. Thus, at the end of 5 years, your possible earnings from this single client, along with the associated (actuarily-based) probablities are:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">possible.earnings =<span class="st"> </span><span class="kw">c</span>( <span class="kw">seq</span>(<span class="op">-</span><span class="dv">19900</span>,<span class="op">-</span><span class="dv">19500</span>,<span class="dv">100</span>), <span class="dv">500</span> ) 
probability =<span class="st"> </span><span class="kw">c</span>(<span class="dv">183</span>,<span class="dv">186</span>,<span class="dv">189</span>,<span class="dv">191</span>,<span class="dv">193</span>,<span class="dv">99058</span>)<span class="op">/</span><span class="dv">100000</span> 

<span class="kw">cbind</span>(possible.earnings,probability)</code></pre></div>
<pre><code>##      possible.earnings probability
## [1,]            -19900     0.00183
## [2,]            -19800     0.00186
## [3,]            -19700     0.00189
## [4,]            -19600     0.00191
## [5,]            -19500     0.00193
## [6,]               500     0.99058</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>continued</li>
</ol>
<ul>
<li>Compute the expected earnings</li>
<li>Compute the variance (and thus the SD) of the possible earnings (a) using the definition (b) using the computational shortcut</li>
<li>Compute the ‘risk’, the SD as a percentage of the mean, as do investors ranking how risky various stocks are.</li>
<li>In statistics, and especially in applied statistics, what is the name for the SD as a percentage of the mean?</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>Errors caused by rounding</strong>. Suppose one has to analyze a large number of 3 digit numbers. To make the job easier, one rounds each number to the nearest 10, e.g.,<br />
<code>460 &lt;-- 460 461 462 463 464 ; 465 466 467 468 469 --&gt; 470.</code><br />
If the ending numbers of the unrounded data were uniformly distributed (each ending digit has a probability of 1/10), calculate:</li>
</ol>
<ul>
<li>the average error per (rounded) number</li>
<li>the average absolute error per (rounded) number</li>
<li>the square root of the average squared error per (rounded) number [‘root mean squared error’, or ‘RMSE’ for short]</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><strong>Correcting for guessing</strong> on multiple choice exams.<br />
Suppose one wishes to estimate via a multiple choice examination [with <span class="math inline">\(k\)</span> answers to choose from for each question], what proportion <span class="math inline">\(\pi\)</span> of questions a student <strong>knows</strong> the answer to (excuse the dangling preposition!). Imagine that <span class="math inline">\(\pi\)</span> refers to the N (&gt;&gt; n) questions in the much larger bank of questions from which the <span class="math inline">\(n\)</span> exam questions are randomly selecetd.</li>
</ol>
<ul>
<li>Show that the simple proportion <span class="math inline">\(p\)</span> of correctly answered questions gives a biased (over) estimate of <span class="math inline">\(\pi\)</span> if the student simply randomly guesses among the <span class="math inline">\(k\)</span> answers on questions where (s)he doesn’t know the answer. Do this by calculating the expected value of p (i.e. the average mark per question) when each answer is marked 1 if correct and 0 if not. (<em>Hint</em>: a tree diagram may help).</li>
<li>One can ‘de-bias’ the estimate by giving each correct answer a mark of 1 and each incorrect answer a negative mark. What negative mark (penalty) will provide an unbiased estimate of <span class="math inline">\(\pi\)</span>? Begin by finding the expected mark per question, then set it to <span class="math inline">\(\pi\)</span> and solve for the penalty. (<em>Hint</em>: If you prefer, use concrete values of <span class="math inline">\(\pi\)</span> and <span class="math inline">\(k\)</span> to see what penalty is needed.)</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><p>Half the purchases of eggs in a market are for 6 eggs and half are for 12. What percentage of purchases are for a quantity that is more than 1 SD from the mean? less than 1 SD?</p></li>
<li><p>Half the people in a population have 2 organs of a certain type and half have none. What is the standard deviation of the number of organs a randomly selecetd person has?</p></li>
<li><p>Verify the variances displayed in the above Figure showing the distribution of a random variable, before and after measurement errors are added to it. Then subtract the smaller variance from the larger one to estimate the error variance. Finally show, by a separate calculation, why your answer ‘fits’ with the details of how the error-containing variable was constructed.</p></li>
<li><p>Consider children of parents who both carry a single copy of the CF gene. (In the absence of ..) How many of their offspring will have 0, 1 or 2 copies?</p></li>
<li><p>Half of a large number of orders were placed on Tuesday and half on Thursday. The combined orders were all jumbled together and shipped in 3 equal sized shipments on Monday Wednesday and Friday of the following week, arriving the same day they shipped. Calculate the mean and standard deviation of the number of days between ordering and arrival. Use a probability tree to depict the randomness, and to show the calculations.</p></li>
<li>Refer to the example where the student tries to estimate the scaling factor between degrees C and degrees F.</li>
</ol>
<ul>
<li>What if the student took the F readings at two C values that are 10 C (rather than 5 C) apart?, i.e. at 12.5 C and 22.5 C?</li>
<li>How would the Variance and the SE be altered?</li>
<li>What if the student took the F readings at four equally spaced C values 5 C apart, i.e., at 7.5 C, 12.5 C, 17.5C and 22.5 C?. +If you were limited to <span class="math inline">\(n\)</span> C values, how would you decide where to place them?</li>
<li>What if, rather than <span class="math inline">\(0.5^2\)</span>, the ‘errors’ in F had a variance of <span class="math inline">\(1^2\)</span> or <span class="math inline">\(2^2\)</span>?</li>
</ul>
<p>UNDER CONSTRUCTION</p>
<ul>
<li><p>Elevators (needs normal)</p></li>
<li><p>Trial of Pyx</p></li>
</ul>
</div>
<div id="summary-slides-1" class="section level2">
<h2><span class="header-section-number">12.10</span> Summary Slides</h2>
<ul>
<li><p>The concepts of a random variable, and of its expectation and variance, underpin all of statistical inference. That is why this chapter is so central, even if we don’t apply the laws by hand.</p></li>
<li><p>The laws governing the variance of the sum and the mean of <span class="math inline">\(n\)</span> random variables are the basis for Standard Errors (SEs) of statistics (parameter estimates based on aggregated data).</p></li>
<li><p>When assessing the sampling variability or a sum or mean of independent random variables, it is not their standard deviations that sum (add), but rather their variances.</p></li>
<li><p>This is why we have the <span class="math inline">\(\sqrt{n}\)</span> law in Statistics. The SE of a statistic is directly proportional to <span class="math inline">\(\sqrt{n}\)</span> if we are dealing with a sum, and inversely proportional to <span class="math inline">\(\sqrt{n}\)</span> (or proportional to 1/<span class="math inline">\(\sqrt{n}\)</span> ) if we are dealing with a mean.</p></li>
<li><p>Since proportions are means (albeit of RVs that just take on two possibvle values), the same laws apply to them as well.</p></li>
<li><p>This law was not understood/appreciated until recent centuries. Statistical historial Stephen Stigler has a very nice example, in <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/c323/pyx.pdf">this article</a>, and retold in his book <a href="https://www-degruyter-com.proxy3.library.mcgill.ca/view/title/521193">The Seven Pillars of Statistical Wisdom</a>, where failure to understand it wrong gave people quite a bit of leeway to cheat.</p></li>
<li><p>It’s also why statisticians are forced to work with variances when developing the properties of estimators. But as an end user, you will typically work with the square roots of these, and speak about the number of children per parent rather than square children per square parent.</p></li>
<li><p>The law governing the variance of a difference of two random variables is even more important, since we are more often interested in contrasts than the level in a single context.</p></li>
<li><p>Whether we are add or subtract independent random variables, the result is more variable than its parts.</p></li>
<li><p>A regression slope can be represented as a linear combination (with varying positive and negative combining weights) of random variables, and so the variance and SD of the sampling distribution of a slope are gioverned by these same fundamental laws.</p></li>
<li><p>Although the main focus was on Variances and SDs, along the way in these above sub-sections, you saw the Central Limit Theorem (CLT) trying to exert itself. Although the narrowness/width of a sampling distribution is measured by a variance or SD, the <em>CLT focuses more on its shape</em>. It is not possible to give a general rule for the <span class="math inline">\(n\)</span> at which the CTL will ensure a sufficiently Gaussian shape for a sampling distribution. How ‘close’ to Gaussian any particular sampling distribution is depends on the ‘parent’ RV and the <span class="math inline">\(n\)</span>, but also on what you consider is ‘close enough for Government work’.</p></li>
<li><p>It is not critical that we ‘do’ several exercises on the theory (laws) in this chapter. After all, you will seldom have to manually do the SE calculations based on these laws – the statistical software will do it for you. But, you do need to understand the factors that make the SE’s big or small, and the concepts involved in the propagation – and reduction – of errors. There are several exercises in the computing session that will allow you to ‘see’ the laws in action, so that you can adopt them as guiding principles for study design, and for appreciating the ‘precision’ with which you can chase (take a shot at) statistical parameters.</p></li>
<li><p>For now, because the chapter is already a long one, we didn’t address sums and differences of <em>non-independent</em> random variables. But some of the computer exercsies will.</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/22-randomVariables.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["statbook.pdf", "statbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
