[
["index.html", "Introduction to Statistical Analysis: a regression-from-the-outset approach Preface 0.1 Target 0.2 Topics/textbooks 0.3 Regression from the outset 0.4 Parameters first, data later 0.5 Let’s switch to “y-bar”, and drop “x-bar”. 0.6 Computing from the outset 0.7 Appendix:", " Introduction to Statistical Analysis: a regression-from-the-outset approach Sahir, Shirin and Jim 2020-04-06 Preface 0.1 Target The target is graduate students in population health sciences in their first year. Concurrently, they take their first courses on epidemiologic methods. The department is known for its emphasis on quantitative methods, and students’ ability to carry out their own quantitative work. Since most of the data they will deal with are non-experimental, there is a strong emphasis on multivariable regression. While some students will have had some statistical courses as undergraduates, the courses start at the beginning, and are pitched at the Master’s level. In the last decade, the incoming classes have become more diverse, both in their backgrounds, and in their career plans. Some of those in the recently begun MScPH program plan to me consumers rather than producers of research; previously, the majority of students pursued a thesis-based Masters that involved considerable statistical analyses to produce new statistical evidence. 0.2 Topics/textbooks For the first term course 607, recent choices have been The Practice of Statistics in the Life Sciences by Baldi and Moore, and Stats by de Veaux, Velleman and Bock. Others that have been recommended are the older texts by Pagano and Gauvreau, and by Rosner. Some of us have also drawn on material in Statistics by Freedman, Pisani, Purves and Adkikari, and Statistical Methods in Medical Research, 4th Edition_ by Armitage,Berry, and Matthews. The newer books have tried to teach the topic more engagingly, by starting with where data come from, and (descriptively) displaying single distributions, or relationships between variables. They and the many others then typically move on to Probability; Random Variables; Sampling Distributions; Confidence intervals and Tests of Hypotheses; Inference about/for a single Mean/Proportion/Rate and a difference of two Means/Proportions/Rates; Chi-square Tests for 2 way frequency tables; Simple Correlation and Regression. Most include a (or point to an online) chapter on Non-Parametric Tests. They typically end with tables of probability tail areas, and critical values. Bradford Hill’s Principles of Medical Statistics followed the same sequence 80 years ago, but in black type in a book that measured 6 inches by 9 inches by 1 inch, and weighed less than a pound. Today’s multi-colour texts are 50% longer, 50% wider, and twice as thick, and weigh 5 pounds or more. The topics to be covered in the second term course include multiple regression involving Gaussian, Binomial, and Poisson variation, as well as (possibly censored) time-durations – or their reciprocals, event rates. Here is is more difficult to point to one modern comprehensive textbook. There is pressure to add even more topics, such as correlated data, missing data, measurement error etc. top the second statistics course. 0.3 Regression from the outset It is important to balance the desire to cover more of these regression-based topics with having a good grounding, from the first term, in the basic concepts that underlie all statistical analyses. The first term epidemiology course deals with proportions and rates (risks and hazards) and – at the core of epidemiology – comparisons involving these. Control for confounding is typically via odds/risk/rate differences/ratios obtained by standardization or Mantel-Haenszel-type summary measures. Teachers are reluctant to spend the time to teach the classical confidence intervals for these, as they are not that intuitive and – once students have covered multiple regression – superceded by model-based intervals. One way to synchronize with epidemiology, is to teach the six separate topics Mean/Proportion/Rate and differences of two Means/Proportions/Rates in a more unified way by embedding all 6 in a regression format right from the outset, to use generalized linear models, and to focus on all-or-none contrasts, represented by binary ‘X’ values. This would have other benefits. As of now, a lot of time in 607 is spent on 1-sample and 2-sample methods (and chi-square tests) that don’t lead anywhere (generalize). Ironically, the first-term concerns with equal and unequal variance tests are no longer raised, or obsessed about, in the multiple regression framework in second term. The teaching/learning of statistical concepts/techiques is greatly enriched by real-world applications from published reports of public health and epidemiology research. In 1980, a first course in statistics provided access to 80% of the articles in NEJM articles. This large dividend is no longer the case – and even less so for journals that report on non-experimental research. The 1-sample and 2-sample methods, and chi-square tests that have been the core of first statistics courses are no longer the techniques that underlie the reported summaries in the abstracts and in the full text. The statistical analysis sections of many of these articles do still start off with descriptive statistics and a perfunctory list of parametric and non-parametric 1 and 2 sample tests, but most then describe the multivariable techniques used to produce the reported summaries. [Laboratory sciences can still get by with t-tests and ‘anova’s – and the occasional ancova’; studies involving intact human beings in free-living populations can not.] Thus, if the first statistical course is to to get the same ‘understanding’ dividend from research articles as the introductory epidemiology course does, that first statistical course needs to teach the techniques that produce the results in the abstracts. Even if it can only go so far, such an approach can promote a regression approach right from week one, and build on it each week, rather than introduce it for the first time at week 9 or 10, when the course is already beginning to wind down, and assignments from other courses are piling up. 0.4 Parameters first, data later When many teachers and students think of regression, they imagine a cloud of points in x-y space, and the least squares fitting of a regression line. They start with thinking about the data. A few teachers, when they introduce regression, do so by describing/teaching it as an equation that connects parameters, constructed in such a way that the parameter-constrast of interest is easily and directly visible. Three such teachers are Clayton and Hills 1995, Miettinen1985, and Rothman 2012. In each case, their first chapter on regression is limited to the parameters and to undersatnding what they mean; data only appear in the next chapter. There is a lot to commend this approach. It reminds epidemiologists – and even statisticians – that statistical inference is about parameters. Before addressing data and data-summaries, we need to specify what the estimands are – i.e, what parameter(s) is(are) we pursuing. It is easy and tempting to start with data, since the form of the summary statistic is usually easy to write down directly. It can also be used to motivate a definition: for example, we could define an odds ratio by its empirical computational form ad/bc. However, this ‘give me the answer first, and the question later’ approach comes up short as soon as one asks how statistically stable this estimate is. To derive a standard error or confidence interval, one has to appeal to a sampling distribution. To do this, one needs to identify the random variables involvced, and the parameters that determine/modulate their statistical distributions. Once students master the big picture (the parameter(s) being pursued), the task of estimating them by fitting these equations to data is considerably simplified, and becomes more generic. In this approach more upfront thinking is devoted to the parameters – to what Miettinen calls the design of the study object – with the focus on a pre-specified ‘deliverable.’ 0.5 Let’s switch to “y-bar”, and drop “x-bar”. The prevailing practice, when introducing descriptive statistics, and even to 1 and two sample procedures, is to use the term x-bar (\\(\\bar{x}\\)) for an arithmetic mean (one notable execption is de Veaux at al.) This misses the chance to prepare students for regression, where E[Y|X] is the object of interest, and the X-conditional Y’s are the random variables. Technically speaking, the X’s are not even considered random variables. Elevating the status of the Y’s and explaining the role of the X’s, and the impact of the X distributions on precision might also cut down on the practice of checking the normality of the X’s, even though the X’s are not random variables. They are merely the X locations/profiles at which Y was measured/recorded. When possible, the X distribution should be determined by the investigators, so as to give more precise and less correlated estimates of the parameters being pursued. Switching from \\(\\bar{x}\\) to \\(\\bar{y}\\) is a simple yet meaningful step in this direction. JH made this switch about 10 years ago. 0.6 Computing from the outset In 1980, most calculations in the first part of 607 were by hand calculator. Computing summary statistics by hand was seen as a way to help students understand the concepts involved, and the absence of automated rapid computation was not considered a drawback. However, doing so did not always help students understand the concept of a standard deviation or a regression slope, since these formulae were designed to minimize the number of keystrokes, rather than to illuminate the construct involved. For example, it was common to rescale and relocate data to cut down on the numbers of digits entered, to group values into bins, and use midpoints and frequencies. It was also common to use the computationally-economical 1-pass-through-the-data formula for the sample variance \\[s^2 = \\frac{ \\sum y^2 - \\frac{(\\sum y)^2}{n}}{n-1},\\] even though the definitional formula is \\[s^2 = \\frac{\\sum(y - \\bar{y})^2}{n-1}.\\] The latter (definitional) one was considered too long, even though having to first have to compute \\(\\bar{y}\\) and then go back and compute (and square) each \\(y - \\bar{y}\\) would have helped students to internalize what a sample variance is. When spreadsheets arrived in the early 1980s, students could use the built-in mean formula to compute and display \\(\\bar{y}\\), another formula to compute and display a new column of the deviations from \\(\\bar{y}\\), another to compute and display a new column of the squares of these deviations, another to count the number of deviations, and a final formula to arrives at \\(s^2.\\) The understanding comes from coding the definitional formula, and the spreadsheet simply and speedily carries them out, allowing to user to see all of the relevant components, and from noticing if each one looks reasonable. Ultimately, once students master the concept, they could move on to built-in formulae that hide (or themselves avoid) the intermediate quantities. Few teachers actually encouraged the use of spreadsheets, and instead promoted commercial statistical packages such as SAS, SPSS and Stata. Thus, the opportunity to learn to `walk first, run later’ afforded by spreadsheets was not fully exploited. RStudio is an integrated environment for R, a free software environment for statistical computing and graphics that runs on a wide variety of platforms. Just like spreadsheet software, one can use R not just as a calculator, but as a programmable calculator, and by programming them, learn the concepts before moving on to the built-in functions. There is a large user-community and a tradition of sharing information and ways of doing things. The graphics language contains primitive functions that allow customization, as well as higher-level functions, and is tighly integrated with the statistical routines amd data frame functions. R Markdown helps to foster reproducible research. Shiny apps allow interactivity and visualization, a bit like ‘what-ifs’ with a spreadsheet. It takes practice to become comfortable with R. Gor those less mathematical, it is somewhat more cryptic than, and not quite as intuitive as, other packages. For the last several years, the department has offered a 13 hour course introduction to R in first term. Initially the aim was to prepare students for using it in course 621 in second term, but in the Fall 2018 and 2019 offerings of course 607, computing with R and use of R Studio became mandatory. Just as the epidemiology material in the Fall is shared between 2 courses (601 and 602), the aim will be to also spread the statistics material over 607 and 613, and to integrate the two more tightly. As an example, the material on ‘descriptive’ (i.e., not model-based) statistics and graphical displays will be covered in 613, while 607 will begin with parameters and models. Rather than treat computing as a separate activity, exercises based on 607 material will be carried out as part of 613 classes/tutorials. The statistical material will be used to motivate the computer tasks. 0.7 Appendix: [Still rough] History of current introductory biostatistics courses The senior author first taught a 2-course sequence for first year graduate students in epidemiology in 1980, using Colton’s Statistics in Medicine as the text for the introductory course (607). He developed his own notes for the second course, which covered multiple regression for quantitative responses. Over the next 10 years, he continued to teach the first course – first from Colton, but latterly from Moore and McCabe (and undergraduate text) and with epi statistics from Armitage and Berry and some other fundamentals from Freedman (Statistics). Stan S taught the second (621 Data Analysis in the Health Sciences), mostly from Kleinbaum’s Applied Regression Analysis and Other Multivariable Methods. In the 1990s, Lawrence J taught 607, and Michal A 621. Neither used a required textbook. LJ developed an extensive set of written notes (still available on his website) (and contributed a chapter Introduction to Biostatistics: Describing and Drawing Inferences from Data book on Surgical Arithmetic) while MA used transparencies that were widely photocopied. 2000s Robert P 621? LJ 621 Meanwhile JH taught to summer students (mostly medical residents and fellows): 607 and a second course (678, Analysis of Multivariable Data). Both sets of content are available on JH’s website. He last taught the Fall version of 607 in 2001, when LJ was on sabbatical. 607: Tina 2006 - 201x Erica M; 20xx - Paramita SC. 2018, 2019 Sahir B 621: Aurelie Alexandra 2020 Shirin "],
["introduction.html", "Chapter 1 Introduction 1.1 Goals 1.2 Structure 1.3 Attitudes, etc….", " Chapter 1 Introduction 1.1 Goals Blah 1.2 Structure Blah Blah 1.3 Attitudes, etc…. Blah Blah Blah "],
["paras.html", "Chapter 2 Statistical Parameters 2.1 Parameters 2.2 Parameter Contrasts 2.3 Parameter functions 2.4 Phraseology to avoid 2.5 SUMMARY 2.6 Exercises 2.7 References", " Chapter 2 Statistical Parameters 2.1 Parameters The objectives of this chapter are to Define what a parameter is in a statistical context See examples of such parameters Understand the concept of a parameter relation or a parameter equation Be able to set up parameter equations that isolate and directly pinpoint parameter differences in both the absolute and relative scales, using a regression equation framework. Do so before fitting any such (regression) equations to data, so that we can focus on the research objects without having data get in the way. See the unity (generality) in what we will be doing in the course, by seeing the big picture, i.e., the forests, not the trees. We begin by defining is meant by the term parameter in a statistical context Parameter – A constant (of unknown magnitude) in a (statistical) model. [OSM2011, p60] In Statistics. A numerical characteristic of a population, as distinguished from a statistic obtained by sampling.[OED] Note that the term can mean other things in other contexts. For example, in clinical medicine, Parameter – any quantitative aspect/dimension of the client’s (patient’s) health, subject to measurement (by means of a test). (Example: systolic blood-pressure.)[OSM, Terms and Concepts of Medicine] The (statistical) parameters we will be concerned with \\(\\mu\\) The mean level of a quantitative characteristic, e.g. the depth of the earth’s ocean or height of the land, or the height / BMI / blood pressure levels of a human population. [One could also think of mathematical and physical constants as parameters, even though their values are effectively ‘known.’ Examples where there is agreement to many many decimal places include the mathematical constant pi, the speed of light(c), and the gravitational constant G. The speed of sound depends on the medium it is travelling through, and the temperature of the medium. The freezing and boiling points of substances such as water and milk depend on altitude and barometric pressure]. At a lower level, we might be interested in personal characters, such as the size of a person’s vocabulary, or a person’s mean (or minimum, or typical) reaction time. The target could be a person’s ‘true score’ on some test – the value one would get if one (could, but not realistically) be tested on each of the (very large) number of test items in the test bank, or observed/measured continously over the period of interest.     Later on we will address sitautions where the mean \\(\\mu\\) is not the best ‘centre’ of a distribution, and why we might want to take some other feature, such as the median, or some other quantile, instead. \\(\\pi\\) Prevalence or risk (proportion): e.g., proportion of the earth’s surface that is covered by water, or of a human population that has untreated hypertension, or lacks internet access, or will develop a new health condition over the next x years. At a lower level, we might be interested in personal proportions, such as what proportion of the calories a person consumes come from fat, or the proportion of the year 2020 the person spent on the internet, or indoors, or asleep, or sedentary. \\(\\lambda\\) The speed with which events occur: e.g., earthquakes per earth-day, or heart attacks or traffic fatalities per (population)-year. At a lower level, we might be interested in personal intensities, such as the mean number of tweets/waking-hour a person issued during the year 2020, or the mean number of times per 100 hours of use a person’s laptop froze and needed to be re-booted. Each of these three parameters refers to a characteristic of the overall domain, such as entire surface of the earth, or the entire ocean, or population. There are no indicators for distinguishing among subdomains, so they refer to locations / persons not otherwise specified. We will drill down later. Especially for epidemiologic research, and also more generally, one can think of \\(\\pi\\) and \\(\\lambda\\) as parameters of occurrence. [Although the word occurrence usually has a time element, it can also be timeless: how frequently a word occurs in a static text, or a mineral in a rock.] Prevalence is the proportion in a current state, and the 5-year risk is the expected proportion or probability of being in a new state 5 years from now. The parameter \\(\\lambda\\) measures the speed with which the elements in question move from the original to the other state. Even though the depths of the ocean, and blood pressures, are measured on a quantitative (rather than on all or none) scale, one can divide the scale into a finite number of bins/caterories, and speak of the prevalence (proportion) in each category. Conversely, one can use a set of descriptive parameters called quantiles, i.e, landmarks such that selected proportions, e.g., 0.05 or 5%, 25%, 50%, 75%, 95% of the distribution are to the left of (‘below’) these quantiles. Occurence Parameters are not constants of nature [OSM1995] It has been noted in the philosophy of science that any science is concerned with functional relations of its objects (Friend and Feibleman, 1937). This proposition is quite evidently tenable for epidemiologic objects of research. Parameters of occurrence, such as the incidence rate for a particular illness, are not constants of nature. Rather, their magnitudes generally depend on — are functions of — a variety of characteristics of individuals — constitutional, behavioral, and/or environmental. Such relations, even if only remotely credible, are generally the objects of medical occurrence research. For example, one is quite usually interested in learning whether the rate of occurrence of some particular illness depends on (is related to or is a function of) gender — regardless of whether there is any express reason to surmise that it might be. EXAMPLE 1.5 The prevalence of any given blood type based on the ABO antigen system,, while constant over gender and essentially constant over age, is not a constant of nature. It varies by ethnic groupings, for example. Thus the prevalence must be quantified in relation to—as a function of—ethnic group. EXAMPLE 1.6. For the occurrence of various values of blood pressure among people, one descriptive parameter is the median of the pressure. (This is a value such that the prevalence of its exceedance is 50%.) This parameter, again, is not a constant of nature but depends on age and other characteristics of individuals. For the quantitative nature of the age relation of systolic blood pressure, a rule of thumb used to be that it is, in mm Hg, 100 plus age in years.&quot; This rule expresses a regression model - a regression function - of the form P = A + B x Age. In this example, P, the occurrence parameter, is the median of systolic blood pressure, A = 100 mm Hg, and B = 1 mm Hg/yr. The characteristics on which the magnitude of an occurrence parameter depends (causally or otherwise) are determinants of the parameter. Thus, in the examples given above, ethnic grouping is a determinant of prevalence of any given blood type, and age is a determinant of the median of systolic blood pressure. “Determinant” has no implication as to causality in science — any more than in everyday locution: the current age of a person is “determined” by his/her year of birth (noncausally), just as the expected outcome of a disease is “determined” by the treatment that is used (causally). The relation of an occurrence measure to a determinant, or a set of determinants, is naturally termed an occurrence relation or an occurrence function. These relations are in general the objects of epidemiologic research. [Even though the general inconstancy of occurrence parameters leads to the consideration of occurrence relations, this latter outlook affords only a partial accommodation of the inconstancy, because occurrence relations the degree also vary according to the type of individual. In particular, measures of a relation (Appendix 2) have determinants of their own.] Before we start, a comment on terminology Before we go on, we need to adopt sensible terminology for referring generically to the states, traits, conditions or behaviours whose category-specific parameter values are being compared. Following OSM (see above) we will use the term `determinant’. It has several advantages over the many other terms used in different disciplines, such as exposure, agent, independent/explanatory variable, experimental condition, treatment, intervention, factor, risk factor, predictor. The main advantage is that it is broader, and closer to causally neutral in its connotaion. Exposure has environmental connotations, and technically refers to an opportity to injest or mentally take on board a substance or message. Agent has causal connotations. The term independent variable suggests the investigator has control over it in a laboratory setting. The term explanatory is ambiguous as to the mechanism by which the parameter value in the index category got to be different from the value in the index category. Not all contrasts are experimentally formed. The term factor, and thus the term risk factor, are to be avoided because the word factor derives from the Latin facere, (the action of) doing, making, creating. Predictor makes one think of the future. The term regressor (or its shorthand, the ‘X’ ) won’t be understood by lay people. While the word ‘determine’ can suggest causality (e.g., demand determines the price), it also refers to ‘fixing the form, position, or character of beforehand’: two points determine a straight line; the area of a circle is determined by its radius. There is considerable philosophical debate as to whether something ‘causes’ something else. Some would argue that the extent to which genetics determines one’s personality is a causal concept. Others argue that since one cannot cannot consider the alternative, ones biological sex or age can not be considered a causal determinant or a risk factor (in the strict causal meaning of the word). They prefer to refer to them as risk indicators. We now move on to the parameter relations we will be concerned with, beginning with the simplest type. 2.2 Parameter Contrasts In applied research, we are seldom interested in a single constant. Much more often we are interested in the contrast (difference) between the parameter values in different contexts/locations (Northern hemisphere vs Southern hemisphere), conditions/times (reaction times using the right versus left hand, or behaviour on weekdays versus weekends), or sub-domains or sub-populations (females vs males). Contrasts involving ‘persons, places, and times’ have a long history in epidemiology. In this section, we will limit our attention to ‘contrasts’: a compariosn of the parameter values between 2 contexts/locations/sub-populations. Thus (unlike in OSM’s example 1.6) the parameter function has just 2 possible ‘input’ values. The next section will address more general parameter functions. ‘Reference’ and ‘Index’ categories In many research contexts, the choice of ‘reference’ category (starting point, the category against which the other category is compared) will be obvious: it is the status quo (standard care, prevailing condition or usual situation, dominant hand, better known category). The ‘index’ category is the category one is curious about and wishes to learn more about, by contrasting its parameter value with the parameter value for the reference category.     In other contexts, it is less obvious which category should serve as the reference and the index categories, and the choice may be merely a matter of persepctive. If one is more famiar with the Northern hemisphere, it serves as a natural starting point (or ‘corner’ to use the terminology of Clayton and Hills, or reference category). The choice of reference category in a longevity contrast between males and females, or in-hospital mortality rates or motor vehicle fatality rates during weekends versus weekdays, might depend on what mechanism one wishes to bring out. Or one might close as the reference category the one with the larger amount of experience, or maybe the one with the lower parameter value, so that the ‘index minus reference’ difference would be a positive quantity, or the ‘index: reference ratio’ exceeds 1. 2.2.1 Parameter relations in numbers and words To make this concrete, we will use hypothetical (and very round) numbers and pretend we ‘know’ the true parameter values – in our example of the mean depth of the ocean in the Northern hemisphere (reference category) and Southern hemisphere (index category) – to be 3,600 metres (3.6Km) and 4,500 metres (4.5Km) respectively. Thus, the difference (South minus North) is 900 metres or 0.9Km. If we wished to show the two parameter values graphically, we might do so using the format in panel (a), which shows the 2 hemisphere-specific parameter values – but forces the reader to calculate the difference. Panel (b) follows a more reader-friendy format, where the difference (the quantity of interest) is isolated: the original 2 parameters are converted to 2 new, more relevant ones. Panel (c) encodes the relation displayed in panel (b) in a single phrase that applies to both categories: Onto the ‘starting value’ of 3.8Km, one adds \\(\\Delta \\mu\\) = 0.9 Km only if the resulting parameter pertains to the Southern hemisphere. The 0.9 Km is toggled off/on as one moves from North to South. 2.2.2 Parameter relations in symbols, and with the help of an index-category indicator Panels (a) and (b) in the following figure repeat the information in panels (a) and (b) in the preceding Figure, but using Greek letters to symbolically represent the parameters. Just to keep the graphics uncluttered, the labels North and South are abbreviated to N and S and used as subscripts. Also, for brevity, the expression \\(\\Delta \\mu\\) denotes \\(\\mu_S - \\mu_N\\). The relation encoded in a single phrase shown in the previous panel (c) has a compact form suitable for verbal communication. The representation can be adapted to be more suitable for computer calculations. (The benefit of doing this will become obvious as soon as you try to learn the parameter values by fiiting these models to actual data.) Depending on whether the hemisphere in question is the northern or southern hemisphere, the expression/statement ‘the specified hemisphere is the SOUTHERN hemisphere’ evaluates to a (logical) FALSE or TRUE. In the binary coding used in computers, it evaluates to 0 or 1, and we call such a 0/1 variable an ‘indicator’ variable.^ ^ In ‘better families’ we speak of INDICATOR variables, not DUMMY variables. The International Statistical Institute’s Dictionary of Statistical Terms objects to the name: the term is ‘used, rather laxly, to denote an artificial variable expressing qualitative characteristics …. [The] word ’dummy’ should be avoided.’ Miettinen’s Epidemiological Research: Terms and Concepts:- Indicator variate – A variate with 0 and 1 as its (only) realizations, with realization 1 indicating something particular. (Examples: Y = 1 indicating membership in the case series of person-moments and X1 = 1 indicating index category of the etio-genetic determinant in an etiogenetic study – in the logistic model for the object of study.) Dummy variate (synonym: indicator variate) – See ‘Indicator variate’ in section II – 2. Note: This term is a misnomer: there is nothing dummy about an indicator variate. We encourage you to use, in your coding, meaningful variable names such as i.South or i.Southern (where i stands for indicator of) or i.Male. Don’t use the name sex or gender, where the coding is not self evident. If you think i.Male is over doing it, then use Male. In panel (c) in the following figure, just to keep the graphics uncluttered, the name of the indicator variable SOUTHERN is abbreviated to S, and \\(\\mu_S\\) is shorthand for the \\(\\mu\\) cooresponding to whichever value (0 or 1) of \\(S\\) is specified (we could also write it as \\(\\mu | S\\), or \\(\\mu\\) ‘given’ \\(S\\). ) Thus, the symbol \\(\\mu_0\\) refers to the \\(\\mu\\) when \\(S=0\\), or in longerhand, to \\(\\mu \\ | \\ S = 0\\). What does the equation in panel (c) remind you of? Probably the equation of a line. In high school you may have learned it in the form \\(A + B \\times X\\) that Miettinen used to describe the relation between median blood pressure and age. Today, in statistics, these equations are referred to as regression equations, and the statistical model is called a regression model. The term ‘regression’ is unfortunate, since it bears little relation to the original application. It concerned the phenomenon of ‘reversion’ first described by Charles Darwin. Following his first studies of the sizes of the ‘daughter’ seeds of sweet peas, his nephew Francis Galton, described the tendency: offspring did not tend to resemble their parent seeds in size, but to be always more mediocre (‘middling’, or closer to the mean) than they — to be smaller than the parents, if the parents were large; to be larger than the parents, if the parents were very small (Galton 1886) One of the first ‘regression’ lines fitted to human data is Galton’s line depicting the ‘rate of regression on hereditary stature’ where, using the term ‘deviate’ where today we would use ‘Z-scores.’ The Deviates of the Children are to those of their Mid-Parents as 2 to 3. (Galton, 1886) Because he used z scores (so the means in the parents and in the children were both 0) the equation of the line simplified to \\[\\mu(\\textrm{Z-score in children of parents with mean z}) = 0 + (2/3) \\times z\\] But don’t we need a cloud of points to have a regression line? Although many courses and textbooks introduce regression concepts this way, the answer is NO. There is nothing in the regression formulation that specifies at which ‘X’ values the mean Y values at these X values are to be determined. Unlike many textbboks that start with Xs on a ‘continuous’ scale, and then later have to deal with a 2-point (binary) X, we are starting with this simplest case, and will move ‘up’ later. We are doing this for a few reasons: in epidemiology, the first and simplest contrasts involve just two categories, the reference category and the index category; a simple subtraction of 2 parameter values is easier to do and to explain to a lay person; and there is no argument about how the function behaves at the values between 0 and 1. There are no parameter values at Male = 0.4 or Male = 1.4, they are only at Male=0 and Male=1. In addition, it is easier to learn the fundamental concepts and principles of regression if we can easily ‘see’ what exactly is going on. Fewer blackbox formulae mean more transparency and understanding. Once we see how to represent parameter values in two determinant-categories, we can easily extend it to more than two, such as the ethnic groups in example 1.5 above. As we will see later on, when we have a value for a dental health parameter (eg the mean number of decayed, missing and filled DMF teeth) at X = 0 parts per million of fluoride in the drinking water, and another parameter value at X = 1 parts per million, we can only look at these 2 parameter values. If this is not enough, we would need to have (obtain) parameter values at the intermediate fluoride levels, or levels beyond 1 ppm, to trace out the full parameter relation, namely how the mean-DMF varies as a function of fluoride levels. If we have large numbers of observations at each level, then the DMF means will trace out a smooth curve. If data are limited, and the trace is jumpy/wobbly, we will probably resort to a sensible smooth function, the coefficients of which will have to be estimated from (fitted to) data. This discussion leads on naturally to situations where the parameter varies over quantitative levels of a determinant - a topic considered in the next section. But meantime, we need to answer this question: why limit ourselves to subtraction? why not consider the ratio of the two parameters, rather than their difference? Relative differences (ratios) – in numbers first A ratio can be more helpful than a difference, especially if you are don’t have a sense of how large the parameter value is even in the reference category. As an example, on average, how many more red blood cells do men have than women? or how much faster are gamers’ reaction times compared with nongamers? Recall our hypothetical mean ocean depths, 3.6 Km in the oceans in the Northern hemisphere (reference category) and 4.5Km in the oceans of the Southern hemisphere (index category). Thus, the S:N (South divided by North) ratio is 4.5/3.6 or 1.25. Panel (a) leaves it to the reader to calculate the ratio of the parameter values. In panel (b) the ratio (the quantity of interest) is isolated: again, the original 2 parameters are converted to 2 new, more relevant ones. Again, panel (c) shows a single master-equation that applies to both hemispheres by togging off/on the ratio of 4.5/3.6. Relative differences (ratios) – expressed in symbols and with the help of the index-category indicator To rewrite these numbers in a symbolic equation suitable for a computer, we again convert the logical ‘if South’ to a numerical Southern-hemisphere-indicator, using the binary variate \\(S\\) (short for Southern) that takes the value 0 if the Northern hemisphere, and 1 if the Southern hemisphere. But go back to some long-forgotten mathematics from high school to be able to tell the computer to toggle the ratio off and on. Recall ‘powers’ of numbers, where, for example, ‘\\(y\\) to the power 2’, or \\(y^2\\) is the square of \\(y\\). The two powers we exploit are 0 and 1. ‘\\(y\\) to the power 1’, or \\(y^1\\) is just \\(y\\) and ‘\\(y\\) to the power 0’, or \\(y^0\\) is 1. We take advantage of these to write \\[\\mu_S = \\mu \\ | \\ S \\ = \\mu_0 \\ \\times \\ \\Big\\{ \\frac{\\mu_{South}}{\\mu_{North}}\\Big\\}^S = \\mu_0 \\ \\times Ratio \\ ^ S.\\] You can check that it works for each hemisphere by setting \\(S=0\\) and \\(S=1\\) in turn. Thus, \\[\\log(y^S) = S \\times \\log(y)\\] Although this is a compact and direct way to express the parameter relation, it is not well suited for fitting these equations to data. However, in those same high school mathematics courses, you also learned about logarithms. For example, that \\[\\log(A \\times B) = \\log(A) + \\log(B); \\ \\ \\log(y^x) = x \\times \\log(y).\\] Thus, we can rewrite the equation in panel (c) as \\[\\log(\\mu_S) = \\log(\\mu \\ | \\ S) \\ = \\underbrace{\\log(\\mu_0)} \\ + \\underbrace{\\log(Ratio)} \\times \\ S.\\] This has the same `linear in the two parameters’ form as the one for the parameter difference: the parameters are \\(\\underbrace{\\log(\\mu_0)}\\) and \\(\\underbrace{\\log(Ratio)}\\) and they are made into the following ‘linear compound’ or ‘linear predictor’ (see Remarks below) : \\[\\log(\\mu_S) = \\log(\\mu \\ | \\ S) \\ = \\underbrace{\\log(\\mu_0)} \\times \\ 1 \\ + \\underbrace{\\log(Ratio)} \\times \\ S.\\] The course is concerned with using ‘regression’ software to ‘fit’/‘estimate’ these 2 parameters from \\(n\\) depth measurements indexed by \\(S\\). 2.3 Parameter functions A very simple example of a function that describes how parameter values vary over quantitative levels of a determinant is the straight line shown in the upper right panel of the next figure. Here the determinant has the generic name X, and the equation is of the \\(A + B \\times X\\) or \\(B_0 + B_1 \\times X\\) or \\(\\beta_0 + \\beta_1 \\times X\\) straight line form. Miettinen used the convention that the upper case letters \\(A\\) and \\(B\\) are used to denote the (true but unknown) coefficient values, whereas the lower case leters \\(a\\) and \\(b\\) are used to denote their empirical counterparts, sometimes called estimated coefficients or fitted coefficients. This sensible and simple convention also avoids the need, if one uses Greek letters for the theoretical coefficient values, to put ‘hats’ on them when we refer to their empirical counterparts, or ‘estimate/fit’ them. Fortunately, journals don’t usually allow investigators to use ‘beta-hats’; but this means that the investigators have to be more careful with their words and terms. As we go left to right in the following grid, the models become more complex. The simplest is the one of the left, in column 1, the one JH refers to as ‘the mother of all regression models.’ It refers to a single or overall situation/population/domain, so \\(X \\equiv 1\\), it takes on the value 1 in/for every instance/member. So the parameter equation is \\(\\mu_X = \\mu \\ | \\ X \\ = \\mu \\times 1.\\) In column 2, there are 2 subdomains, indexed by the 2 values of the determinant (here generically called ‘X’), namely \\(X = 0\\) and \\(X = 1\\). In the 3rd column, the number of of parameters is left unspecified, since the numbers of coefficients to specify a line/curve might vary from as few as 1 (if we were describing how the volume of a cube dependeds or, was is function of, its radius) to 2 (for a straight line that did not go through the origin, or for a symmetric S curve) to more than 2 (e.g., for a non-symmetric S curve, or a quadratic shape). A few more remarks on the panels in this Figure The 3 rows refer to the 3 core parameters we have given examples of above. All 3 are governed by the same principles, although there are more possibilities of different possible scales for some parameters. In setting (column) 1: there is just 1 parameter (value shown as a dot) corresponding to the ‘overall’ population or the entire domain. You can think of it as the limiting or ‘degenerate’ case of the columns to its right. One can still write it in a ‘regression’ model. It is of the form P(arameter) = \\(B\\), involving no indicators for distinguishing among subdomains of the referent domain of the distribution, say adults not otherwise specified. [MSH2018, p63] It is sometimes referred to as a null or ‘incercept only’ regression model.     We will exploit this idea to take a more holistic/general and economical approach to this introductory course. Many textbooks/courses do not mention regression models until quite late, and spend a lot of time on ‘1-sample’ (and even ‘2-sample’) problems without pointing out that these are merely sub-cases of regression models. This ‘silos’ practice of promoting/learning a separate software routine for dealing with a 1 sample problem, when one can get the same answer from a regression routine, leads to dead ends and wastes time.     Once we get to fitting/estimating a mean (or proportion or rate) parameter to/from data, we will encourage doing so within a regression framework. In setting (column) 2, there are 2 parameter values, one for the reference category and one for the index category of the determinant. As we have seen, how they relate to each other can be can be expressed in a number of different ways. A common and useful way is via a parameter equation that contains a parameter for the reference category and a comparative parameter (some measure of the difference between the two parameters) – the latter is often of most interest. In setting (column) 3, the parameter equation traces the parameter over a continuum of possible values of the determinant, using as many coefficients as are needed. In this particular diagram, the values of the determinant (X) are shown starting at X = 0, but this does not have to be. In data analysis, one often shifts the X origin, so that the ‘intercept’ makes more sense. For example, if one was plotting world temperatures, or ice-melting dates (see Chapter on Computning) against calendar year, it would be better to have the incercept refer to the fitted temperature for when the series begins, rather than when our current Western calendat begins (at the year 0 AD). Likewise, if we were describing the relation between ideal weight and height it is good to start near where people’s heights are. Thus, ‘100 pounds for a height of 5 feet, with five additional pounds for each added inch of height’ for women, and ‘106 pounds for a height of 5 feet, and six additional pounds for every added inch of height.’ for men. Of course, if you wish, for women you could use the mathematically equivalent ‘-300 pounds for a height of 0 feet, with five additional pounds for each added inch of height’ but it is not that easy to remember, and doesn’t apply for much of the (unspecified) height range! A few remarks on associated terminology Instead of ‘regression models’, some textbooks and courses refer to ‘linear models’ : Linear model: Formulation of the mean/‘expectation’ of (the distribution of) a random variate (Y) as a linear compound of a set \\(B_0 , B_1 , B_2 , \\dots\\) of parameters: as \\(B_0 + B_1 X_1 + B_2 X_2 + \\dots\\) [Miettinen 2011, p54] The meaning of ‘linear’ in the appellation of this model has nothing to do with straight lines; it refers to the mathematical concept of ‘linear compound’: given quantities Q\\(_1\\), Q\\(_2\\), etc., a linear compound of these is the sum C\\(_1\\)Q\\(_1\\) + C\\(_2\\)Q\\(_2\\) + …, where C\\(_1\\) etc. are the ‘coefficients’ that define a particular linear compound of the set of quantities constituted by the Qs. So, the ‘general linear model’ is linear in the sense that the dependent parameter, M, is formulated as a linear compound of the independent parameters B0, B1, etc., the coefficients in this linear compound being 1, X1, etc. The model is, in this way, ‘linear in the parameters.’ [MSH2018, p65] Statistics courses in the social sciences, the biological laboratory sciences, and other experimentally-based sciences, typically move on from 1- and 2-sample procedures (unfortunately, mainly focusing on statistical tests) to ‘analysis of variance’ models Miettinen explains an ’analysis of variance models this way: In the ‘analysis of variance model,’ the random variate at issue – Gaussian – has a mean whose value depends on a nominal-scale determinant, a nominal scale being characterized by discrete categories without any natural order among them. The names of the (nominal) categories, some N in number, could be Category 1, Category 2, … , Category N. The term for the model is a misnomer. For, at issue is not analysis but synthesis of data, and the synthesis is not directed to learning about the variance of the random variate; it focuses on the mean, the relation of the mean to the (nominal-scale) determinant of it. A simple example of these models might address the mean of systemic blood-pressure – defined as the weighted average of the diastolic and systolic pressures with weights 2/3 and 1/3, respectively – in relation to ethnicity, represented by three categories. An ‘analysis-of- variance’ model would define a random variate (Y) as representing the numerical value of the pressure (statistical variates inherently being numerical) and having a Gaussian distribution with means M1, M2, and M3 in those ethnicity categories 1, 2, and 3, respectively, with the variance of the distribution invariant among them. The random variate (Y) is the ‘dependent’ variate in the meaning that the value of its mean depends on ethnicity; and the ethnicity categories are represented in terms of suitably-defined ‘independent’ – non-random – variates (Xs). The form of the ‘analysis-of-variance’ model in this simple example is: \\(M = B_0 + B_1 X_1 + B_2 X_2\\), where \\(M\\) is the mean of \\(Y\\) and the two independent variates are indicators of two particular ones of the three ethnic categories. One possibility in this framework is to take \\(X_1\\) and \\(X_2\\) to be indicators of Category 2 and Category 3, respectively – an indicator variate being one that takes on the value 1 for the category it indicates, 0 otherwise. In terms of this model, \\(B_0\\) is the value of \\(M\\) when \\(X_1 = X_2 = 0\\), that is, for Category 1 (i.e., \\(B_0 = M_1\\)); and for Category 2 and Category 3 the values of \\(M\\) are represented by \\(B_0 + B_1\\) and \\(B_0 + B_2\\), respectively (i.e., \\(M_2 = B_0 + B_1\\), and \\(M_3 = B_0 + B_2\\)). Thus, the difference between \\(M_1\\) and \\(M_2\\) is represented by \\(B_1\\); \\(B_2\\) represents the difference between \\(M_1\\) and \\(M_3\\); and the difference between \\(M_2\\) and \\(M_3\\) is the difference between \\(B_1\\) and \\(B_2\\). In this ‘analysis-of-variance’ framework it is feasible to accommodate, jointly, whatever number of nominal-scale determinants of the magnitude of the mean of the dependent variate. A simple example of this is the addition of the two categories of gender for consideration jointly with the three categories of ethnicity. These two determinants jointly imply a single nominal-scale determinant with six categories (as each of the three categories of ethnicity is split into two subcategories based on gender). When involved in the definition of the independent variates is only a single determinant, the model is said to be for ‘one-way analysis of variance’; with two determinants the corresponding term (naturally) is ‘two-way analysis of variance’; etc. 2.4 Phraseology to avoid It is quite common to hear a regression coefficient (fitted or theoretical) interpreted this way: “For every 1 unit increase in X, the ‘Y’ parameter increases by \\(\\beta_X\\) units.” or as follows “As (when) you increase X by 1 unit, you increase the Y parameter by \\(\\beta_X\\) units.” We pick up this terminology very early, maybe even back in high school, and from other people around us. But, in interpreting the B = 1 mm Hg/yr in Miettenen’s example (100 plus age in years), should we use such phrases? Or, since you don’t know the source of, or the data behind this rule, you can take a look at the distributions of some anthropometric chacacteristics (height, weight, forced expiratory volume, FEV) measured cross-sectionally, in different populations – Busselton, Australia and rural Southwest Ethiopia – in 1972 and 1992. By eye, try to estimate the slope you would get if you regressed the age-and sex-specific means or medians on the ages. and then summarize the gradient across age. Remember that these these subjecsts aren’t aging or going anywhere, and nobody was watching them age. It is more accurate to say: People who were aged a+1 years at the time of the survey had heights/weights/FEVs that were t.tt units higher/lower than people who were aged a years. or The mortality rate was u.uu units higher/lower (or u.u times higher/lower) in the experience in the index category than the reference category. This way, you are telling the reader that this is a static source, and not a dynamic situation where conditions are being manipulated by the investigators, or the subjects being watched as their ages go up [for many readers, the word ‘increased’ implies that some human force deliberately changed the dial, and turned the X up or down, as one could do with temperature or humidity in a laboratory.] One of JH’s favourite examples of people being misled into thinking that a cross-sectional dataset allows you to say that ‘as people get older, they …’ is the McGill epidemiology department’s studies, in the 1960s, on the health of the more than 10,000 millers and miners of asbestos. These workers were born between 1890 and 1920. In cross-sectional studies, there were gradients in mean height across attained age. It would be easy to give them a ‘as people get older, they shrink in height’ or they ‘gain in height’ interpretation. It is easy to overlook the fact that some of these were children and adolescents during the Depression. Next Chapter The next chapter will begin in the upper left corner of the grid, and address situations where the estimand (the parameter to be estimated) is \\(\\mu.\\) It will describe how we ‘estimate’ / ‘fit’ a single \\(\\mu\\) parameter from/to a finite number of observations, and how we quantify and report how far off the target our method of estimation can/might be. 2.5 SUMMARY 2.6 Exercises So far, we have only dealt with equations involving a difference and the ratio of two \\(\\mu\\) parameters. Extend the graphs and the equations (for the difference of means and the ratio of means) to the \\(\\pi\\) parameter. Use as an example the proportions of the surfaces of the Northern (reference category) and Southern hemisphere (index category) covered by water, i.e, \\(\\pi_{North}\\) and \\(\\pi_{South}.\\) Use the hypothetical values \\(\\pi_{North} = 0.65\\) and \\(\\pi_{South} = 0.75.\\) Instead of focusing on the proportions covered by water, focus on the proportions covered by land. How does the difference of the two proportions relate to the difference calculated in 1? How does the ratio of the two proportions relate to the ratio calculated in part 1? i.e., is one the reciprocal of the other? Can you think of different scale, where the ratio when the focus is land IS just the reciprocal of the ratio when the focus is water.? If you can, show that the log of the ratio when the focus is land IS just the negative of the log of the ratio when the focus is water.? Extend the graphs and the equations (for the difference of means and the ratio of means) to the \\(\\lambda\\) parameter. Use as an example the mean number of earthquakes per year in the Northern (reference category) and Southern hemisphere (index category) , i.e, \\(\\lambda_{North}\\) and \\(\\lambda_{South}.\\) Use the hypothetical values \\(\\lambda_{North} = 5.0\\) and \\(\\lambda_{South} = 7.5\\) 2.7 References Olli S. Miettinen. Theoretical epidemiology: Principles of Occurrence Research in Medicine. Wiley, New York, 1985. Chapter 1: The study of occurrence patterns in medicine. Introduction. David Clayton and Michael Hills. Statistical Models for Epidemiology. Oxford University Press, 1993. Chapter 22: Introduction to regression models. Kenneth J. Rothman. Epidemiology: An introduction. Oxford University Press, 2012. Chaper 12: Using regression models in epidemiologic analysis. Olli S. Miettinen, Johann Steurer, Albert Hofman. Clinical Research Transformed. Springer, 2019. Chapter 7: The Logistic Regression Model (The Precursors of the General Linear Model; The General Linear Model; The Generalized Linear Model; The Logistic Regression Model) "],
["inference.html", "Chapter 3 Statistical Inference 3.1 The Bayesian Approach 3.2 Frequentist approach 3.3 Does the approach matter?", " Chapter 3 Statistical Inference Google gives the following defintion The theory, methods, and practice of forming judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling. The Oxford English Dictionary defines it as The drawing of inferences about a population based on data taken from a sample of that population; an inference drawn in this way; the branch of statistics concerned with this procedure. We would add to these numerical statements about unknown (and unknowable) constants, as well as the mechanism or process that generated the limited data you got/get to observe. Some example parameters – some scientific, some more personal or particularistic – include Whether a potential hemophilia carrier is in fact a carrier a particular email is malicious a person committed the crime they are accused of a person has been infected with a certain virus The proportion of thumbtacks that land on their back when tossed your time that you are being productive the earth’s surface that is covered by water your driving time that you are on the phone your time (over the entire year) that you spend inside patients whose disease would respond to a medication people who would volunteer for a demanding survey or long-term research study The numerical value for the density of the Earth,relative to water the age of a person whom you have just met your cholesterol level the mean depth of the ocean the 20th percentile of the depths of the ocean the median age of a population To address the uncertainties involved in the judgements/inferences, some use of probabilities is required. In their preamble to their chapter on inferemce, Clayton and Hills tell us that There are two radically different approaches to associating a probability with a range of parameter values, reflecting a deep philosophical division amongst mathematicians and scientists about the nature of probability. We shall start with the more orthodox view within biomedical science. Clayton and Hills completed their book in 1993. Since then, propelled by greater computer power, and by people like Clayton’s Cambridge colleague David Spiegelhalter, whose book we will start with, the Bayesian approach to ‘associating a probability with a range of parameter values’ has become more common. It has not yet reached the status of ‘customary or conventional, as a means or method; established.’ that the dictionaries give as the meaning of orthodox. In any case, we should not take Clayton and Hills’ use of the phrase ‘more orthodox’ to describe the frequentist approach to mean that the Bayesian approach does not conform to the approved form of analysis' or is in some sensewrong.’ The first-established of the two ‘schools’ (or ‘churches’) of statistical inference** makes direct probabilistic statements about the possible parameter values. This approach goes back at least as far as the mid-1700’s essay ‘A method of calculating the exact probability of all conclusions based on induction’; ironically the author was a Presbyterian minister. The developments since then are nicely told in the very readable book The Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy by Sharon Bertsch McGrayne, and in her Microsoft lecture and her Google lecture Maybe, by calling it the more ‘orthodox’, all that Clayton and Hills mean is that the frequentist approach is more popular method today. It got a slow start, and dates from the early 20th century. (In one of our sampling exercises, we will try to determine the relative frequencies of the two approaches in the epidemiology and medical literature). Interestingly, if you use Google Books Ngram Viewer you get a different sense. Maybe this is because the majority don’t need to justify the methods they use! Frequentist statements are indirect. They are (conditional) probabilistic statements about the data and about the performance of the procedure used to bracket the parameter values. A variant on it ranks the various possible parameter values according to how probable the observed data would be under each of these, but does not make direct probabilistic statements about the parameter values themselves. Because it is indirect, conditional, the results are often interpreted incorrectly. We begin with the direct method, one that studies tell us we are born with, and use throughout our lives, both consciously and subconsciously, to continue to learn/update. When we learn a new motor skill, such as playing an approaching tennis ball, both our sensors and the task possess variability. […] We show that subjects internally represent both the statistical distribution of the task and their sensory uncertainty, combining them in a manner consistent with a performance-optimizing bayesian process. The central nervous system therefore employs probabilistic models during sensorimotor learning. Bayesian integration in sensorimotor learning leading to this New York Times headline Subconsciously, Athletes May Play Like Statisticians 3.1 The Bayesian Approach to probability statements concerning parameter values. This paragraph is taken from this chapter An Overview of the Bayesian Approach in the book Bayesian Approaches to Clinical Trials and Health-Care Evaluation by David Speigelhalter et al, describes it well: The standard interpretation of probability describes long-run properties of repeated random events (Section 2.1.1). This is known as the frequency interpretation of probability, and standard statistical methods are sometimes referred to as ‘frequentist’. In contrast, the Bayesian approach rests on an essentially ‘subjective’ interpretation of probability, which is allowed to express generic uncertainty or ‘degree of belief’ about any unknown but potentially observable quantity, whether or not it is one of a number of repeatable experiments. For example, it is quite reasonable from a subjective perspective to think of a probability of the event ‘Earth will be openly visited by aliens in the next ten years’, whereas it may be difficult to interpret this potential event as part of a ‘long-run’ series. Methods of assessing subjective probabilities and probability distributions will be discussed in Section 5.2. Section 3.1 SUBJECTIVITY AND CONTEXT emphasizes that ‘the vital point of the subjective interpretation is that Your probability for an event is a property of Your relationship to that event, and not an objective property of the event itself.’ Moreover, ‘pedantically speaking, one should always refer to probabilities for events rather than probabilities of events, and the conditioning context used in Section 2.1.1 includes the observer and all their background knowledge and assumptions.’ That there is ‘always a context’ goes along with what we read in Alan Turing’s recently de-classified essay The Applications of Probability to Cryptography. Under section 1.2 (‘Meaning of probability and odds’) he starts out I shall not attempt to give a systematic account of the theory of probability, but it may be worth while to define shortly probability and odds. The probability of an event on certain evidence is the proportion of cases in which that event may be expected to happen given that evidence. For instance if it is known the 20% of men live to the age of 70, then knowing of Hitler only Hitler is a man we can say that the probability of Hitler living to the age of 70 is 0.2. Suppose that we know that Hitler is now of age 52 the probability will be quite different, say 0.5, because 50% of men of 52 live to 70. Not all context is subjective. We will start with a context where the initial (starting out, pre-new-data) probability is objective. Just before we do, we include this passage from Clayton and Hills, in subchapter 10.2 Subjective probablity, which they denote as optional material. The second approach to the problem of assigning a probability to a range of values for a parameter is based on the philosophical position that probability is a subjective measure of ignorance. The investigator uses probability as a measure of subjective degree of belief in the different values which the parameter might take. With this view it is perfectly logical to say that there is a probability of 0.9 that the parameter lies within a stated range. Before observing the data, the investigator will have certain beliefs about the parameter value and these can be measured by a priori probabilities. Because they are subjective every scientist would be permitted to give different probabilities to different parameter values. However, the idea of scientific objectivity is not completely rejected. In this approach objectivity lies in the rule used to modify the a priori probabilities in the light of the data from the study. This is Bayes’ rule and statisticians who take this philosophical position call themselves Bayesians. Bayes’ rule was described in Chapter 2, where it was used to calcu- late the probabilities of exposure given outcome from the probabilities of outcome given exposure. Once we are prepared to assign probabilities to parameter values, Bayes’ rule can be used to calculate the probability of each value of a parameter (\\(\\theta\\)) given the data, from the probability of the data given the value of the parameter. The argument is illustrated by two tree diagrams. Fig. 10.2 illustrates the direction in which probabilities are specified in the statistical model — given the choice of the value of the parameter, \\(\\theta\\), the model tells us the probability of the data. The probability of any particular combination of data and parameter value is then the product of the probability of the parameter value and the probability of data given the parameter value. In this product, the first term, Pr(\\(\\theta\\)), represents the a priori degree of belief for the value of \\(\\theta\\) and the second term, Pr(Data | \\(\\theta\\) ), is the likelihood. Fig. 10.3 reverses the conditioning argument, and expresses the joint probabilityas the product of the overall probability of the data multiplied by the probability of the parameter given the data. This latter term, Pr( \\(\\theta\\) | Data), represents the posterior degree of belief in the parameter value once the data have been observed. Since the joint probability of data and parameter value is the same no matter which way we argue, so that \\[Pr(\\theta) \\times Pr(Data | \\theta) = Pr(Data) \\times Pr(\\theta | Data),\\] so that \\[Pr(\\theta | Data) = \\frac{Pr(\\theta) \\times Pr(Data | \\theta)}{Pr(Data)}\\] Thus elementary probability theory tells us how prior beliefs about the value of a parameter should be modified after the observation of data. Their 2 figures nicely show that the difference is in the directionality or conditioning, i.e. is the object \\[Pr(\\theta | data) \\ \\ \\ OR \\ \\ \\ Pr(data | \\theta) \\ \\ ?\\] Figure 3.1: From Chapter 10.2 of Clayton and Hills Without getting into the details of the calculations, we will apply this approach to the first example in each of the parameter genres listed above. The point is to illustrate how direct and unambigous the answer is in each case. 3.1.1 Example: parameter is 2-valued: yes or no In the first genre, the parameter is personal or particular. In each of the examples, the true state is binary. The potential hemophilia carrier is a hemophia carrier or is not; the particular email is either malicious or is not; the person in question either committed the crime ot did not. So, there are just two possible parameter values: yes or no, is or is not. From the outset, just like in Turing’s example, there is a given context. For example, suppose a woman’s brother is known to have haemophilia. hemophilia: a medical condition in which the ability of the blood to clot is severely reduced, causing the sufferer to bleed severely from even a slight injury. The condition is typically caused by a hereditary lack of a coagulation factor, caused by a mutation in one of the genes located on the X chromosome. - Google Just knowing this, the probability that the woman is a hemophilia carrier is 50% or 1/2. Today, genetic testing of the carrier can help determine whether the woman is a carrier. But when JH first taught 607, the only time to learn more about her carrier status (and move her probability to 1, or towards 0) was after the births of her sons: their status was knowable virtually immedediately. If it is determined that the first son has hemophilia, it establishes that she IS a carrier, thereby moving the probability up to 1. If he was not, it moves the probability down to 1/3: in other words, among ‘women like her’, i.e, other potential carriers who also have had 1 son who turned out to be Normal (NL), 1/3 of the sons are the sons of carriers, and 2/3 are the sons of non-carriers. The continued updating as the women with a NL son gave birth to a second son, and so on, is shown in the diagram below, with C used as shorthand for ‘Is a Carrier.’ Technically speaking, each sequential P[C] should indicate that it is ‘conditioned on’ – and thus reflects the information in – the history up to that point. In other words, the 1/5 probability refers to P[C | both sons are NL], where “|” stands for ‘given that’, or – to use Turing’s phrase – ‘on the evidence that’. Figure 3.2: At the outset, each woman had a 50:50 chance of being a haemophilia carrier. Accumulating information from the hemophilia status of the sons increasingly ‘sorts’ or segregates the women by moving their probabilities of being a carrier TO 1 (100%) or FURTHER TOWARDS 0 (0%). It ‘updates’ the probablity of being a carrier, P[C]. For brevity, the ‘| data’ in each P[C | data] is omitted. ASIDE: This is similar to how researchers develop strains of “transgenic” mice, by introducing an altered gene (transgene) into the genome. In order to breed true, theanimals must be made to be homozygous, i.e., to have two copies of the introduced gene (+ +). Molecular biology techniques can detect whether the transgene is present in an individual animal (without having to sacrifice the animal), but cannot distinguish a hemizygote, with one copy of the gene (+ -), from a homozygote (+ +). This difference can only be detected by breeding strategies. First generations: A copy of the transgene is injected into the pronucleus of a newly fertilized ovum, prior to fusion with the male pronucleus. Thus all animals that develop from these zygotes can have at most one copy of the gene, from the ovum. After birth, screening is performed to detect these ‘positive’ animals, called founders. After sexual maturation, all founders are bred to normal ‘wild type’ (WT) animals, to ensure that the transgene has been incorporated in such a way as to be heritable. Pairs of positive (hemizygous) animals in this F1 generation are then bred to each other. By Mendelian genetics, the distri- bution of F2 offspring should be 1:2:1, homozygous transgenic : hemizygous transgenic : homozygous normal. The homozygous normal animals are not used. The question is, how to tell the homozygous transgenic mice (the desired ones) from the hemizygous transgenic ones? Note that the mix in this reduced population is 1 homozygous transgenic to 2 hemizygous transgenic. F2 breeding: All ’positive’ F2 animals (i.e. all homozygous and hemizygous animals) are bred to wild type. Possible F3 genotypes are as follows: (by Mendelian genetics) Hemizygous (which comprise 2/3 of the F2 animals used) x wild type = 50:50, hemizygous (and therefore ‘positive’) : normal (and therefore ‘negative’), Homozygous (which comprise 1/3 of the F2 animals used) x wild type = all hemizygous (and therefore ‘positive’). That is, while only half of the offspring from a Hemi x WT pair will be ‘positive’ when screened, all of the offspring of a Homo x WT pair will be ‘positive’. The question: How many F3 offspring from a particular pairing does the researcher have to screen before declaring the positive parent as homozygous? Note: as soon as an offspring is screened as ‘negative,’ one knows the parent must have been hemizygous. A variant on the above diagram can help withe probabilities. Furthe details are avilable on the bios601 website, in the ‘probability’ chapter. Before moving on the the next type of parameter, a few points In both the hemophilia and transgenic mice examples, the ‘starting’ probability is objective and the post-data probabilities have a ‘long-run’ or ‘in large numbers of similar instances’ interpretation. One could make a diagram that shows the expected numbers ‘in every 100 women like this.’ There is nothing special about the ‘starting out’ probability P[C] of 0.5. Before a pregnancy test, or a pre-natal diagnostic test, for example, the probability of the target Condition/state of interest/concern would be a function of many other factors, and could in theory take on any value between 0 and 1. The (starting out, pre-filter) proportion of malicious emails would depend on which of a person’s email accounts it was. In the language of diagnostic tests, each ‘Son as a test of the mother’s carrier status’ has 50% sensitivity and 100% ‘specificity’. For sensitivity, this puts it on par with the Pap test for cervical cancer: the main problem withe latter is in the sampling. For specificity, it is better than most tests. The shiny app From Pre-test to Post-test Probabilities shows how the initial average (pre-test) probability is segregated into 2 post-test probabilities by the 2 possible test results, and the role of the 2 error-probabilities (just about all tests are fallible) in how well they push them out. The ‘starting out’ probability could be subjective. For example it could be one’s impression (before getting to see up close how wrinkled their face is) as whether the person is a smoker, or one’s assessment of the probability that the accused is guilty (before getting to hear the DNA expert, or lie detection report) based on how credible the accused appears to be, and all of the other evidence to date. The main point is that we are merging (adding) two sources of information. 3.1.2 Example: parameter is a proportion In theory, in this genre, the true parameter value could in theory lie anywhere between 0 and 1, But again, just like in Turing’s example, we seldom start from complete ignorance, or with – in the title of pscychologist Stephen Pinker’s book – a blank slate. Even if you have never seen thumbtacks tossed onto on a surface, you can reason informally, and indicate what proportions are unlikely and likely, and where along the (0,1) scale you would ‘put your money’. You could do the same when asked what proportions of your time that you are being productive, or on the phone, or sedentary, or indoors. Mind you, you might be ‘way off’ with your claims, but the nice thing is that — and this is the point of this course – you can generate data to narrow down the true proportion. The other nice thing with the Bayesian approach in particular is that – no matter whether you believe the proportion is low or medium or high, we can work out what your post-data beliefs should be. It is a matter of mathematics. If, before collecting any new data, we have ‘no idea’ – a common phrase among todays’s generation, one that, if it is uttered with empahsis on the ‘no’, irks JH to no end – what the true parameter value is, that is easily handled. Moreover, enough valid data will (or should!) trump the pre-data beliefs. On a side note: Dick Pound, a former chancellor of McGill University, and first president of the World Anti-Doping Agency is a staunch advocate of strict drug testing for athletes. Discussing the National Hockey League in November 2005, Pound said, ‘you wouldn’t be far wrong if you said a third of hockey players are gaining some pharmaceutical assistance.’ Pound would later admit that he completely invented the figure. Both the NHL and NHLPA have denied the claims, demanding Pound provide evidence rather than make what they term unsubstantiated claims. Since his comments were made, some NHL players have tested positive for banned substances, including Bryan Berard, José Théodore, and two of 250 players involved in Olympic testing. As of June 2006, there had been 1,406 tests in the program jointly administered by the league and the union, and none has come up with banned substances under NHL rules. Pound remained skeptical, claiming the NHL rules were too lax and unclear, as they do not test for some banned substance, including certain stimulants. In an interview with hockey blogger, B. D. Gallof, of Hockeybuzz on December 19, 2007, Pound was asked to expand on the 30% comment and subsequent reaction, expounded that stimulants was ‘the NHL’s drug of choice’. He also cited that the NHL will have no credibility on a drug policy if it, and other sports, continue to run things ‘in-house’. https://en.wikipedia.org/wiki/Dick_Pound and https://www.cbc.ca/sports/hockey/dick-pound-slams-nhl-s-drug-policy-1.557993 Even before studying/asking them, investigators would have some sense of the proportions of patients whose disease would respond to a medication, or people who would volunteer for a survey or research study. These iimpressions would probably be based on previous analogous situations, and the ‘literature’, but would vary from pundit to pundit. But ultimately, they could be much improved and narrowed (and even replaced entirely) by new-data-based ones. The proportion of the earth’s surface that is covered by water is easy to determine: just look up a reputable source. But what if you weren’t able to, but did have access to the database of 933 million recordings in the SRTM30PLUS database. It has altitude/depth measurements for 43,200 x 21,600 = 933,120,000 locations. This database is so large that you would have to sample from it. From a thousand randomly chosen loactions, you would be able to ‘trust’ the first decimal in your estimate; from a million you should be able to trust the second – and maybe the third. Since we already know/remember from high school ‘roughly’ what the proportion is, we will leave it for an exercsie in another chapter. In this chapter, following the advice of master-teacher Fred Mosteller, we use examples where the correct answer is not known with any precision. The proportion of these we probably know the least about is the thumbtack one. However, it has fewer personal benefits than knowing what proportion of your time you are being productive. Moreover, we have a nice written account of how you might go about learning this personal proportion. In his book Elementary Bayesian Statistics, Gordon Antelman informally introduces and illustrate a Bayesian analysis of an uncertain proportion with a slightly modified version of a novel and useful application of work sampling discussed by Fuller (1985). We have changed his notation for the proportion of your time spent in productive work, and called it \\(\\pi\\), and also modified some of his words. Suppose you, as a good up-to-date manager practicing continuous quality and productivity improvement, have some ideas on improving your own productivity. To see if these ideas have any merit, you would like to compare some ‘before’ measure of productivity with a comparable ‘after’ measure of productivity. For now — we shall come back to this example several times — let us focus on just a ‘before’ measure. The measure to be used is the proportion of your time spent in productive work, call it \\(\\pi\\), as opposed to time spent doing something that would not have needed doing if things had been done right the first time. Examples of the latter might include searching for a misplaced document, recreating a deleted computer file, following up on a customer’s complaint, or waiting past a scheduled time for a meeting to start. [Today, we would add being on social media, or browsing the web for non-work-related matters] Rather than saying \\(\\pi\\) is not (precisely) ‘known’, it is better to say that ‘\\(\\pi\\) is uncertain’; from your job experience, you would really know quite a lot about p. For example, you might be almost certain that it is greater than 0.50, less than 0.90, and you might assess your odds that \\(\\pi\\) is between, say, 0.60 and 0.80 to be about 9 to 1. A precise statement of these beliefs will be your prior distribution for \\(\\pi\\). You would probably feel uncomfortable — most people do — about assessing this prior distribution, especially since there are an infinite number of states; viz., all of the values between zero and one. But, without any real loss, you can bypass the infinite-number problem by rounding the values of \\(\\pi\\) to the nearest 5% or 10%, making the problem discrete. Then you have a contemplatable Bayes’ theorem, like those discussed in Chapter 4, with the finitely many \\(\\pi\\)-values as the possible “states”. (When we reconsider this example later in this chapter, you will see that, with a little theory, the infinite number of \\(\\pi\\)-values can almost always be handled very neatly and more easily.) PRIOR BELIEFS For illustration, he supposes you choose just five possible values for \\(\\pi\\), and assess your prior distribution. Since R does not allow Greek symbols, we will refer to it by uppercase P and assess your prior distribution. This possible prior distribution, shown below, would reflect, for example, that your judgment is that there is only about one chance in 20 that \\(\\pi\\) rounds to 0.50, about one chance in 20 that \\(\\pi\\) rounds to 0.90, about one chance in four that it rounds to 0.60, a little more than one chance in three that it rounds to 0.70, and a little less than one chance in three that it rounds to 0.80. P = c(0.50, 0.60, 0.70, 0.80, 0.90) PriorProbForP = c(0.05, 0.25, 0.35, 0.30, 0.05) par(mfrow=c(1,1),mar = c(5,5,1,1)) plot(P, PriorProbForP, type=&quot;h&quot;, xlim=c(0,1), lwd=20, ylim=c(0,0.5), lend=1, col=&quot;grey80&quot;, xlab=&quot;P: Proportion of time that I am being productive&quot;, ylab=&quot;My probability that P rounds to... &quot;, cex.lab = 1.25, cex.axis = 1.25) text(0.5,0.25, &quot;Prior Probabilities&quot;, font=2, col=&quot;grey75&quot;, adj=c(1,1), cex=1.75) Figure 3.3: Prior Probabilities for the parameter P, the proportion of time that I am being productive. DATA: Suppose you are fitted with a beeper set to beep at random times; when the beeper beeps, you classify the task being worked on as W — for ‘productive Work’, or F — for ‘Fixing’ (or today we mght say ‘Fiddling’ or ‘Fooling around’ or wasting time). Although we will skip the technicalities, it is important that the experiment be designed so that the trials are independent. Beeps should be unpredictable so you do not arrange, possibly subconsciously, to be doing productive work at the beep. They probably also should not be too close together to make the independence assumption more reasonable. Suppose the first four trials give the data \\(F_1, F_2, W_3\\), and \\(F_4\\). Below is a picture showing the effects of the data FFWF on the prior distribution. Three F’s in four trials increase your probabilities for the two smaller values of P and decrease your probabilities for the three larger ones. Lik = (1-P)^3 * P PosteriorProbForP = (PriorProbForP * Lik) / sum(PriorProbForP * Lik) par(mfrow=c(1,1),mar = c(5,5,1,1)) plot(P, PriorProbForP, type=&quot;h&quot;, xlim=c(0,1), lwd=20, ylim=c(0,0.5), lend=1, col=&quot;grey80&quot;, xlab=&quot;P: Proportion of time that I am being productive&quot;, ylab=&quot;My probability that P rounds to... &quot;, cex.lab = 1.25, cex.axis = 1.25) text(0.5,0.25, &quot;Prior Probabilities&quot;, font=2, col=&quot;grey75&quot;, adj=c(1,1), cex=1.75) lines(P, PosteriorProbForP, type=&quot;h&quot;, lwd=6, col=&quot;red&quot;,lend=1) text(0.5,0.4, &quot;Posterior Probabilities\\nafter observing 1W, 3F&quot;, font=2, col=&quot;red&quot;, adj=c(1,1), cex=1.75) Figure 3.4: Prior Probabilities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior probabilities, after observing that in n = 4 randomly sampled occasions, I was actually productive in only 1 of the 4. The sample alone most strongly supports a value for P of 0.25 (one W in four trials); had the prior included a value of P of 0.25, the (relative) increase in going from prior to posterior would have been greatest for that value. For the assumed prior, in which only p’s of 0.50, 0.60, 0.70, 0.80, or 0.90 are considered, the sample evidence FFWF in favor of a P near 0.25 can only push up the posterior probabilities for the nearest possible values - 0.50 and 0.60. (The seemingly harder consideration of all possible p’s between zero and one will handle this kind of situation more logically.) Below we show the ‘continuous P ’ version Antelman refers to. To make this, we calculated the mean and variance of his discrete (5-point) prior distribution, and converted them to the 2 parameters, \\(a\\) and \\(b\\), of the beta distribution with the same mean and variance. Conveniently, the posterior density is also a beta distribution, but with parameters \\(a+1\\) and \\(b+3\\). mean.P.prior = sum( P* PriorProbForP ) var.P.prior = sum( (P-mean.P.prior)^2 * PriorProbForP ) a.plus.b = mean.P.prior * (1-mean.P.prior) / var.P.prior - 1 a = mean.P.prior * a.plus.b b = (1-mean.P.prior) * a.plus.b P = seq(0,1,0.01) prior.density = dbeta(P,a,b) posterior.density = dbeta(P,a+1,b+3) Figure 3.5: Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 4 randomly sampled occasions, I was actually productive (W) in only 1 of the 4. Before moving on the the next type of parameter, a few points: The beta distribution nicely shows how the prior information/impression and the new data get combined. The \\(a\\) and \\(b\\) parameters of the prior distribution are 14.8 and 6.2. Together, they determine the mean, \\(a/(a=b)\\), the median, the mode, \\((a-1)/(a+b-2)\\), and the variance, \\(a^2b^2/(a+b+1)^2\\) of the prior distribution. Their conterparts in the data-likelihood are 1 and 3. The ‘\\(a\\)’ and ‘\\(b\\)’ parameters of the posterior distribution are 15.8 and 9.2: the \\(a\\)’s add, and the \\(b\\)’s add. In other words, the distribution of one’s pre-data beliefs is the distribution one would have after ‘seeing’ 14.8 W’s and 6.2 F’s; the distribution of one’s post-data beliefs is the distribution one would have after ‘seeing’ 15.8 W’s and 9.2 F’s. The (synthetic) experience-equivalent of the numbers of Ws and F’s in the prior are added to the actual (observed) numbers of Ws and F’s in the data to arrive at the (new) posterior distribution. You are probably wondering what the posterior distribution would look like with more data. Here is what it would look like after observing 7 out of 25 or 13 out of 25. The modes of the posterior distributions are still somewhat influenced by the prior – as they are still well above P=7/25 = 0.28 and P=13/25 = 0.52. If the data were 70/250 or 13/250, the modes would be closer to P= 0.28 or P = 0.52; in other words, the data would ‘swamp’ – or ‘trump’ – the prior. Figure 3.6: Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 25 randomly sampled occasions, I was actually productive (W) in only 7 of the 25, or 13 of the 25. Be thinking about your prior for the proportion of thumbtacks that land on their back, and the proportion of the Earth’s surface that is covered by water, or [these words written on March 30, 2020, before any trial data] the proportion of patients with mild symptoms of covid-19 who would benefit from chloroquine. Think about how you might elicit a prior distribution. You might want to Google ‘tools for eliciting prior distributions’ – or consult Chapter 5 of Spiegelhalter’s book. We now move on to last parameter genre we will consider. 3.1.3 Example: parameter is a personal number or population mean We will start with a discrete version of a commonly-wondered-about parameter, the age of a person whom you have just met, or seen a photo of. We will then go on to a full numerical example, your average cholesterol or blood pressure level. Example 1 How old (or what age – if you prefer to avoid speaking of ‘old’) do you think this IBC2006 attendee was when this photo was taken? This is the parameter of interest. Figure 3.7: An attendee at the International Biometrics Conference, held at McGill in July 2006 He tells you he got his PhD 32 years earlier. Based on the distribution of ages at which people get their PhD (shown in grey below), that puts his current age somewhere in the blue distribution. Figure 3.8: Current ages of persons who obtained PhD 32 years earlier. This is a somewhat unusual example, as the blue distribution is very wide – partly because we could not find age-at-graduation data specific to PhD graduates in statistics in 1974. We suspect that that specific distribution is a good deal narrower than the one shown. In the next chapter, you will see that other indirect measures of age are a good bit tighter than this. Nevertheless, it emphasizes that, depending on what impression you got form the photo alone, you may now wish to revise your estimate of the person’s age. We suspect that many of you would have initially though he looks like he was in mid 50s, and so would have made guesses like those shown in red in the top panel. If you are one of those, then you will want to revise the age upwards, as we do in the green in the bottom panel. Figure 3.9: If he looked like he was in mid 50s If you initially thought he looked like he was ‘around 60’ you would have made guesses like those shown in red in the top panel. If you did, then you will want to revise upwards a little, as is shown in green, and now put hime him somewhere around 60, or a bit above. Figure 3.10: If If he looked like he was around 60. If you initially thought he looked like he was ‘in his mid 60s’ your estimate is more in line with the age-at-PhD data, and so you would not revise as much. You might bet a bit more on the ‘around 60’ age-bracket. Figure 3.11: If If he looked like he was in his mid 60s. As we noted, the PhD data have too much of a right tail, and so it is driving up the estimates. If you are now curious as how keen your ‘age-estimation’ skills are, here is a link to the Google Scholar page of the statistician whose age we have been trying to determine. Age estimation via face images (image-based age estimation) is a growing research area, with many possible applications. We now describe 2 more classical examples Example 2 Spiegelhalter et al. address this in their Example 3.4: ‘Suppose we are interested in the long-term systolic blood pressure (SBP) in mmHg of a particular 60-year-old female.’ We take two independent readings 6 weeks apart, and their mean is 130. We know that SBP is measured with a standard deviation \\(\\sigma = 5.\\) What should we estimate her SBP to be? They then go on to give the frequentist (‘standard’, ‘orthodox’) 95% confidence interval, of 123.1 to 136.9, centered on the measured value of 130 [we will come later to how they calculated this]. They continue, … However, we may have considerable additional information about SBPs which we can express as a prior distribution. Suppose that a survey in the same population revealed that females aged 60 had a mean long-term SBP of 120 with standard deviation 10. This population distribution can be considered as a prior distribution for the specific individual. The posterior distribution, computed from the combination of the 130 measured on the woman, and the prior, is centered on 128.9 and the 95% interval is 122.4 to 135.4. This posterior distribution reveals some ‘shrinkage’ towards the population mean, and a small increase in precision from not using the data alone. Intuitively, we can say that the woman has somewhat higher measurements than we would expect for someone her age, and hence we slightly adjust our estimate to allow for the possibility that her two measureshappened by chance to be on the high side. As additional measures are made, this possibility becomes less plausible and the prior knowledge will be systematically downgraded. Before going on to example 3, we emphasize that the 128.9 is a compromize between the persnal mean of 130, and the (prior) poulation mean of 120: it is a weighted average, with (relative) weights that are the reciprocals of the squares of the 2 standard deviations, the reciprocals of \\(\\frac{5^2}{2}\\) and \\(10^2,\\) i.e., \\(\\frac{2}{25} = 0.08\\) and \\(\\frac{1}{10^2} = 0.01.\\) The 128.9 is 8/9ths closer to the measured 130 than it is to the population mean of 120. It is slightly ‘shrunk’ toawards the population. This is why physicians might not believe that the 130 is correct, and might ask for more measurements before putting this woman on blood pressure-lowering drugs. [The already cited ‘Bayesian integration in sensorimotor learning’ illustrates how as humans, we automatically combine estimates of different precisions into one more precise estimate, and do so using the same mathematical laws that are used in the Bayesian approach!] Example 3 One article that does go into some detail about a similar situation is the very nice medically-useful – and didactic article Estimating an Individual’s True Cholesterol Level and Response to Intervention by Les Irwig, Paul Glasziou, Andrew Wilson and Petra Macaskill. It begins with a single measurement, before dealing with an average of several measurements on the same person. It also gives separate charts for persons of different ages, and deals not just with point and interval estimates, but also derives probability statements for the possibility that the person’s true cholesterol is above some threshold that should trigger intervention. The appendix is a nice tutorial for combining information. Their abstract begins: An individual’s blood cholesterol measurement may differ from the true level because of short-term biological and technical measurement variability. Using data on the within-individual and population variance of serum cholesterol, we addressed the following clinical concerns: Given a cholesterol measurement, what is the individual’s likely true level? The confidence interval for the true level is wide and asymmetrical around extreme measurements because of regres-sion to the mean. Of particular concern is the misclassification of people with a screening measurement below 5.2 mmol/L who may be advised that their cholesterol level is ‘desirable’ when their true level warrants further action. The first half of the paper, which deals with two related topics, (a) Estimating the True Cholesterol Level, and (b) assesing the Probability of Misclassification shows the primary elements, and these notes will focus on the highlights. [after these, extensive excerpts will be included] The results for (a) and (b) were presented as 2 Figures. The first gave the (posterior) credible interval for a person’s true cholesterol level based on either 1, or an average of 3, measurements, using on the horizontal axis the measured value, and on the vertical one the point and 95% credible interval. Using a graph (rather than a formula) allows the clinician to use it for all possible ‘what if’s. Below, we will illustrate it using one specific example, a person whose measured value was 7.15. The second uses the (posterior) credible interval to calculate the probability that someone with a specific measured value has a true level that is above a certain threshold level used in treatment guidelines. Thus, the key tool is the posterior distribution itself, and so we give the statistical basis for this. Reasons to take a Bayesian approach The reason this problem arises in the first place is because of short term biological variability in the quantity of interest in the person in question. If we were measuring a person’s height, we could do so carefully at just one time-point: it would not be different a week or month from now; it remains quite stable over several years. [it does vary slightly over the day, but, be keep it simple, we could speak of one’s height at mid-day]. The same is not the case for a person’s cholesterol level: even if we measured it very carefully at one time, it would be genuinely different a week or month later, even in the absence of any intervention of lifestyle change. (The same applies, more strikingly, for other blood levels such as C-reactive protein (CRP), which is a marker of inflammation). Thus any single measurement, or any average of a finite number of determinations, is imprecise. So what’s new? Don’t we meet this issue all the time in statistics? The point of Irwig’s article is that we should not rely solely on the estimate based on the person’s measurements, but rather should combine it with an estimate based on outside information. The same reasoning is at work when a physician repeats a measurement that seems extreme. In so doing, (s)he is not relying only on the point or interval estimate provided by the measurement itself: rather (s)he is also using knowledge of how this measurement behaves in other similar persons! And what we know about others, even if collectively, can help us with an individual. All of the technical details are available in these notes, prepared for bios601 Here we will skip to the ‘botom line’ diagrams. Figure 3.12: A Person’s estimated true cholesterol level from one measurement for a population of individuals where the mean is 5.2 mmol/L (i.e, at the time of the article, men less than 35 and women less than 45 years old). The thicker solid line indicates the estimated true level, the thinner solid lines are 80% confidence intervals. The dotted diagonal line is an equivalence line. The arrow shows the amount by which a single measurement of 7.15 mmol/L is shrunk towards the population mean, to a rcorrected value of 6.8 mmol/L. Below, we show the detailed calculations for a person with a single measurement of 7.15 mmol/L. For now, just focus on the items shown in red (the prior distribution for \\(\\theta\\), blue (the 7.15 for the individual, and the associated likelihood function) and green (the posterior distribution of \\(\\theta\\)). All the calculations are on the log scale. Figure 3.13: Worked example, for a person with a measured value of 7.15 3.2 Frequentist approach 2 separate issues Ho and intervals (treated together in Bayesian) • Statistical Quality Control procedures [for Decisions] • Sample survey organizations: Confidence intervals • Statistical Tests of Hypotheses Unlike Bayesian inference, there is no quantified pre-test or pre- data “impression”; the ultimate statements are about data, conditional on an assumed null or other hypothesis. Thus, an explanation of a p-value must start with the conditional “IF the parameter is … the probability that the data woul 3.3 Does the approach matter? semantics "],
["CI.html", "Chapter 4 Parameter Intervals 4.1 ‘100% confidence’ intervals 4.2 More-nuanced intervals 4.3 SUMMARY", " Chapter 4 Parameter Intervals The objective of this first section is to provide simple examples of reverse engineering that show some of the logic behind statistical ‘confidence intervals’ for parameters. We begin with ‘100% confidence’ intervals, and then, in section 2, we explain why we have to move to ‘less-than-100% confidence’ intervals, where things get a bit more nuanced. In both sections, we emphasize the reverse engineering, i.e, by using as our limits the worst-case or almost-worst-case scenarios involving the (unknown) values of the parameter that is being estimated. 4.1 ‘100% confidence’ intervals Example 1 Consider a very ‘particularistic’ parameter, the height of a particular building. There is nothing ‘scientific’ about the parameter, except maybe that we use tools of mathematical science (of trignometry) to measure it. Nevertheless, we will sometimes refer to it one of the generic symbols for a parameter, namely \\(\\theta.\\) Suppose you measure the height of this building by standing a known horizontal distance (e.g. 100 metres) from the bottom of the building and using an instrument to measure the angle between the horizontal and the top of the building. Suppose, as shown in the left panel of the Figure below, that the instrument gives a reading of 70 degrees. Remembering from trigonometry that the tangent of a 70 degree angles is 2.75, the angle of 70 degrees suggests that the height of the building is \\(\\hat{\\theta}\\) = 275 metres. The ‘hat’ is statistical shorthand for ‘estimate of.’ Since it is sometimes referred to as a ‘point estimate’ of \\(\\theta\\), we display the value using a dot or point. After calculating this, you learn that the measuring instrument only displays the angle is to the nearest 10 degrees. This means that the true angle is somewhere between 65 and 75 degrees. [This is the same range you would get if it was dark and you used a laser pointer or flashlight attached to a wheel that rotates in fixed 10-degree steps, i.e., 5 degrees, 15 degrees, 25 degrees, etc. At 65 degrees, the light is visible on the building, but at 75 degrees, it goes above the building and shines into the sky.] So you cannot say that the true height is exactly 275 metres. What can you say? And with what certainty? You can put limits on the true height by asking what are the minimum and maximum heights that could have produced the observed reading of 70 degrees? To do this you need to take the limits one at a time. The minimum angle that could have given the (rounded) readout of 70 degrees is 65 degrees, and this corresponds to a minimum height (lower limit) height of \\(\\theta_L\\) = 214 metres. The maximum angle that could have given the readout of 70 degrees is 75 degrees, and this corresponds to a maximum height (upper limit) of \\(\\theta_U\\) = 373 metres. Thus, assuming that the instrument is measuring the angle correctly, and then doing what you are told it does, you are 100% confident that the true height lies in the interval (214, 373). As is clear in the graph, this does not have the typical 275 \\(\\pm\\) a single-number (or in sybols, \\(\\hat{\\theta} \\pm\\) ME [‘margin of error’]) that we typically see in reports. Figure 4.1: Estimating the height of an building by measuring subtended angles. The ‘70’ in the left panel signifies that the real angle was somewhere between 65 and 75 degrees; thus the real height lies between the L and U limits of 214 and 373 metres. In the righ panel, the interval shown by the thicker black segment to the right of the 3 individual intervals is the set of parameter values common to all 3. More data The panel on the right shows how, by obtaining 3 measurements at 3 different distances, and finding the interval they have in common (the overlap), you can narrow the interval within which the true height must lie. What allows us to be 100% confident in the parameter interval The reason is the limited error range. How wide the error range is, and how many measurements one makes, determine how wide the parameter interval is. Example 2 This one is less artificial, and indeed is motivated by a real court case in the late 1990s in Quebec, where a defendant’s age (which would determine wheter he was tried in an adult or a juvenile court) was in doubt. He was adopted, while still a young child, from another country. Official birth records were not available, and his adoptive parents were able to get a cheaper airfare by claiming that he was under age 2 at the time. Bone age, and Tanner Staging, also known as Sexual Maturity Rating (SMR), an objective classification system used to track the development and sequence of secondary sex characteristics of children during puberty, were other pieces of information used by the judge. For more on this topic of determining chronological age, see this article, entitled Many applications for medical statistics and thos one, entitled People smugglers, statistics and bone age, by UCL statistics professor and child growth expert, Tim Cole. Again, the person’s correct chronological age is a particularistic parameter, one that had nothing to do with science, or universal laws of Nature. But it can be estimated by using the laws of mathematics and statistics. For didactic purposes, we will simplify matters, and assume that ‘our’ indirect method gives estimates such that if many of them were averaged, they would give the correct chronological age of the person (in statistical lingo, statisticians say that the method/estimator is ‘unbiased’). However, as is seen below, the individual measurements vary quite a bit around the correct age. They can be off by as much as 25% (1/4th) in either direction. [In practice, a measuring instrument with this much measurement error would not be useful – unless it was fast, safe, inexpensive, non-invasive, easily repetaed, and so on – but we make the measurement variations this large just so we can see the patters more clearly on the graph!]. Another unrealistic feature of our ‘measurement model’ is that the ‘error distribution’ has a finite range. The shape of the error distribution doesn’t come into the 100% ‘confidence intervals’ below, but it will matter a little bit – but not a whole lot unless the sample size is small – later on when we cut corners. Consider first a single indirect measurement of chronological age, that yielded a value of 17.6 years. Given what you know about the sizes of the possible errors, you cannot say that the true age is exactly 17.6 years What can you say? And with what certainty? You can put limits on the true age by asking what are the minimum and maximum ages that could have produced the observed reading of 17.6 years. To do this you need to consider the limits one scenario at a time. The minimum age that could have given the estimate of 17.6 years is / 1.125 = 15.7 years. The maximum age that could have produced this reading is 17.6 / 0.875 = 20.1 years. Thus (assuming the error model is correct!) you are 100% confident that the true age lies in the interval (15.7 , 20.1) years. Again, as is clear in the graph, this does not have the typical 17.6 \\(\\pm\\) a single-number margin of error that we typically see in reports. Rather, it is 17.6 - 2.6 and 17.6 + 4.4 ! But, you can’t arrive at these directly; get there this way. You have to try on various limits, until \\[ LowerLimit + margin = 17.6 \\ = \\ UpperLimit - margin \\] Figure 4.2: 100% Confidence Intervals for a person’s chronological age when error distributions (that in this example are wider at the older ages) are 100% confined within the shaded ranges. Left: based on n = 1 measurement; right: based on n = 4 independent measurements. More data The panel on the right shows how, by obtaining 4 independent measurements, and finding the interval they have in common, you can narrow the interval containing the true age. Can we narrow the interval more, maybe by first averaging the 4 measurements? Should the mean of 4 measurements give us more information, ie., a tighter interval, that the one based on the overlap? The sad fact is that, as long we insist on 100% confidence in our interval (or our procedure), we can not: the mean of the 3 measurements can still – theoretically – be anywhere in the same 0.75 \\(\\times\\) True Age to 1.25 \\(\\times\\) True Age range – just as a single measurement can. The only way to narrow the interval is to take a chance, cut corners, and accept a lower confidence level. To do this, we need to know a bit more about where the pattern (shape) of the error distribution (**up to now we didn’t use the shape, just the boundaries). In other words, we need to know how much of the error distribution is in the corners, so that we can cut them! In the next section, we will stick for now with Daniel Bernoulli’s error distribution, but cut some corners. (Later on, we will cut some corners on Laplace’s and Gauss’s error distributions, but with the same standard deviation as in Bernoulli’s error curve.) 4.2 More-nuanced intervals We will cut 5% from each corner of the distribution, and focus on the middlemost 90%. From the formula for its mathematical shape, we can calculate that this measurement range is from -1 \\(\\times\\) the radius of the semi-ellipse to +1 \\(\\times\\) the radius. There is only a 5% probability of observing a measurement below (to the left of) this interval, and a 5% probability of observing a measurement above (to the right of) the interval. After we observe our single measurement, we ‘try it on’ against all possible true-age-scenarios. We retain only those true-age-scenarios in which the observed measurement would fall within this central (90%) range. We discard (‘rule out’) those age scenarios in which the measurement would be at one extreme or the other extreme, in one of the two excluded or ‘cut’ corners. The left panel shows the (now narrower, and more nuanced) range of true-ages (rahe of parameter values) that is compatible with the observed measurement of 13.1 years. In all other age-scenarios, the 13.1 would have been too extreme, and so these scenarios are discarded. We can think of the ‘ruled in’ range as our (nuanced, compromise) parameter interval. Note again the method of constructing this non-symmetric parameter interval, namely one boundary at a time. It does not fit the \\(\\pm\\) mold. It does, however, give a way of talk about such an interval: The observed measurement (point-estimate) may be an underestimate of the parameter: it might have fallen short of the true parameter value. Or, it may be an underestimate: it might have overshot the true parameter value. The plus and the minus amounts are the almost-maximal amounts by which our shot might have been off-target. (as we will see later, the maximal error can be infinite, so we have to put some probalistic limits on the error if we are to narrow the interval). Q: Does this procedure for constructing intervals have a 90% success rate, if used up and down all of the ages, say from 10 to 30 years? We could try it out with people of known ages. [answer by simulation] You will discover in your simulations that it might matter whether you simulate the same number of 16 year olds as 10 years, i.e., what the mix of real ages is. This does not matter in the 100% intervals, but it might if you are more nuanced. For example,instead of estimating age by an indirect method, pretend you were were estimating a person’s height indirectly, by just measuring their arm span (at each height, the mean armspan is very close to the height, but there is a spread of armspans (pardon the pun!)). And (just like in our example 2 where the spread increases with the mean) the spread of armspans is larger in people who are 6 feet tall than it is in people 5 feet tall. BUT, there aren’t as many people 5 feet and 6 feet tall as there are people 5 feet 6 inches. So, the distribution of heights in people with a span of 5 feet 11 might have a different shape than that in people with a span of 5 feet 6, or 5 feet. Simulations (or even some diagrams) could settle the issue as to whether the height-mix (or, in example 2, the age mix) matters. What is your intuition as to whether it affects the perfornace of your nuanced parameter estimates? The point is that your method needs to have the same claimed performance (say 90%) at any age you throw at it. Figure 4.3: 90% Confidence intervals for Chronological Age when only 90% of the error distributions lie within the shaded ranges. When we have \\(n = 3\\) observations (right panel), it is not so easy to say how confident we should be about the overlap of the 3 intervals. Instead, we would be bettter off taking the mean of the 3 measurements, and ‘trying on’ this single mean against the various sampling distributions of the means of 3 independent measurements from a semi-circular error distribution. Again, since the range remains the same, we would again have to cut corners. We will illustrate this when we now consider the easier-to-work-with Gaussian error distribution. A more realistic error distribution Although it makes it easier to demonstrate ‘100% confidence’ intervals, Daniel Bernoulli’s error distribution is both mathematically unfamiliar, and a bit unrealistic. And it isn’t that easy to see how to use it for situations where you wiull want to take an average of several independent indirect measurements. So, we now switch to a more familiar and more realistic bell-shaped error distribution. It is often called the Gaussian distribution, even if Gauss was not the first to publish it. Variations of it were alraedy discovered by deMoivre many decades earlier, and Gauss’ competotor, Laplace , publihed it well before Gauss did. Gauss claimed he was using it for many years before he published it. Laplace also used a different (more spiked) error distibution that today is called after him. In any case, lets switch to bell-shaped distributions of the age measurements at each true age, but let’s keep the ‘average squared deviation’ or its square root, the standard deviation, the same as it was in the Bernoulli model. on 10 Mark note NEXT we deliberately took non-symmetric situations where it is not +/- Now will show sitations where the error doesnt get bigger of smaller with the context, and where the ‘trying-on’ is faster. 4.3 SUMMARY what a parameter interval is If an error distribution is bounded, we can be 100% confident in our parameter interval, and we can narrow it by taking more measurements. Moreover, we don’t need to specify the exact shape of the error distribution. All that matters are its bounds. I don’t think you should take for granted that students (or even professors!) will know what an error distribution is. So I think a first point should be to describe what you mean by an error distribution. With unbounded error distributions, a 100% parameter interval may be unacceptably wide, even if we take many measurements. Thus, we have to ‘give up something’ (in certainty) in order to ‘get something’ (a narrower interval). Moreover, we need to either (a) specify a model for the shape of the error distribution, or (b) use data-intensive techniques, such as re-sampling, to be able to ‘cut the corners.’ Either way, a logical way to determine parameter intervals is to have them consist of all the parameter-value scenarios in which the observed measurement (or summary measurement) is ‘plausible’. The upper limit for the parameter is the scenario in which the measurement would be probalistically near the bottom of the corresponding sampling distribution; the lower limit is the scenario in which the measurement would be near the top of the corresponding sampling distribution. If the error (or sampling) distributions have differing spreads at different parameter values, then the parameter interval will not be symmetric about the point estimate. If the error (or sampling) distributions have the same spreads at different parameter values, then the parameter interval will be symmetric about the point estimate, and thus, easier to calculate. it would be good to give a concrete example here.e.g. repeat the age example –&gt; It’s harder to accurately guess the true age of someone who is older. perhaps also a good time to mention that in this situation, the +/- formula will fail you It is not correct to view the parameter as ‘falling’ on one or other side of the measurement. The true parameter values is fixed, and isn’t moving or falling anywhere. Rather, it is the observed measurement (point-estimate) that may have fallen to the left of (fallen short of), and thus provided an underestimate of, the true parameter value: Or, it may have overshot the true parameter value, and thus overestimated it. This point also explains why the +/- formula fails us Missing from the above summary points is a direct answer to the question or criticisms we are likely to face: “Why do we care about Wilson when the +/- gives you the same answer?” The answer to this question is given indirectly in points 4) and 5). But I think we need to be explicitly clear about this, since for me at least, is the main motivation for this chapter. I also think the objective of the chapter needs to be revised. Based on our discussions, the point of the chapter is to explain: 1) Why the +/- formula fails us in the interpretation of a CI and when the spread of the error distribution is non-constant and 2) Wilson’s idea to remedy the situation. If you agree with those two objectives, the next question to answer is how ‘100% confidence intervals’ and ultimately ‘cutting corners’ helps you explain the points 1) and 2). By the way, a simplified version of the Wilson plot you made will be a great help in visualizing point 3: Another unrealistic feature of our ‘measurement model’ is that the ‘error distribution’ has a semi-circular (or rather semi-eliptical) shape. In statistics, it is called the ‘Wigner’ semicircle distribution. First written about by Daniel Bernoulli, predates the error distributions of Laplace and Gauss. If the archer makes innumerable shots, all with the utmost possible care, the arrows will strike sometimes the first band next to the mark, sometimes the second, sometimes the third and so on, and this is to be understood equally of either side whether left or right. Now is it not self-evident that the hits must be assumed to be thicker and more numerous on any given band the nearer this is to the mark? If all the places on the vertical plane, whatever their distance from the mark, were equally liable to be hit, the most skilful shot would have no advantage over a blind man. That, however, is the tacit assertion of those who use the common rule in estimating the value of various discrepant observations, when they treat them all indiscriminately. In this way, therefore, the degree of probability of any given deviation could be determined to some extent a posteriori, since there is no doubt that, for a large number of shots, the probability is proportional to the number of shots which hit a band situated at a given distance from the mark. Moreover, there is no doubt that the greatest deviation has its limits which are never exceeded and which indeed are narrowed by the experience and skill of the observer. Beyond these limits all probability is zero; from the limits towards the mark in the centre the probability increases and will be greatest at the mark itself. [Note: developers and teachers of statistical methods have long made use of archery, gunnery, darts, and aiming at targets. Indeed, the word stochastic, which refers to randomnees, has its roots in the Greek word stokhastikos, meaning able to guess, with the root stokhos meaning a target. Klein (1997) writes of ‘men reasoning on the likes of target practice’ and describes how this imagery has pervaded the thinking and work of natural philosophers and statisticians ] The foregoing give some idea of a scale of probabilities for all deviations, such as each observer should form for himself. It will not be absolutely exact, but it will suit the nature of the inquiry well enough. The mark set up is, as it were, the centre of forces to which the observers are drawn; but these efforts are opposed by innumerable imperfections and other tiny hidden obstacles which may produce in the observations small chance errors. Some of these will be in the same direction and will be cumulative, others will cancel out, according as the observer is more or less lucky. From this it may be understood that there is some relation between the errors which occur and the actual true position of the centre of forces; for another position of the mark the outcome of chance would be estimated differently. So we arrive at the particular problem of determining the most probable position of the mark from a knowledge of the positions of some of the hits. It follows from what we have adduced that one should think above all of a scale (scala) between the various distances from the centre of forces and the corresponding probabilities. Vague as is the determination of this scale, it seems to be subject to various axioms which we have only to satisfy to be in a better case than if we suppose every deviation, whatever its magnitude, to occur with equal ease and therefore to have equal probability. Let us suppose a straight line in which there are disposed various points, which indicate of course the results of different observations. Let there be marked on this line some intermediate point which is taken as the true position to be determined. Let perpendiculars expressing the probability appropriate to a given point be erected. If now a curve is drawn through the ends of the several perpendiculars this will be the scale of the probabilities of which we are speaking. If this is accepted, I think the following assumptions about the scale of probabilities can hardly be denied. Inasmuch as deviations from the true intermediate point are equally easy in both directions, the scale will have two perfectly similar and equal branches. Observations will certainly be more numerous and indeed more probable near to the centre of forces; at the same time they will be less numerous in proportion to their distance from that centre. The scale therefore on both sides approaches the straight line on which we supposed the observed points to be placed. The degree of probability will be greatest in the middle where we suppose the centre of forces to be located, and the tangent to the scale for this point will be parallel to the aforesaid straight line. If it is true, as I suppose, that even the least-favoured observations have their limits, best fixed by the observer himself, it follows that the scale, if correctly arranged, will meet the line of the observations at the limits themselves. For at both extremes all probability vanishes and a greater error is impossible. Finally, the maximum deviations on either side are reckoned to be a sort of boundary between what can happen and what cannot. The last part, therefore, of the scale, on either side, should approach steeply the line on which the observations are sited, and the tangents at the extreme points will be almost perpendicular to that line. The scale itself will thus indicate that it is scarcely possible to pass beyond the supposed limits. Not that this condition should be applied in all its rigour if, that is, one does not fix the limits of error over-dogmatically. If we now construct a semi-ellipse of any parameter on the line representing the whole field of possible deviations as its axis, this will certainly satisfy the foregoing conditions quite well. The parameter [radius] of the ellipse is arbitrary, since we are concerned only with the proportion between the probabilities of any given deviation. However elongated or com-pressed the ellipse may be, provided it is constructed on the same axis, it will perform the same function; which shows that we have no reason to be anxious about an accurate description of the scale. In fact we can even use a circle, not because it is proved to be the true scale by mathematical reasoning, but because it is nearer the truth than an infinite straight line parallel to the axis, which supposes that the several observations are of equal weight and probability, however distant from the true position. This circular scale also lends itself best to numerical calculations; meanwhile it is worth observing in advance that both hypotheses come to the same whenever the several observations are considered to be infinitely small. They also agree if the radius of the auxiliary circle is supposed to be infinitely large, as if no limits were set to the deviations. Thus if the deviation of an observation from the true position is thought of as the sine of a circular arc, the probability of that observation will be the cosine of the same arc. Let the auxiliary semicircle, which I have just described, be called the controlling semicircle (moderator). Where the centre of this semicircle is located, the true position, which fits the observations best, is to be fixed. Admittedly our hypothesis is, to some extent, precarious, but it is certainly to be preferred to the common one, and will not be hazardous to those who understand it, since the result that they will arrive at will always have a higher probability than if they had adhered to the common method. When by the nature of the case a certain decision cannot be reached, there is no other course than to prefer the more probable to the less probable. I will illustrate this line of argument by a trivial example. The particular problem is the reconciliation of discrepant observations; it is therefore a question of difference of observations. Now if a dice-thrower makes three throws with one die so that the second exceeds the first by one and the third exceeds the second by two, the throws may arise in three ways, viz. 1,2,4 or 2,3,5 or 3,4,6. None of these throws is to be preferred to the other two, for each is in itself equally probable. If you prefer the one in the middle, viz. 2,3,5, the preference is illogical. The same sort of thing happens if you choose to consider observations which, so far as you are concerned, are accidental, whether they are astronomical or of some other kind, as equally probable. Now suppose the thrower produces the same result by throwing a pair of dice three times. There will then be eight different ways in which he would obtain this result, viz. 2,3,5; 3,4,6; 4,5,7; 5,6,8; 6,7,9; 7,8,10; 8,9,11 and 9,10,12. But they are far from being all equally probable. It is well known that the respective probabilities are proportional to the numbers 8, 30, 72, 100, 120, 80, 40 and 12. From this known scale I have better right to conclude that the fifth set has happened than that any other has, because it has the highest probability; and so the three throws of a pair of dice will have been 6, 7 and 9. No-one, however, will deny that the first set 2, 3 and 5 might possibly have happened, even though it has only a fifteenth part of the probability corresponding to the fifth set. Forced to choose, I simply choose what is most probable. Although this example does not quite square with our argument, it makes clear what contribution the investigation of probabilities can make to the determination of cases. Now I will come more to grips with the actual problem. First of all, I would have every observer ponder thoroughly in his own mind and judge what is the greatest error which he is morally certain (though he should call down the wrath of heaven) he will never exceed however often he repeats the observation. He must be his own judge of his dexterity and not err on the side of severity or indulgence. Not that it matters very much whether the judgement he passes in this matter is fitting or somewhat flighty. Then let him make the radius of the controlling circle equal to the aforementioned greatest error; let this radius be r and hence the width of the whole doubtful field = 2r. If you desire a rule on this matter common to all observers, I recommend you to suit your judgement to the actual observations that you have made: if you double the distance between the two extreme observations, you can use it, I think, safely enough as the diameter of the controlling circle, or, what comes to the same thing, if you make the radius equal to the difference between the two extreme observations. Indeed, it will be sufficient to increase this difference by half to form the diameter of the circle if several observations have been made; my own practice is to double it for three or four observations, and .to increase it by half for more. Lest this uncertainty offend any one, it is as well to note that if we were to make our controlling semicircle infinite we should then coincide with the generally accepted rule of the arithmetical mean; but if we were to diminish the circle as much as possible without contradiction, we should obtain the mean between the two extreme observations, which as a rule for several observations I have found to be less often wrong than I thought before I investigated the matter. We must wonder whether its was that this eminent mathematician (who also, incidentally contributed to the epidemiological debate about smallpox vaccination) was unable to think of a formula for an error distribution that did not end abruptly, and that would instead ‘flow out’ further in both directions. Or was it that he wasn’t bothered by having to find the roots of 5th degree polynomials to find the best ‘centre’ of his semicircukar distribution? As it turns out, the mathematics involved in finding (what we now call today) the Maximun Likelihood Estimator for Laplace’s and Gauss’s (infinitely wide) error distributions is much simpler. But the, we have the benefit of hindsight. The key ideas – the semi-ellipse and the fixed (assumed known) – are highlighted. "],
["paraMu.html", "Chapter 5 The ‘mean’ parameter \\(\\mu\\) 5.1 Two genres 5.2 Fitting these to data / Estimating them from data", " Chapter 5 The ‘mean’ parameter \\(\\mu\\) [and other location (and spread and shape) parameters] The objectives of this chapter are to Although few textbooks do so, we think it is worth distinguishing two contexts. 5.1 Two genres The first is where, if there were no measurement issues, we would ‘see’ / ‘get’ / ‘observe’ the same constant every time we made a determination, but where, because of unavoidable measurement variations, there is a statistical distribution of ‘measurements’ around that constant. Examples include measurements of constants such as the speed of light, or of physical constants, such as a standard weight (e.g. 1 Kg) or of a fixed distance measured by a smart phone app or a fixed number of steps measured by a step-counter. Examples of ‘personal’ constants that are constant – at least in the short term – but not easily or reproducibly measured might be the size of a person’s vocabulary, or a person’s mean (or minimum, or typical) reaction time. Or, the target could be a person’s ‘true score’ on some test – the value one would get if one (could, but not realistically) be tested on each of the (very large) number of test items in the test bank, or observed/measured continously over the period of interest.     Starting with this simpler ‘measurement variation only’ context makes it easier to master the statistical laws that govern the variation of values derived from a combination of measurements, the variation of statistics. The second is where the variation is primarily (or in a few deluxe cases where there are no measurement issues, entirely) due to genuine – e.g., biological – variation. Examples of such (often effectively infinite in size) biological distributions include the depths of the ocean, or the heights or weights or blood pressures of a specific population.     In this context, the distribution is less likely to display the symmetry observed when the variation is entirely due to measurement variations around some constant. Thus, there may be several possible choices of the ‘centre’ of the distribution. So, in addition to pursuing the ‘mean’ parameter \\(\\mu\\) we will also pursue other numerical parameters for the centre. No matter which of the two genres we are dealing with, it may be important to quantify the spread (and maybe the shape) of the distribution. Even if this aspect may be of secondary interest, it has a bearing on what we can say about how far off the target (off the parametr) our estimators might be. We will begin with the first genre, where, if there were no measurement issues, we would ‘see’ / ‘get’ / ‘observe’ the same constant every time we made a determination, but where, because of unavoidable measurement variations, there is a statistical distribution of ‘measurements’ around that constant. Because its estimation involves the same statistical laws as when the distribution/variation is biological, we will refer to this elusive ‘constant’ as \\(\\mu.\\) 5.2 Fitting these to data / Estimating them from data Experiments to Determine the Density of the Earth. By Henry Cavendish, Esq. F.R.S. and A.S. Philosophical Transactions of the Royal Society of London, Vol. 88. (1798), pp. 469-526. http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Mean-Quantile/Cavendish1798.pdf The following Table contains the Results of the Experiments density = c( 5.50, 5.61, 4.88, 5.07, 5.26, 5.55, 5.36, 5.29, 5.58, 5.65, 5.57, 5.53, 5.62, 5.29, 5.44, 5.34, 5.79, 5.10, 5.27, 5.39, 5.42, 5.47, 5.63, 5.34, 5.46, 5.30, 5.75, 5.68, 5.85) round(mean(density),2) ## [1] 5.45 lm.fit = lm(density ~ 1) print(summary(lm.fit),digits=1) ## ## Call: ## lm(formula = density ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57 -0.15 0.01 0.16 0.40 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.45 0.04 133 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2 on 28 degrees of freedom round(confint(lm.fit),2) ## 2.5 % 97.5 % ## (Intercept) 5.36 5.53 library(mosaic) bootstrap.fits &lt;- do(1000) * lm( resample(density) ~1) round(confint(bootstrap.fits$Intercept),2) ## 2.5% 97.5% ## percentile 5.37 5.53 round(sd(bootstrap.fits$Intercept),2) ## [1] 0.04 Metrics (criteria) for measuring (best) fit "],
["paraPi.html", "Chapter 6 The (proportion) parameter 6.1 Example one 6.2 Example two", " Chapter 6 The (proportion) parameter 6.1 Example one etc 6.2 Example two etc "],
["paraLambda.html", "Chapter 7 The (event rate) parameter 7.1 Etc 7.2 ETC", " Chapter 7 The (event rate) parameter 7.1 Etc 7.2 ETC "],
["contrast2Muparas.html", "Chapter 8 Contrast: 2 mean parameters 8.1 Estimand, estimator, estimate", " Chapter 8 Contrast: 2 mean parameters 8.1 Estimand, estimator, estimate "],
["contrast2Piparas.html", "Chapter 9 Contrast: 2 proportion parameters 9.1 Estimand, estimator, estimate", " Chapter 9 Contrast: 2 proportion parameters 9.1 Estimand, estimator, estimate "],
["contrast2Lambdaparas.html", "Chapter 10 Contrast: 2 speed parameters 10.1 Estimand, estimator, estimate", " Chapter 10 Contrast: 2 speed parameters 10.1 Estimand, estimator, estimate "],
["Probability.html", "Chapter 11 Probability 11.1 Conditional – forwards 11.2 Conditional – reverse", " Chapter 11 Probability 11.1 Conditional – forwards 11.2 Conditional – reverse Application: medical diagnostic tests Exercises: Efron, Monty Hall, Economist, Wald : CF screening "],
["Distributions.html", "Chapter 12 Distributions /Random Variables 12.1 Gaussian Bernoulli-Binomial Poisson 12.2 Expectation and Variance 12.3 Functions/combinations of random variables", " Chapter 12 Distributions /Random Variables 12.1 Gaussian Bernoulli-Binomial Poisson 12.2 Expectation and Variance 12.3 Functions/combinations of random variables "],
["math.html", "Chapter 13 Mathematics 13.1 Notation 13.2 Powers, Logarithms and Anti–logarithms", " Chapter 13 Mathematics 13.1 Notation Variables and Subscripts Variable \\(Y\\) with \\(n\\) sample values denoted \\(y_1, y_2, ..., y_n\\) in order of entry; The “1”, “2”, … are called subscripts or indices. We use the letter i (or j) and the range “1 to n” to denote the n different \\(y\\) values and refer to the value of the ith \\(y\\) as “\\(y_i\\)”. Summation The term \\(\\Sigma y\\) (spoken: “sigma y” or “sum of y’s”) is used as a shorthand for the sum \\(y_1 + y_2 + \\dots + y_n\\). 13.2 Powers, Logarithms and Anti–logarithms The term \\(y^{1/2}\\) is shorthand for the square root of \\(y\\) or \\(\\sqrt{y}\\). Likewise, \\(y^{1/n}\\) denotes the n-th root of \\(y\\). \\(\\ln (y)\\) denotes the “natural log of \\(y\\)” or “log of \\(y\\) to the base e” i.e. log\\(_e(x)\\), where e is 2.718. Note: y must be positive; ln (\\(y\\)) ranges from -\\(\\inf\\) to +\\(\\inf\\).   ln (0.1) = -2.30; ln (1) = 0; ln (2) = 0.69; ln (10) =2.30 \\(\\ln(A \\times B) = \\ln(A) + \\ln(B); \\ \\ \\ \\ln(\\frac{A}{B}) = \\ln(A) - \\ln(B)\\) exp(\\(y\\)) is shorthand for \\(e^y\\) or “exponential of \\(y\\)” or the natural anti-log of \\(y\\). \\(y\\) ranges from -\\(\\infty\\) to +\\(\\infty\\). and exp(\\(y\\)) yields a positive value. eg. exp(-1) = 0.36; exp(0) =1; exp(.5) =1.64; exp(1) = 2.71… "],
["computing01.html", "Chapter 14 Computing Session 1 14.1 Biological background 14.2 Statistical Task 14.3 Shapes of Distributions 14.4 Exercises 14.5 SUMMARY", " Chapter 14 Computing Session 1 The ‘computing’ objectives are to learn how to use R to put series of observations into vectors, and how to plot one series against another. The ‘statistical’ objective of this exercise is to understand the concept of a distribution of a numerical characteristic (here an amount of elapsed time), and the various numbers describing its ‘central’ location and spread, and other ‘landmarks’. You will also be introduced (in the next section) to 2 functions that give a more complete description of a distribution. 14.1 Biological background Later on we will examine climate trends using unusual datasets, which suggest that over the last few centuries, winter tends to end earlier, and plants tend to flower earlier. One such dataset arose as part of a long-running contest, the Nenana Ice Classic More here. Here is the 2020 brochure. There is still time to compete this year, but it looks like you need to be in Alaska to find and fill out a ticket, and the organizers ask for your mailing address, not your email one! 14.2 Statistical Task You are asked to approximate and carefully examine the distribution of guesses in 2018, contained in the Book of Guesses for that year. The full book is available as the 7th tab in the 2020 website. If the full book takes too long to load, here are some excerpts from the book. For now, we will measure the guesses (and eventually the actual time) as the numbers of days since the beginning of 2018. Thus a guess of Tuesday April 17 5:20 p.m. would be measured as 31 + 28 + 31 + 16 + (17 + 20/60)/24 = 106.7208 days since the beginning of 2018. It would be tedious to try to apply optical character recognition (OCR) to each of the 1210 pages in order to be able to computerize all of the almost 242,000 guesses. Instead, you are asked to reconstruct the distribution of the guesses in two more economical ways: By determining, for each of the 36 x 2 = 72 half-days days from April 10 to May 15 inclusive, the proportion, p, of guesses that are earlier than mid-day and midnight on that date. [ In R, if p = 39.6% of the guesses were below xy.z days, we would write this as pGuessDistribution(xy.z) = 0.396. Thus, if we were dealing with the location of a value in a Gaussian (‘normal’) distribution, we would write pnorm(q=110, mean = , sd = ) ] Once you have determined these 72 proportions (p’s), plot them on the vertical axis against the numbers of elapsed days since the beginning of the year on the horizontal axis. Thus the horizontal axis runs from 92 + 10 = 102 days to 92 + 30 + 15 = 137 days. By determining the 1st, 2nd, … , 98th, 99th percentiles. These are specific examples of ‘quantiles’, or q’s. The q-th quantile is the value (here the elapsed number of days since the beginning of 2018) such that a proportion q of all guesses are below this value, and 1-q are above it. [ In R, if 40% of the guesses were below 110.2 days, we would write this as qGuessDistribution(p=0.4) = 110.2 days. Thus, if we were dealing with the 40th percentile of a Gaussian distribution with mean 130 and standard deviation 15, we would write qnorm(p=0.4, mean = 130, sd = 15). ] Once you have determined them, plot the 99 p’s (on the vertical axis) against the 99 (elapsed) times on the horizontal axis. [see the spreadsheet]. Before we get to the actual Exercises in subsection 2.2, an orientation… 14.2.1 The p and q functions: an orientation The ‘p’ function tells us, for a given value of the characteristic, what proportion of the distribution lies to the left of this specified value. The ‘q’ (or quantile) function tells us, for a given proportion p, what is the value of the characteristic such that that specified proportion p of the distribution lies to the left of this ‘q’ value. In the plot below, the values of the p function are shown on the vertical axis, in red, against the (in this case, equally-spaced) values of the characteristic, shown on the horizontal axis. You enter on the horizontal axis, and exit with an answer on the vertical axis. The q function (in blue) goes into the opposite direction. You enter at some proportion on the vertical axis, and exit with a value of the characteristic (a quantile) on the horizontal axis. In our plot, the proportions on the vertical axis are equally-spaced. Percentiles and quartiles are a very specific sets of quantiles: they are obtained by finding the values that divide the distribution into 100 or into 4. 14.3 Shapes of Distributions There are a lot of misconceptions here. Part of the confusion stems from not distinguishing distributions of individual values (the theme in this chapter) from the (conceptual) distributions of statistics (summaries such as means, proportions, regression slopes) computed from samples of individual values. Distributions of the second type are called sampling distributions, and their shapes cannot be observed … unless we simulate them, or work them out using the mathematical statistics. Otherwise, they are unseen, virtual, conceptual. The sample size (the \\(n\\)) plays a central (pun intended) role in the shape, even more so than the shape of the distribution being sampled from. Sampling distributions are the topic of the next computing chapter. For now, in this chapter, we are only concentrate on distributions of individual values. The variation of these individual values can be due just to measurement error, or to genuine biological variation, or – in the case of the pedictions about the date of the ice break-up – differences in individual perrceptions and knowledge. The main points concerning the shape of a distribution of individual values are that: The size of the population has nothing to do with the shape. If we could measure the distribution of the diameters of one’s red blood cells, the size distribution should look the same whether you are smaller person or a larger person, or male or female. Moreover, the distribution of individuals’ red bllod cell concentrations would be the same in provinces/states/countries with larger/smaller populations of persons. There is no default shape. The shape is determined by the context This is especially the case if the values are determined/generated by humans and human behaviour/choice and by circumstances. A good example is the distribution of finishing times of marathon runs. In one setting you might find peculiarities such as seen in this dataset. In another, it might have very different shapes in those who are and are not elite runners and whether there is a ‘qualifying’ time requirement – see ‘Elite and the rest of us’ in the link. Another example are the distributions of hospital size (no. of beds) in different countries or states. [Likewise with the sizes of schools and universities] These might be determined by government policy. In former communist countries, there are often cookie-cutter hospitals, all the same size within a region, and given the names hospital # 1, hospital # 2, etc. In other countries, they might reflect variations in population density, or historical reasons. Yet another examples are age-distributions: many factors determine the age-structure of a country or region of a country. It used to be like a pyramid, and still is in less developed countries. See here for some historical and modern Canadian and world examples. The distribution of age-at-death is determined by the age-distribution of the living – and by the many factors that drive age-specific death rates. The (sex-specific) distribution of adult heights of a homogeneous population might be close to symmetric (and even close to Gaussian), since the heights are determined by a large number of both ‘nature’ and ‘nurture’ factors. But, the (sex-specific) distribution of adult weights would not. Weight is determined in part by height, but also by a large number of ‘elective’ factors that are determined by one’s own lifestyle, and by the environment one lives in. In over-nourished North America, the distribution has a long right tail, whereas in some under-nourished parts of the world, it would have a long left tail. Why does the distribution of blood levels of lead in women of child bearing age have a long right tail, and why is it better fiot by a log-‘normal’ than a ‘normal’ curve? Note that the shape would not change much if a bigger sample had been taken. The histogram would just be smoother. Might it have to do with subpopulations of persons with different other sources of exposure besides Glasgow’s drinking water? For older distributions of blood lead, see here. For modern ones, use Google. The distribution of the lengths of flight delays might well be determined by competition, regulation, location, etc. ‘Then God said, ’Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.’ Thus God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament; and it was so. And God called the firmament Heaven. So the evening and the morning were the second day’ (Genesis 1:6-8). Did ‘God’ try to make it so that the distribution of the depths of the ocean would have a Gaussian curve? The distribution is a property of the population or the owner. The testing for normality that is so common needs to stop. There are a few individual-based contexts (such as with growth curves, where the percentile where an individual is located on a growth curve critical, or in tests to detect doping in sports, where an athlete’s test value is located in a reference distribution) where the shape is critical. However, when we are interested in estimating the mean or another measure of (covariate-patten-specific) distributions, the shape of the distribution of individaul values is not that relevant. See here for shapes – some odd – of distributions of individual values – and material on graphics. 14.4 Exercises Refer again to the guesses about the time of the ice-breakup. Once you have determined the 72 (cumulative) proportions (p’s) associated with the 72 half-days, plot them on the vertical axis against the numbers of elapsed days since the beginning of the year on the horizontal axis. Thus the horizontal axis runs from 92 + 10 = 102 days to 92 + 30 + 15 = 137 days. The 1st, 2nd, … , 98th, 99th percentiles are not so easy to determine since you have to locate the 2419th, 4839th, 7258t, … entries in the 1201-page Book of Guesses and plot the 99 p’s (on the vertical axis) against the 99 (elapsed) times (q’s) on the horizontal axis. Instead, use the first entry on each of pages 11, 21, … in this excerpt. [see the shared spreadsheet]. Using a different colour, plot these slightly-more-dense quantiles on the horizontal axix against the following percentages: entries = 200*seq(10,1200,10) + 1 percent = 100 * entries/241929 noquote( paste(head(round(percent,1),10),collapse=&quot;%, &quot;) ) ## [1] 0.8%, 1.7%, 2.5%, 3.3%, 4.1%, 5%, 5.8%, 6.6%, 7.4%, 8.3 tail(round(percent,1),10) ## [1] 91.8 92.6 93.4 94.2 95.1 95.9 96.7 97.5 98.4 99.2 Compare the Q\\(_{25}\\), Q\\(_{50}\\), and Q\\(_{75}\\) obtained directly with the ones obtained by interpolation of the curve showing the results of the other method. Compare the directly-obtained proportions of guesses that are before (the end of) April 20, April 30, and May 10 with the ones obtained by interpolation of the curve showing the results of the other method. By successive subtractions, calculate the numbers of guesses in each 1/2 day bin, and make a histogram of them. From them, calculate the mean, the mode, and the standard deviation. How far off was the median guess in 2018 from the actual time? Answer in days, and (with reservations stated) as a percentage? {see the 2020 brochure} Why did the ‘experts’ at the 1906 English country fair do so much better that their Alaskan counterparts? Why was the shape of the distribution of guesses by Dutch casino goers so different from the English and Alaskan ones? Instead of measuring the guessed times from the beginning of the year, suppose that, as Fonseca et al did, we measure the guessed times from the spring equinox in Alaska, i.e. from 8:15 a.m. on Tuesday, March 20, 2018, Alaska time. In this scale, compute the mean guess, and the SD of the guesses. Suppose, again, we measure the guessed times from the spring equinox, but in weeks. In this scale, compute the mean guess, and the SD of the guesses. How much warmer/colder in Nov-April is Montreal than Nenana? (For a future assignment, but you can start thinking about how) From a random sample of 100 guesses from the book, estimate how many guesses in the entire book are PM. my.id = 800606 set.seed(my.id) n = 50 sample.entry.numbers = sample(x = 1:241929, size=n) sorted.sample.entry.numbers = sort(sample.entry.numbers) head(sorted.sample.entry.numbers,10) ## [1] 10542 17437 18351 21113 24086 28782 30055 32220 33162 36443 page.number = ceiling(sorted.sample.entry.numbers/200) within.page = sorted.sample.entry.numbers-200*(page.number-1) column.number = ceiling(within.page/100) row.number = within.page - 100*(column.number-1) dataset = data.frame(page.number,column.number,row.number) head(dataset) ## page.number column.number row.number ## 1 53 2 42 ## 2 88 1 37 ## 3 92 2 51 ## 4 106 2 13 ## 5 121 1 86 ## 6 144 2 82 tail(dataset) ## page.number column.number row.number ## 45 1087 1 80 ## 46 1097 2 3 ## 47 1121 1 16 ## 48 1131 1 55 ## 49 1175 2 52 ## 50 1181 2 30 Some more links on the ‘Wisdom of Crowds’ https://www.technologyreview.com/s/528941/forget-the-wisdom-of-crowds-neurobiologists-reveal-the-wisdom-of-the-confident/ https://www.all-about-psychology.com/the-wisdom-of-crowds.html http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf 14.5 SUMMARY 14.5.1 Computing Assigning values to objects via &lt;- or = Putting numbers into vectors via concatenation c( , , ) Putting a sequence of values into vectors via the seq() function Looking at the first n and the last n elements of an object via head(object,n) and tail(object) – if you omit the n, it defaults to 6 Making a new numerical value or vector of numerical values from existing ones via, e.g. via + , * ( multiplication), ^ power etc. Using str(object) to see the **str*ucture of an object Using plot(x,y) to plot an ‘x’ vector versus a ‘y’ vector. lines(),points()andtext()` can be added to an existing plot. The approx() function for fitting 14.5.2 Statistical Concepts and Principles Definition of the p (Cumulative Distribution) function of a random variable \\(Y:\\) \\[CDF_Y(y) = F_Y(y) = Prob( Y \\le y).\\] Definition of the q (quantile, or InverseCDF) function of a random variable \\(Y:\\) \\[ q_Y(p) : the \\ y \\ such \\ that \\ Prob( Y \\le q) = p.\\] The is no default shape for the distributions of random variables, especially those representing individual values generated by, or concerning, humans. The shape is determined by the context. "],
["computing02.html", "Chapter 15 Computing: Session No. 2 15.1 Scientific background 15.2 Random Variation 15.3 When these Laws don’t apply 15.4 SUMMARY", " Chapter 15 Computing: Session No. 2 The ‘computing’ objectives are to learn how to use R to simulate random variation, and random variables visualize the consequences of aggregating independent random variables discover the statistical laws that govern the variability of combinations of independent observations The ‘statistical’ objectives of this exercise are to be introduced to parameters that measure the spread of the distribution of a random variable, whether it be an error distribution, or a biological one. learn (empirically, and heuristically) the statistical laws that govern how the spread of a linear combination (e.g, the sum, or the mean) of several (generically, \\(n\\)) independent random variates is related to the spread of the individual random variables. The ultimate objective is to be able to use these laws to help investigators, such as Henry Cavendish [see below], to (probabilistically) quantify ‘how far off’ their parameter estimates might be. 15.1 Scientific background In Isaac Newton’s Principia, Book III, The System of the World, Proposition 10, Theorem 10 we read: ‘If the earth were not denser than the seas, it would emerge from those seas and, according to the degree of its lightness, a part of the earth would stand out from the water, while all those seas flowed to the opposite side. By the same argument the spots on the sun are lighter than the solar shining matter on top of which they float. And in whatever way the planets were formed, at the time when the mass was fluid, all heavier matter made for the centre, away from the water. Accordingly, since the ordinary matter of our earth at its surface is about twice as heavy as water, and a little lower down, in mines, is found to be about three or four or even five times heavier than water, it is likely that the total amount of matter in the earth is about five to six times greater than it would be if the whole earth consisted of water, especially since it has already been shown above that the earth is about four times denser than Jupiter.’     The fact that the average of Newton’s ‘five or six’ is very close to today’s value of the mean relative density of the Earth shows just how prescient he was. The mean density of the Earth was an extremely important quantity in early Renaissance science as it provided a strong clue as to planetary composition. Some of words above are taken from this article by astronomer David Hughes. Table 2 in the article shows ‘Values suggested for the mean density of the Earth, as a function of the date of publication’ starts with Newton’s 1687 guesstimate, and ends, (23 estimates later) with Heyl and Chrzanowski’s 1942 value. Five of the 24 estimates are accompanied by a \\(\\pm\\) value, but what these \\(\\pm\\) values signify is left unexplained. One of the 24 is Cavendish’s 1798 estimate, which he obtained by taking the mean of \\(n =\\) 29 density measurements derived using a torsion balance on 17 days in the months of August and September, and the following April and May. Cavendish’s ‘point estimate’ was 5.48. Since his’extreme results do not differ from the mean by more than 0.38,or \\(\\frac{1}{14}\\) of the whole’ […] Therefore, it seems very unlikely that the density of the earth should differ from the 5.48 by so much as \\(\\frac{1}{14}\\) of the whole.’ Here we have an effort, by no less than Isaac Newton, to give a numerical interval within which the true parameter value is likely to lie. To Cavendish, it seems very unlikely that the density of the earth should differ from the 5.48 by so much as \\(\\frac{1}{14}\\) of the whole.’ Our objective is to learn the statistical laws and tools, unknown in their time, to statistically quantify how far off the mark our modern statistical estimators are (un)likely to be – and to tighten Cavendish’s lower and upper bounds! 15.2 Random Variation 15.2.1 Measurement errors The ‘standard’ has since been replaced by fancier methods, but for now let’s imagine that every family in the world make their own independent physical copy (using say a length of string or a paper or cardboard strip) of the official 1 meter platimun bar that used to be stored in Paris. Suppose these copies had a measurement error of either +1 centimeter or -1 centimeter (with plus errors and minus errors being equally likely). Thus, these errors, or ‘deviations’ from the 100cm, average out to 0. Each squared deviation is 1, so the mean squared deviation (called the error variance) is also 1. Its square root, also 1 in this instance, is called the standard deviation of the errors, or \\(\\sigma_{e}\\) for short. Now imagine taking a random sample of \\(n\\) of these copies, and computing the sum and the mean of the \\(n\\) lengths. Although it is possible to mathematically enumerate/calculate the exact probalities that the sum or mean of the lengths of the \\(n\\) copies takes on the various possible values, we will instead use R to approximate the probabilities by simulation. The probabilities in the following Figure are based on large enough numbers of simulations that – while they don’t show the perfect symmetry they would exhibit if we worked them out mathematically – they give quite close approximations. The panels in the following figure shows the probability of obtaining various possible sums and various possible sample means. Clearly, the patterns of the probabilities are strong functions of \\(n.\\) Instead of a 2-point error distribution, the next Figure shows how variable the sample totals and sample means would be if the errors were distributed a bit differently, as in the first row below. There, a quarter of the measurements have errors of +\\(\\sqrt{2} \\approx\\) +1.4cm, half have no error, and a quarter have errors of -\\(\\sqrt{2} \\approx\\) -1.4cm. Thus the error variance, the average of the squared deviations, is \\[\\sigma_e^2 = \\frac{1}{4} \\times (-1.4)^2 + \\frac{1}{2} \\times (0)^2 + \\frac{1}{4} \\times (1.4)^2 = 1,\\] so that the standard deviation is again \\(\\sigma_e = 1.\\) The row- (i.e., n-) specific distributions in the two Figures are not exactly the same. For example, the possible means in samples of size \\(n\\) = 2 have a 3-point distribution in the first one, but a 5-point distribution in the second one. But, as you will be asked to verify in the exercises below, the row-specific variances are the same in the two figures. More important than this are the statistical laws governing how widely the sums deviate from \\(\\mu_{sum}\\) = \\(n \\times\\) 100cm, and the means deviate from \\(\\mu\\) = 100cm. Clearly, the possible sums of 5 copies have a wider spread than the possible sums of 2, and the sums of 10 a wider spread than the sums of 5. Conversely, the means of 5 copies have a narrower spread than the means of 2, and the means of 10 a narrower spread than the means of 5. Instead of just telling you what the laws are, we ask you to use R to discover them yourself. 15.2.1.1 Discovering the Laws [via this computing exercise] Put the two (equally likely) errors (or deviations), i.e., -1cm and +1cm, into an R vector of length 2. [By the way, c(a,b) in R means concatenate a and b into a vector.] Then, make a new vector containing the squares of these deviations. [you can use vector * vector or vector^2]. Then use the built-in R function mean to compute the mean squared deviation. Although it is not needed in this case, use the round function to display the average squared deviation to a suitable number of decimals. Since what you have computed is a variance', usethisvariance (or better still, error.variance as the name for the mean squared deviation. Finally, check by hand that the calculation is correct. Use the sqrt() function (or the ^0.5 power) to compute the standard deviation of the errors. Change the errors from -1cm and +1cm to say -5cm and +5cm, and repeat steps 1 and 2. From this, what did you learn about the laws goverming variances and standard deviations? Add the errors onto 100cm to make 2 measurements (imperfect copies) of the meter bar, and calculate the mean, variance and standard deviation of the 2 measurements. From this, and by varying the sizes of the 2 errors, what did you learn about the mean the variance and the standard deviation of a shifted (re-located) random variable? {Hint: be careful to use your own variance and standard deviation functions, not the inbuilt var and sd functions. The reason is that whereas we say there are just 2 errors, in reality there are as many as there are copiers – effectively an infinite number, about half of whom will make an error of +1cm, and about half of whome will an error of -1cm. Or you can say that the probabilities of errors of -1cm and +1cm are 0.5, and 0.5. Change the -1cm and +1cm errors to -10mm and +10mm, and tepeat steps 1 and 2. What did you learn? (In passing: At St Hubert airport on Montreal’s South Shore from 1928 to 2004, over the 68 years where records were kept, the mean temperature for the month of January was recorded. The mean of these 68 values was -6.6 degrees Celsius (C) and the standard deviation was 2.9 degrees Celsius. Convert these to degrees F [Hint: \\(F = 32 + (9/5) \\times C\\)]) In order to see the laws in action, and figure them out (with the aid of the R code below) Begin with many simulated random pairs of measurements of (possibly imperfect copies) of the meter bar. For example, you might specify no.of.pairs = 10000. Then, using the 2-point (-1cm, +1cm) error distribution shown in the first Figure, simulate this large number of pairs of measurements. There are almost as many ways to do this as there are R programmers. One way (looking ahead to when we want to generalize and simulate larger samples) would be to use a matrix, i.e. a 2-way array, with as many rows as there are pairs (sets), and as many columns as the number of measurements per sample (here 2, but adjustable as you go along). See below. For each of these simulated samples of size \\(n\\) = 2, calculate the sample sum and sample mean. The apply function is very helpful here: you tell it to apply the desired function (FUN) separately for each row of the matrix by specifying MARGIN = 1. (Specifying MARGIN = 2 would give you a separate result for each column.) Now (finally) calculate the spread of the sample sums and sample means. Do so using both the standard deviation, and its square (the variance). The latter is not a natural quantity for non-statistians, but, as you will deduce, it is variances that scale with \\(n\\). Repeat these steps for samples of size \\(n\\) = 1, 3, 4, 5, 6, 7 , 8, 9, and 10. and plot the variances and sd’s against \\(n.\\) What laws do these plots suggest? According to Stephen Stigler, a historian of statistics, understanding of this law is one of the things that separates statisticians from mathematicains and computer scientists. Indeed, it is the second of what he calls the Seven Pillars od Statistics: The first recognition that the variation of sums did not increase proportionately with the number of independent terms added (and that the standard deviation of means did not decrease inversely with the number of terms) dates to the 1700s. This novel insight, that information on accuracy did not accumulate linearly with added data, came in the 1720s to Abraham de Moivre as he began to seek a way to accurately compute binomial probabilities with a large number of trials. In 1733 this would lead to his famous result, which we now call the normal approximation to the binomial, but by 1730 he already noticed that a crucial aspect of the distribution was tied to … [Stigler The Seven Pillars of Statistical Wisdom. Chapter 2. Its Measurement and Rate of Change.] His story of The Trial of the Pyx dramatically illustrates the early and continued blindness to the correct form of the laws. But he doesn’t think that Newtom, who was Master of the Mint for many years, took advantage of this blindness to become rich. possible R code: ERRORS = c(-1,1) no.of.pairs = 10000 means.samples.of.size.2 = rep(NA,no.of.pairs) measurements = matrix(100+sample(ERRORS,size = 2*no.of.pairs, replace = TRUE), nrow = no.of.pairs, ncol=2) str(measurements) ## num [1:10000, 1:2] 99 101 99 99 99 99 101 99 101 99 ... head(measurements,4) ## [,1] [,2] ## [1,] 99 99 ## [2,] 101 99 ## [3,] 99 101 ## [4,] 99 99 tail(measurements,4) ## [,1] [,2] ## [9997,] 101 99 ## [9998,] 101 101 ## [9999,] 99 99 ## [10000,] 101 99 sums.samples.of.size.2 = apply(measurements,MARGIN=1,FUN=sum) str(sums.samples.of.size.2) ## num [1:10000] 198 200 200 198 198 200 200 198 202 198 ... means.samples.of.size.2 = apply(measurements,MARGIN=1,FUN=mean) tail(sums.samples.of.size.2,4) ## [1] 200 202 198 200 tail(means.samples.of.size.2,4) ## [1] 100 101 99 100 table(sums.samples.of.size.2) ## sums.samples.of.size.2 ## 198 200 202 ## 2529 4941 2530 round( mean(sums.samples.of.size.2), 2) ## [1] 200 round( var(sums.samples.of.size.2), 2) ## [1] 2.02 round( sd(sums.samples.of.size.2) , 2) ## [1] 1.42 table(means.samples.of.size.2) ## means.samples.of.size.2 ## 99 100 101 ## 2529 4941 2530 round( mean(means.samples.of.size.2), 2) ## [1] 100 round( var(means.samples.of.size.2), 2) ## [1] 0.51 round( sd(means.samples.of.size.2) , 2) ## [1] 0.71 References David Hughes. The mean density of the Earth. Journal of the British Astronomical Association, Vol. 116, No. 1, p.21. 2006 15.2.2 Biological variation 15.2.2.1 Example We move now to contexts where the variation is primarily (or in a few deluxe cases where there are no measurement issues, entirely) due to genuine – e.g., biological or geographical – variation. Below, you will address one such example, the weights of a specific population where we have good national-level data to help us to set up simulations that demonstate and let us discover the statistical laws governing the (sampling) variability of various statistics derived from samples. For for now, we will consider a demographic characteristic, namely age, in a setting where it was documented for an entire population. The ages were gathered on the night of Sunday April 2nd in the 1911 Census of Ireland, the last one to be carried out under British rule. Here are the returns of one famous statistician, whose important work, published under the pen-name ‘Student,’ we will meet shortly. We will limit our attention to the county of Dublin, which at the time has a population of about half a million people, mostly urban. The male age-distribution displayed here in the usual vertical orientation has the (1/2) pyramid shape that characterizes developing countries. The somewhat different female age-distribution is in part because Ireland’s capital city, Dublin City, attracted many female workers in their late teens and their 20s (cf. the last 3 entries in Gosset’s return). The age-distributions today show large city vs. rural differences. Shapes of distributions Below, the age-distribution is rotated by 90 degrees, so that age is on the horizontal axis, and numbers of persons in the different age-bins (we will combine male and female) are on the vertical axis. In this more familiar orientation, it is easier to see that the age-distribution does not have the symmetry observed in the earlier section, where the variation was entirely due to measurement variations around some constant. (Incidentally, in the ‘age-distributions of today’s Irish population, there are none of the ’spikes’ at ages 40, 50, 60, .. that were seem in the 1911 data. The origin of these spikes is left for you to puzzle about. There are many modern examples of this phenomenon, as in this example of the recording of emergency department arrival and departure times . Another difficulty faced by earlier-century census takers is described in this account The Puzzled Census-Taker ) This – first of many – distribution of a human characteristic will serve as a strong messsage that we should not expect such distributions to be symmetric, let alone bell-shaped (‘Gaussian’ / ‘Normal’). The default stance should be that they are not. The same stance should apply to distributions of characteristics of ‘man-made’ or ‘human-made’ institutions – such as the distribution of the sizes of the hopitals or schools in a province or country. Nor should we expect symmetric distributions in the physical world, such as the distribution of the depths of the ocean, heights of land, lengths of rivers, daily temperatures in a region over a year, magnitudes of earthquakes, or intervals between eruptions of the Old Faithful geyser. Foe example, the distributions of the amounts of income tax paid by individuals in Quebec, the salaries of professional ice-hockey players, and the lengths of stay {LOS] of hospital patients do not have symmetric shapes (some distributions may even be multi-modal). Despite this, in some instances it may be critical to know the mean of the distibution, so that one can calculate the total revenue, or total payroll, or how much the savings would be if the mean LOS were reduced by 2%. We have to think about a summary appropriate to the situation. If you are representing the players, would you cite the median or the mean when telling the team-owners how poorly paid the ‘average’ player is? What if represented the owners, and wanted to show how much the team costs in payroll? Measures of ‘centre’ (Online, there are now many jokes about silly statistics and silly statisticians, such as on this site. A radio prgram used to end with &gt;Well, that’s the news from Lake Wobegon, where all the women are strong, all the men are good-looking, and all the children are above average. There are stories of a statistician drowning in a river that was 3 feet deep on average or being comfortable on average. One year in the 607 summer course for doctors, JH used some of these stories to warn against silly averages, and even quoted Francis Galton. Why do statisticians commonly limit their inquiries to Averages? It is difficult to understand why statisticians commonly limit their inquiries to Averages, and do not revel in more comprehensive views. Their souls seem as dull to the charm of variety as that of the native of one of our flat English counties, whose retrospect of Switzerland was that, if its mountains could be thrown into its lakes, two nuisances would be got rid of at once. [Natural Inheritance, 1889.] A urologist in the class gave a better example. The average person has 1 ovary and 1 testicle. Despite these jibes, there are some situations where it does make sense to talk about the mean value, even if nobody has a value close to the mean. When it comes to the age distribution of a population, the reports of national statistical agencies use a number of summary parameters, sometimes the median, sometimes the mean, and sometimes a ‘dependency ratio’ such as non-working-age to working-age numbers. For now, we will pursue the ‘mean’ parameter \\(\\mu\\). Later, we also pursue other numerical parameters for the centre. Part of the reason for our first choice is that the behavioural properties of the sample mean (an estimator of the {population mean_) are much easier to describe with standard statistical laws. Fortunately, nowadays, we have intensive computer techniques that use data-driven techniques rather than formulae-based ones, that can handle estimators of other parameters. Mean-Pursuit In the following R code, we simulate trying to estimate the mean age (\\(\\mu\\)), using just a random sample of \\(n\\) persons. Clearly, the \\(n\\)’s we will use are too small to give estimates that are ‘close enough for government work’ but the purpose is to understand what size \\(n\\) would ensure sufficiently precise estimates. In the following code, AGES refers to the ages (0 to 109) and Proportions the proportions of the population in each 1-year age bin. no.of.sims = 10000 ; # no. of samples of each size # enough to generate relatively smooth histograms sample.sizes = c(1,2,4,10,25) ; par(mfrow=c(length(sample.sizes),1),mar = c(0.5,1,0.5,1) ) for (n in sample.sizes ){ # loop over the various sample sizes ages = matrix(sample(AGES, # 1 row per simulation size = n*no.of.sims, # to save time, do all at once replace = TRUE, # only because data compressed prob = Proportions), # = FALSE if has indiv. data nrow = no.of.sims, ncol=n) # put into rows / columns if(n &gt; 1 &amp; n &lt;= 10){ print( noquote( paste(&quot;Ages of sampled persons in first 2 samples of size&quot;, toString(n)) ) ) print(head(ages,2)) } if( n == max(sample.sizes) ){ cat(&quot;The first panel shows the age-distribution of the entire population.\\n&quot;) cat(&quot;The remaining ones show the distributions of the sample sums and means.\\n&quot;) message(&quot;test&quot;) } # compute the row-specific (simulation-specific) sums and means # apply sum/mean to MARGIN=1, i.e., to each simulation (each row) sums.samples.of.size.n = apply(ages,MARGIN=1,FUN=sum) means.samples.of.size.n = apply(ages,MARGIN=1,FUN=mean) fr = table(sums.samples.of.size.n) # fr = frequency Y = max(Proportions*no.of.sims)/sqrt(0.8*n) # scale the y axis plot(fr,lw=0.4,xlim=c(0,n*(max(AGES)+12) ), ylim=c(-0.25,1)*Y, xaxt=&quot;n&quot;) text(n*105,0.55*Y,paste(&quot;n =&quot;,toString(n)),cex=2,font=3,adj=c(0,1)) for(a in seq(0,100,5)) { text(n*a, -0.01*Y, toString(n*a),adj=c(0.5,1),cex=1.5) txts = paste(&quot;Sum of&quot;,toString(n),&quot;Ages&quot;) if(n==1) txts = &quot;Individual Ages&quot; if(a==100 ) text(n*105, -0.01*Y, txts,adj=c(0,1),cex=1.5) if(n &gt; 1) text(n*a, -0.15*Y, toString(a),adj=c(0.5,1),font=4,cex=1.5) if(a==100 &amp; n &gt; 1) text(n*105, -0.15*Y, paste(&quot;Mean of&quot;,toString(n),&quot; Ages&quot;),adj=c(0,1),font=4,cex=1.5) } # how big is the spread (sd) of the simulated sums and means ? sd.sums = round( sd(sums.samples.of.size.n), 1 ) sd.means = round( sd(means.samples.of.size.n),1 ) txt.s = paste( &quot;SD of Sum:&quot;, toString(sd.sums) ) if(n==1) txt.s = paste(&quot;SD of Individual Ages:&quot;,toString(sd.sums) ) txt.m = paste(&quot;\\n\\n SD of Mean:&quot;,toString(sd.means)) if(n==1) txt.m = &quot;\\n\\n &quot; text(n*mu + sd.sums,Y*0.7, paste(txt.s,txt.m), cex=1.5,adj=c(0,0.5) ) points(n*mu,0,pch=19,col=&quot;red&quot;,cex=1.5) if(n==1){ text(A.50-0.1,0.95*Y,&quot;50% &lt;- | -&gt; 50%&quot;,adj=c(0.5,1),cex=1.5,col=&quot;blue&quot;) segments(A.50-0.1,0.95*Y, A.50-0.1,0,col=&quot;blue&quot;) text(115,Y, &quot;Reported Ages of Population of Dublin\\nIrish Census of 1911&quot;, adj=c(1,1),cex=1.25) } } ## [1] Ages of sampled persons in first 2 samples of size 2 ## [,1] [,2] ## [1,] 18 28 ## [2,] 0 45 ## [1] Ages of sampled persons in first 2 samples of size 4 ## [,1] [,2] [,3] [,4] ## [1,] 7 30 21 25 ## [2,] 15 50 38 58 ## [1] Ages of sampled persons in first 2 samples of size 10 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 81 7 25 58 14 21 37 74 23 17 ## [2,] 44 3 8 69 14 34 11 33 61 22 ## The first panel shows the age-distribution of the entire population. ## The remaining ones show the distributions of the sample sums and means. 15.2.2.2 Exercises Based on the numbers in the 5 panels, derive the statistical law that connects the spreads of the sampling distributions of the sample sum to the spread of the individual ages. (Since the calculated sd’s are based on a finite set of simulations, the numbers may not fit the law exactly ; also, the sd’s shown are rounded) Likewise, state the statistical law that connects the spreads of the sampling distributions of the sample mean to the spread of the individual ages. Use this law to predict the spread of the sampling distribution if we were to use a sample size of \\(n\\) = 100. What \\(n\\) would you need to have so that the (approx. 95%) Margin of Error, i.e., 2 times the SD of the mean (or 2 times the ‘Standard Error of the Mean’ or 2 times the ‘SEM’) is less than (a) 1 year (b) 0.5 years? Are these laws the same as the ones that apply when the only source of variation is measurement error? In the ‘measuring the meter bar’ example, the error distribution was symmetric; in this example, the ages do not have a symmetric distribution: it has a long right tail. Describe how the shape of the (sampling) distribution changes with the sample size involved. Look online for the name of this law or theorem. Let’s go back and help Cavendish with his Margin of Error. Calculate the standard deviation of his 29 individual measurements. [Hint: Don’t call it \\(\\sigma\\), since \\(\\sigma\\) refers to an infinite set of possible errors, and is therefore unknown. Call it \\(s\\), the conventional name for standard deviation of the individual values in a sample, sometimes abbreviated to sample standard deviation. It is an estimate of \\(\\sigma\\), so you are allowed to call it \\(\\hat{\\sigma}\\) or ‘sigma-hat’.] Now ‘plug-in’ the \\(s\\) value into the \\(\\frac{\\sigma}{\\sqrt{n}}\\) formula and calculate \\(\\frac{s}{\\sqrt{29}}\\). You should refer to this as the Standard Error of the Mean, or SEM for short. For a sample size this large, approx. 2 times the SEM can be used as the 95% Margin of Error or ‘ME’. [‘Student’, whom we met above, and will meet again soon, worked out what (bigger) multiple Cavendish would need to use if he had say just 4 measurements, or maybe 9, or 19 measurements rather than 29. Why, do you think, did he suggested bigger multiples when \\(s\\) – and thus the SEM and the ME – are based on just a few measurements?] 15.2.3 Example 2 The frequency distribution of the self-reported weights of a population of adults is available here. ds = read.table(&quot;http://www.biostat.mcgill.ca/hanley/statbook/weightsEtc.txt&quot;) MEAN=round(weighted.mean(ds$Weight.lbs,w=ds$Freq)) VAR = sum( ds$Freq * (ds$Weight.lbs-MEAN)^2 ) / sum(ds$Freq) SD = sqrt(VAR) WEIGHTS = sort(unique(ds$Weight.lbs)) Freq = aggregate(ds$Freq,by=list(ds$Weight.lbs),sum)$x Proportions = Freq/sum(Freq) W.50 = WEIGHTS[min(which(cumsum(Proportions) &gt; 0.50))] mu = sum(WEIGHTS*Proportions) no.of.sims = 10000 ; # no. of samples of each size # enough to generate relatively smooth histograms sample.sizes = c(1,2,4,8,16) ; par(mfrow=c(length(sample.sizes),1),mar = c(0.5,1,0.5,1) ) for (n in sample.sizes ){ # loop over the various sample sizes weights = matrix(sample(WEIGHTS, # 1 row per simulation size = n*no.of.sims, # to save time, do all at once replace = TRUE, # only because data compressed prob = Proportions), # = FALSE if has indiv. data nrow = no.of.sims, ncol=n) # put into rows / columns if(n &gt; 1 &amp; n &lt;= 10){ print( noquote( paste(&quot;Weights (lbs) of sampled persons in first 2 samples of size&quot;, toString(n)) ) ) print(head(weights,2)) } if( n == max(sample.sizes) ){ cat(&quot;The first panel shows the weight-distribution of the entire population.\\n&quot;) cat(&quot;The remaining ones show the distributions of the sample sums and means.\\n&quot;) message(&quot;test&quot;) } # compute the row-specific (simulation-specific) sums and means # apply sum/mean to MARGIN=1, i.e., to each simulation (each row) sums.samples.of.size.n = apply(weights,MARGIN=1,FUN=sum) means.samples.of.size.n = apply(weights,MARGIN=1,FUN=mean) fr = table(sums.samples.of.size.n) # fr = frequency Y = max(Proportions*no.of.sims)/sqrt(0.75*n) # scale the y axis plot(fr,lw=0.4,xlim=c(n*100,n*(max(WEIGHTS)+50) ), ylim=c(-0.25,1)*Y, xaxt=&quot;n&quot;) text(n*320,0.55*Y,paste(&quot;n =&quot;,toString(n)),cex=2,font=3,adj=c(0,1)) for(w in seq(100,300,20)) { text(n*w, -0.01*Y, toString(n*w),adj=c(0.5,1),cex=1.5) txts = paste(&quot;Sum of&quot;,toString(n),&quot;Weights&quot;) if(n==1) txts = &quot;Individual Weights&quot; if(w==300 ) text(n*310, -0.01*Y, txts,adj=c(0,1),cex=1.5) if(n &gt; 1) text(n*w, -0.15*Y, toString(w),adj=c(0.5,1),font=4,cex=1.5) if(w==300 &amp; n &gt; 1) text(n*310, -0.15*Y, paste(&quot;Mean of&quot;,toString(n),&quot; Weights&quot;),adj=c(0,1),font=4,cex=1.5) } # how big is the spread (sd) of the simulated sums and means ? sd.sums = round( sd(sums.samples.of.size.n), 1 ) sd.means = round( sd(means.samples.of.size.n),1 ) txt.s = paste( &quot;SD of Sum:&quot;, toString(sd.sums) ) if(n==1) txt.s = paste(&quot;SD of Individual Weights:&quot;,toString(sd.sums),&quot;(lbs)&quot; ) txt.m = paste(&quot;\\n\\n SD of Mean:&quot;,toString(sd.means)) if(n==1) txt.m = &quot;\\n\\n &quot; text(n*mu + sd.sums,Y*0.7, paste(txt.s,txt.m), cex=1.5,adj=c(0,0.5) ) points(n*mu,0,pch=19,col=&quot;red&quot;,cex=1.5) if(n==1){ text(W.50-1,0.95*Y,&quot;50% &lt;- | -&gt; 50%&quot;,adj=c(0.5,1),cex=1.5,col=&quot;blue&quot;) segments(W.50-0.1,0.95*Y, W.50-0.1,0,col=&quot;blue&quot;) text(352,-0.175*Y, &quot;Reported Weights, US Adults, 2014-2018&quot;, adj=c(1,1),cex=1.25) } } ## [1] Weights (lbs) of sampled persons in first 2 samples of size 2 ## [,1] [,2] ## [1,] 120 110 ## [2,] 130 140 ## [1] Weights (lbs) of sampled persons in first 2 samples of size 4 ## [,1] [,2] [,3] [,4] ## [1,] 130 170 150 190 ## [2,] 200 200 150 150 ## [1] Weights (lbs) of sampled persons in first 2 samples of size 8 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 150 130 160 130 180 130 180 140 ## [2,] 130 130 170 150 130 200 220 150 ## The first panel shows the weight-distribution of the entire population. ## The remaining ones show the distributions of the sample sums and means. 15.2.3.1 Exercises If an elevator is designed to lift a maximum of 5,000 pounds, what is the probability that it will be overloaded by a random group of 25 persons? What maximum number of persons should you specify so that the probability is below 1 in a million? The weights of individuals do not have a Gaussian (“Normal”) distribution. Are you still comfortable using the Normal distribution for your calculations. Explain carefully. Explain also why the ‘random’ is key to being able to answer, and what the impact would be if it is not the case. 15.3 When these Laws don’t apply In these panels, it’s the same elevator, and the same population. But what’s going on with these elevator-loads? Is the safe to have a load of 25? ``` 15.4 SUMMARY 15.4.1 Computing Assigning values to objects via &lt;- or = Putting numbers (or character strings) into vectors via concatenation c( , , ) Putting repeated values into vectors via the rep() function Looking at the first n and the last n elements of an object via head(object,n) and tail(object) – if you omit the n, it defaults to 6 Making a new numerical value or vector of numerical values from existing ones via, e.g. via + , * ( multiplication), ^ power etc. Using built-in functions, such as mean(), sum(), sd() and var(), that operate on vectors, or on single numbers (or ‘element-wise’ on vectors), such as round() and sqrt() Making numerical arrays using the matrix function Taking values at random from a vector via sample() Using str(object) to see the **str*ucture of an object Using apply(object, MARGIN = , FUN = ) to apply a function to the spefified margins (1=rows,2=cols) of an matrix or of a (possibly higher-dimensional) array. Using table to create a vector, matrix or array of the frequencies (cell counts) corresponding to a single variable, or combination of variables Using plot(x,y) to plot an ‘x’ vector versus a ‘y’ vector. lines(),points()andtext()` can be added to an existing plot. A graphic can be split into a 2-way grid of panels by using a vector of the form c(nr, nc) as the input to mfrow or mfcol in the par statement. The bottom, left, upper, and right margins can be set using the mar parameter Consider a Gaussian distribution with a specified mean.value and standard deviation sd.value. Then the proportion of the distrution to that lies ot the left of a specified value q is given by the inbuilt R function pnorm(q, mean = mean.value, sd = sd.value). For example, if in a certain population IQ has a Gaussian distribution with a mean of 100, and a sd of 15, then using this R expression round( 100*pnorm(110, mean = 100, sd = 15) , 1 ) we can determine that 74.8 percent of the distribution would be below 110, and thus that 25.2 percent would be below 90. The middle 50% could be calculated using this expression round( 100*qnorm(c(1/4,3/4), mean = 100, sd = 15) , 1 ) to give 89.9, 110.1 15.4.2 Statistical Concepts and Principles Definition of the Standard Deviation \\(\\sigma,\\) (and its square, the ‘Variance’, \\(\\sigma^2\\), of a random variable \\(Y\\) with mean \\(\\mu\\). \\[ Var[Y] = \\sigma^2 = \\textrm{mean of } (Y - \\mu)^2 \\ ; \\ \\ \\ SD[Y] = \\sigma.\\] \\[ Var[Y \\pm a \\ constant] = Var[Y] ; \\ \\ \\ SD[Y \\pm a \\ constant] = \\sigma.\\] \\[ Var[Y \\times a \\ constant] = constant^2 \\ \\times \\ Var[Y] ; \\ \\ \\ SD[Y \\times a \\ constant] = |constant| \\times \\sigma.\\] Rules for Variances and SDs of sums of \\(n\\) independent random variables, say \\[ Var[ Y_1 + Y_2 + \\dots + Y_n] = \\sigma^2 + \\sigma^2 + \\dots + \\sigma^2 = n \\times \\sigma^2.\\] \\[ SD[ Y_1 + Y_2 + \\dots + Y_n] = \\sqrt{n} \\times \\sigma.\\] Rules for Variances and SDs of means of \\(n\\) independent random variables. \\[ Var\\bigg[\\frac{Y_1 + Y_2 + \\dots + Y_n}{n}\\bigg] = \\frac{1}{n} \\times \\sigma^2.\\] \\[ SD\\bigg[\\frac{Y_1 + Y_2 + \\dots + Y_n}{n}\\bigg] = \\sqrt{\\frac{1}{n}} \\times \\sigma.\\] Sums of \\(n\\) independent random variables are \\(\\sqrt{n}\\) times more variable than their individual components. [But on a relative scale, since the sum is proportional to \\(n\\) while its spread is proportional to \\(\\sqrt{n}\\), the percentage spread is a decreasing function of \\(n.\\) Means of \\(n\\) independent random variables are \\(\\sqrt{n}\\) times less variable than their individual components. The SHAPES of the sampling distributions of the sums and means of \\(n\\) independent random variables become more ‘Gaussian-looking’ with larger \\(n.\\) "],
["computing03.html", "Chapter 16 Computing Week3 16.1 Ages of books 16.2 ngrams 16.3 Ice Breakup Dates 16.4 Galton’s data on family heights 16.5 Temperature perceptions 16.6 Natural history of prostate cancer 16.7 Serial PSA values 16.8 Graphics 16.9 Possible Body Mass Indices 16.10 Galton 16.11 Epidemics 16.12 Duplicate Birthdays 16.13 Lottery payoffs 16.14 Chevalier de Méré 16.15 Detecting a fake Bernoulli sequenece 16.16 Cell occupancy 16.17 Life Tables 16.18 Carrier Status (genetics) 16.19 Diagnostic and statistical tests", " Chapter 16 Computing Week3 (Probabilities, evaluated by simulation) 16.1 Ages of books 16.2 ngrams 16.3 Ice Breakup Dates Here are some details on the Nenana Ice Classic More here 16.3.1 The 2018 Book of Guesses We are keen to establish the distribution of guesses, with the guessed times measured from midnight on December 31, 2017. Thus a guess of April 06, at 5:15 p.m. would be measured as 31 + 28 + 31 + 5 + 12/24 + (5+ 15/60)/24 = 95.71875 days into the year 2018. It would be tedious to apply optical character recognition (OCR) to each of the 1210 pages in order to be able to computerize all 240,000 guesses. Instead, you are asked to reconstruct the distribution of the guesses in two more economical ways: By determining, for (the beginning of) each day, from April 01 to June 01 inclusive, the proportion, p, of guesses that predede that date. [ In R', if p = 39.6% of the guesses were below 110 days, we would write this as pGuessDistribution(110) = 0.396. Thus, if we were dealing with the location of a value in a Gaussian ('normal') distribution, we would writepnorm(q=110, mean = , sd = )` ] Once you have determined them, plot these 62 p’s (on the vertical axis) against the numbers of elapsed days (90-152) on the horizontal axis. By determining the 1st, 2nd, … , 98th, 99th percentiles. These are specific examples of ‘quantiles’, or q’s. The q-th quantile is the value (here the elapsed number of days since the beginning of 2018) such that a proportion q of all values are below this value, and 1-q are above it. [ In R', if 40% of the guesses were below 110.2 days, we would write this as qGuessDistribution(p=0.4) = 110.2 days. Thus, if we were dealing with the 40th percentile of a Gaussian distribution with mean 130 and standard deviation 15, we would writeqnorm(p=0.4, mean = 130, sd = 15)`. ] Once you have determined them, plot the 99 p’s (on the vertical axis) against the 99 (elapsed) times on the horizontal axis. Compare the Q\\(_{25}\\), Q\\(_{50}\\), and Q\\(_{75}\\) obtained directly with the ones obtained by interpolation of the curve showing the results of the other method. Compare the directly-obtained proportions of guesses that are before April 15, April 30, and May 15 with the ones obtained by interpolation of the curve showing the results of the other method. By successive subtractions, calculate the numbers of guesses in each 1-day bin, and make a histogram of them. From them, calculate the mean, the mode, and the standard deviation. To measure the spread of guesses, Galton, in his vox populi (wisdom of crowds) article, began with the interquartile range (IQR), i.e. the distance between Q75 and Q25, the 3rd and 1st quartiles. In any distribution, 1/2 the values are within what was called the ’probable error (PE) of the mean; i.e., it is equally probable that a randomly selected value would be inside or outside this middle-50 interval. Today, we use standard deviation (SD) instead of probable error. In a Gaussian distribution, some 68% of values are within 1 SD of the mean, whereas 50% of values are within 1 PE of the mean. We can use R to figure out how big a PE is in a Gaussian distribution compared with a SD. By setting the SD to 1, and the eman to 0, we have Q75 = qnorm(p = 0.75, mean=0, sd=1) round(Q75,2) ## [1] 0.67 i.e, a PE is roughtly 2/3rds of a SD. Galton – convert to SD. Geometric mean Amsterdam study How far off was the median guess in 2018 from the actual time? Answer in days, and (with reservations stated) as a percentage? Why did the experts at the country fair do so much better? Where were the punters in 2019 wrt the actual ? https://www.technologyreview.com/s/528941/forget-the-wisdom-of-crowds-neurobiologists-reveal-the-wisdom-of-the-confident/ https://www.all-about-psychology.com/the-wisdom-of-crowds.html http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf [Nenana Ice Classic] Tanana River 16.3.2 Trends over the last 100 years fill in the data since 200x. cbind with … https://www.adn.com/alaska-news/2019/04/14/nenana-ice-classic-tripod-goes-down-setting-record-for-earliest-river-break-up/ time trends morning vs afternoon ? 2019 extreme… how many SD’s from the line? Where were the punters in 2019 wrt the actual ? Sources: 1917-2003 data in textfile on course website; http://www.nenanaakiceclassic.com/ for data past 2003 ascii (txt) and excel files with data to 2003 Working in teams of two … *Create a dataframe containing the breakup data for the years 1917-2007. Possible ways to do so include: directly from the ascii (txt) file; from the Excel file *From the decimal portion of the Julian time, use R to create a frequency table of the hour of the day at which the breakup occurred. *From the month and day, use R to calculate your own version of the Julian day (and the decimal portion if you want to go further and use the hour and minute) *Is there visual evidence that over the last 91 years, the breakup is occurring at an earlier date? *Extract the date and ice thickness measurements for the years 1989-2007 from the website and use your software of choiceR` to create a single dataset with the 3 variable, year, day and thickness. From this, fit a separate trendline for each year, and calculate the variability of these within-year slopes. 16.4 Galton’s data on family heights These data were gathered to examine the relation between heights of parents and heights of their (adult) children. They have been recently ‘uncovered’ from the Galton archives. As a first issue, for this exercise, you are also asked to see whether the parent data suggest that stature plays “a sensible part in marriage selection”. For the purposes of this exercise, the parent data [see http://www.epi.mcgill.ca/hanley/galton ] are in a file called parents.txt , with families numbered 1-135, 136A, 136-204 ( {the heights of the adult offspring will be used in a future exercise) Do the following tasks using R Categorize each father’s height into one of 3 bins (shortest 1/4, middle 1/2, tallest 1/4). Do likewise for mothers. Then, as Galton did [ Table III ], obtain the 2-way frequency distribution and assess whether “we may regard the married folk as picked out of the general population at haphazard”. Calculate the variance Var[F] and Var[M] of the fathers’ [F] and mothers’ [M] heights respectively. Then create a new variable consisting of the sum of F and M, and calculate Var[F+M]. Comment. Galton called this a “shrewder” test than the “ruder” one he used in 1. When Galton first anayzed these data in 1885-1886, Galton and Pearson hadn’t yet invented the correlation coefficient. Calculate this coefficient and see how it compares with your impressions in 1 and 2. 16.5 Temperature perceptions Create 5 datasets from the questionnaire data on temperature perceptions etc. by importing directly from the Excel file applied to .csv version of Excel file); by first removing the first row (of variable names) and exporting the Excel file into a ’comma-separated-values&quot; (.csv) text file, then … reading the data in this .csv file via the INFILE and INPUT statements in a SAS DATA step, [SAS] INFILE ‘path’ DELIMITER =“,”; INPUT ID MALE $ MD $ EXAM TEMPOUTC TEMPINC TEMPOUTF TEMPINF TEMPFEEL TIME PLACE $ ; by reading the data in the text file temps_1.txt into the SAS dataset via the INFILE and INPUT statements. Notice that the ‘missing’ values use the SAS representation (.) for missing values. or the Stata dataset using the ‘infile’ command by reading the data in the text file temps_2.txt via [in SAS] the INFILE and INPUT statements in a DATA step or [in Stata] the ‘infix’ command. Here you will need to be careful, since ‘free-format’ will not work correctly (it is worth trying free format with this file, just to see what goes wrong!). When using the INFILE method, you can control some of the damage by using the ‘MISSOVER’ option in the INFILE statement: this keeps the INPUT statement from continuing on into the next data line in order to find the (in our example) 11 values implied by the variable list. JH uses this ‘defensive’ option in ALL of his INFILE statements. by cutting and pasting the contents of the text file temps_2.txt directly into the SAS or Stata program - in SASthe lines of data go immediately after the DATALINES statement, and there needs to be a line containing a semicolon to indicate the end of the data stream. In Stata, the lines of data go immediately after the infile or infix statement, and there needs to be a line containing the word ‘end’ to indicate the end of the data stream This Cut and Paste Method is NOT RECOMMENDED when the number of observations is large, as it is too all too easy to inadvertently alter the data, and the SAS/Stata porogram becomes quite long and unwieldy. It is Good Data Management Practice to separate the program statements from the data. [Run [in SAS] PROC MEANS [in Stata] the ‘describe’ command, on the numerical variables, and [in SAS] PROC FREQ or [in Stata] the ‘tabulate’ command, on the non-numerical variables, to check that the 5 datasets you created contain the same information. Also, get in the habit of viewing or printing several observations and checking the entries against the ‘source’. When using (i), have SAS show you the SAS statements generated by the wizard. Store these, and the DATA steps for (ii) to (v) in a single SAS program file (with suffix .sas). Annotate liberally using comments: in SAS, either begin with * ; or enclose with /* … */ in Stata ..begin the line with * or place the comment between /* and */ delimiters or begin the comment with // or begin the comment with /// Q2 Use one of these 5 datasets, and the appropriate [in SAS, PROCs (see Exploring Data under UCLA SAS Class Notes 2.0)], or [in Stata, the list comamnd, and the analyses from the Statistics menu] to list the names and characteristics of the variables list the first 5 observations in the dataset list the id # and the responses just to q3, w5 and q6, for all respondents, with respondents in the order: female MDs, male MDs, female non-MDs, male non-MDs. Indicate the [sub-]statement that is required to reverse this order. create a 2-way frequency table, showing the frequencies of respondents in each of the 2 (MD nonMD) x 2 (male female) = 4 ‘cells’ (one defintion of an epidemiologist is ‘an MD broken down by age and sex’). Turn off all the extra printed output, so that the table just has the cell frequencies and the row and column totals. compare the mean and median attitude to exams in MDs vs. non-MDs (hint: in SAS, the CLASS statement may help). Get SAS/Stata to limit the output to just the ‘n’, the min, the max, the mean and the median for each subgroup. And try to also get it to limit the number of decimal places of output (in SAS the MAXDEC option is implememnted in some procedures, but as far as JH can determine not in all) compare the mean temperature perceptions (q6) of male and female respondents [in SAS] create a low-res (‘typewriter’ resolution) scatterplot of the responses to q5 (vertical axis) vs. q4 (horizonatal axis), using a plotting symbol that shows whether the responsdent is a male or a female. If we have not covered how to show this ‘3rd dimension’, look at the ONLINE Documentation file {the guide for most of the procedures covered in this set of exercises is in the Base SAS Procedures Guide; other procedures are in sthe more advanced ‘STAT’ module}. You can specify the variable whose values are to mark each point on the plot. See PLOT statement in PROC PLOT, and the example with variables height weight and gender. [in Stata] use the (automatically hi-res) graphics capabilities available from the ‘Graphics’ menu [if SAS] Put all of the programs for Q1, and all of these program steps and output for Q2 in a single .txt file (JH will use a mono-spaced font such as Courier to view it – that way the alignment should be OK), with PROC statements interleaved with output, and a helpful 2-line title (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line (that way, lines won’t wrap-around when JH views the material). [if Stata] paste the results and graphics into Word. NOTE: To be fair to SAS, it CAN produce decent (and even some publication-quality) graphics. See http://www.ats.ucla.edu/stat/sas/topics/graphics.htm Then submit the text file electronically (i.e., by email) to JH by 9 am on Monday October 2. 16.6 Natural history of prostate cancer Q1 The following data items are from an investigation into the natural history of (untreated) prostate cancer [ report (.pdf) by Albertsen Hanley Gleason and Barry in JAMA in September 1998 ]. id, dates of birth and diagnosis, Gleason score, date of last contact, status (1=dead, 0=alive), and – if dead – cause of death (see 2b below). data file (.txt) for a random 1/2 of the 767 patients Compute the distribution of age at diagnosis (5-year intervals) and year of diagnosis (5 year intervals). Also compute the mean and median ages at diagnosis. For each of the 20 cells in Table 2 (5 Gleason score categories x 4 age-at-dx categories), compute the number of man-years (M-Y) of observation number of deaths from prostate cancer(1), other causes(2), unknown causes(3) prostate cancer(1) death rate [ deaths per 100 M-Y ] proportion who survived at least 15 years. For a and b you can use the ‘sum’ option in PROC means; ie PROC MEANS data = … SUM; VAR vars you want to sum; BY the 2 variables that form the cross-classification. Also think of a count as a sum of 0s and 1s. For c (to avoid having to compute 20 rates by hand), you can ‘pipe’ i.e. re-direct the sums to a new sas datafile, where you can then divide one by other to get (20) rates. Use OUTPUT OUT = …. SUM= …names for two sums; On a single graph, plot the 5 Kaplan-Meier survival curves, one for each of the 5 Gleason score categories (PROC LIFETEST .. Online help is under the SAS STAT module, or see http://www.ats.ucla.edu/stat/sas/seminars/sas_survival/default.htm. For Stata, see http://www.ats.ucla.edu/stat/stata/seminars/stata_survival/default.htm. [OPTIONAL] In order to compare the death rates with those of U.S. men of the same age, for each combination of calendar year period (1970-1974, 1975-1979, …, 1994-1999) and 5 year age-interval (55-59, 60-64, … obtain the number of man-years of follow-up and the number of deaths. Do so by creating, from the record for each man, as many separate observations as the number of 5yr x 5yr “squares” that the man traverses diagonally through the Lexis diagram [ use the OUTPUT statement within the DATA step]. Then use PROC MEANS to aggregate the M-Y and deaths in each square. If you get stuck, here is some SAS code that does this, or see the algorithm given in Breslow and Day, Volume II, page ___ Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday Nov 7. 16.7 Serial PSA values Q1 These two files contain PSA values [pre-] and [post-] treatment of prostate cancer *. Create a ‘wide’ PSA file of 25 log-base-2 PSA values per man (some will be missing, if PSA not measured 25 times). Print some excerpts. (b)From the dataset created in (a), create a long file, with just the observations containing the non-missing log-base-2 PSA values [OUTPUT statement in DATA step]. Print and plot some excerpts. From the dataset created in (b), create a wide file [ RETAIN, first. and last. helpful here; or use PROC TRANSPOSE ]. Print some excerpts. The order of the variables is given in this sas program . Some of the code in the program may also be of help. Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday Nov 14. 16.8 Graphics 1a Re-produce (or if you think you can, improve on) three of the graphs shown in “Examples of graphs from Medical Journals.” These examples are in a pdf file on the main page. Use Excel for at least one of them, and R/Stata/SAS for at least one other. Do not go to extraordinary lengths to make them exactly like those shown – the authors, or the journals themselves, may have used more specialized graphics software. You may wish to annotate them by making (and sharing with us) notes on those steps/options that were not immediately obvious and that took you some effort to figure out. Insert all three into a single electronic document. 1b Browse some medical and epidemiologic journals and some magazines and newspapers published in the last 12 months, Identify the statistical graph you think is the worst, and the one you think is the best. Tell us how many graphs you looked at, and why you chose the two you did. If you find a helpful online guide or textbook on how to make good statistical graphs, please share the reference with us. [The bios601 site http://www.epi.mcgill.ca/hanley/bios601/DescriptiveStatistics/ has a link to the Textbook by Cleveland and the book “R Graphics” by Paul Murrell. If possible, electronically paste the graphs into the same electronic file you are using for 1a. 2 [OPTIONAL] The main page has a link to a lifetable workbook containing three sheets. Note that the ‘lifetable’ sheet in this workbook is used to calculate an abridged current life table based on the 1960 U.S. data. Use this sheet as a guideline, and create a current life-table (‘complete’, i.e., with 1-year age-intervals) for Canadian males, using the male population sizes, and numbers of deaths, by age, Canada 2001. [The calculations in columns O to W of the lifetable sheet are not relevant for this exercise]. Details on the elements of, and the construction of current lifetables can be found in the chapters (on website) from the textbooks by Bradford Hill and Selvin, and in the technical notes provided by the US National Center for Health Statistics in connection with US Lifetable 2000. See also the FAQ for 613 from 2005. The fact that the template is for an abridged life table, with mostly 5-year intervals, whereas the task is to construct a full lifetable with 1 year intervals, caused some people problems last year.. they realized something was wrong when the life expectancy values were way off! Since this is an exercise, and not a calculation for an insurance company that wants to have 4 sig. decimal places, don’t overly fuss about what values of ‘a’ you use for the early years.. they don’t influence the calculations THAT much: If you try different sets of values (such as 0.1 in first year and 0.5 thereafter) you will not find a big impact. But don’t take my word for it .. the beauty of a spreadsheet is that you can quickly see the consequences of different assumptions or ‘what ifs’. [In practice, in order not to be unduly influenced by mortality rates in a single calendar year (e.g. one that had a very bad influenza season), current lifetables are usually based on several years of mortality data. Otherwise, or if they are based on a small population, the quantities derived from them will exhibit considerable random fluctuations from year to year ] Once you have completed the table, use the charting facilities in Excel to plot the survival curve for the hypothetical (fictitious) male ‘cohort’ represented by the current lifetable. On a separate graph, use two histograms to show the distributions of the ages at death (i) for this hypothetical male ‘cohort’ and (ii) those males who died in 2001. To make it easy to compare them, superimpose the histograms or put them ‘side by side’ or ‘back to back’ within the same graph. Explain why the two differ in shape and location. Calculate/derive (and include them somewhere on the spreadsheet) the median and mean age at death in the hypothetical cohort and the corresponding statistics for the actual deaths in 2001. 16.9 Possible Body Mass Indices This exercise investigates different definitions of Body Mass Index (BMI). BACKGROUND: With weight measured in Kilograms, and height in metres, BMI is usually defined as weight divided by the SQUARE of height, i.e., BMI = Wt / (Height*Height), or BMI = Wt/(height2) using, as SAS and several other programming languages do, the symbol for ‘raised to the power of’. [ NB: Excel uses ^ to denote this ] What’s special about the power of 2? Why not a power of 1 i.e., Weight/height? Why not 3, i.e., Weight/*(height3) ? Why not 2.5 i.e. Weight/(height2.5)? One of the statistical aims of a transformation of weight and height to BMI is that BMI be statistically less correlated with height, thereby separating height and height into two more useful components height and BMI. For example in predicting lung function (e.g. FEV1), it makes more sense to use height and BMI than height and weight, since weight has 2 components in it – it is partly height and partly BMI. Presumably, one would choose the power which minimizes the correlation. The task in this project is to investigate the influence of the power of height used in the ratio, and to see if the pattern of correlations with power is stable over different settings (datasets). DATA: To do this, use 2 of the 6 datasets on the 678 webpage: [usernane is c678 and p w is H**J44 ] Children aged 11-16 Alberta 1985 (under ‘Datasets’) 18 year olds in Berkeley longitudinal study, born 1928/29 (under ‘Datasets’) Dataset on bodyfat – 252 men (see documentation) (under ‘Datasets’) Pulse Rates before and after Exercise – Australian undergraduates in 1990’s (under ‘Projects’) Miss America dataset 1921-2000 (under ‘Resources’) Playboy dataset 1929-2000 (under ‘Resources’) METHODS: First create each of the two SAS datasets, and if height and weight are not already in metres and Kg, convert them to these units. Drop any irrelevant variables. Inside each dataset, create a variable giving the source of the data (we will merge the two – and eventually all six– datasets, so we need to be able to tell which one each observation came from). Combine the two datasets, i.e. ‘stack’ them one above the other in a single dataset. Print out some excerpts. For each subject in the combined dataset, create 5 versions of &lt; using the powers 1, 1.5, 2, 2.5 and 3. Calculate the correlation between the ‘BMI’ obtained with each of these powers, and height. Do this separately for the observations from the two different sources (the BY statement should help here). Report your CONCLUSIONS. 16.10 Galton The objective of this exercise is to examine the relation between heights of parents and heights of their (adult) children, using recently ‘uncovered’ data from the Galton archives, You are asked to assess if Galton’s way of dealing with the fact that heights of males and females are quite different produces sharper correlations than we would obtain using ‘modern’ methods of dealing with this fact. As side issues, you are also asked to see whether the data suggest that stature plays “a sensible part in marriage selection” and to comment on the correlations of the heights in the 4 {father,son}, {father,daughter}, {mother,son} and {mother,daughter} pairings. BACKGROUND: Galton ‘transmuted’ female heights into their ‘male-equivalents’ by multiplying them by 1.08, and then using a single combined ‘uni-sex’ dataset of 900-something offspring and their parents. While some modern-day anayysts would simply calculate separate correlations for the male and female offspring (and then average the two correlations, as in a meta-analysis), most would use the combined dataset but ‘partial out’ the male-females differences using a multivariable analysis procedure. The various multivariable procedures in effect create a unisex dataset by adding a fixed number of inches to each female’s height (or, equivalently, in the words of one of our female PhD students, by ‘cutting the men down to size’). JH was impressed by the more elegant ‘proportional scaling’ in the ‘multiplicative model’ used by Galton, compared with the ‘just use the additive models most readiliy available in the software’ attitude that is common today. In 2001, he located the raw (untransmuted) data that allows us to compare the two approaches. DATA: For the purposes of this exercise, the data [see http://www.epi.mcgill.ca/hanley/galton ] are in two separate files: the heights# of 205 sets of parents ( parents.txt ) with families numbered 1-135, 136A, 136-204 the heights# of their 900-something* children ( offspring.txt ) with families numbered as above The data on eight families are deliberately omitted, to entice the scholar in you to get into the habit of looking at (and even double checking) the original data. Since here we are more interested in the computing part in this course, and because time is short, ignore this invitation to inspect the data – we already had a look at them in class. In practice, we often add in ‘missing data’ later, as there are always some problem cases, or lab tests that have to be repeated, or values that need to be checked, or subjects who didn’t get measured at the same time as others etc.. JH’s habit is to make the additions in the ‘source’ file (.txt or .xls or whatever) and re-run the entire SAS DATA step(s) to create the updated SAS dataset (temporary or permanent). If the existing SAS datset is already large, and took a lot of time to create, you might consider creating a small dataset with the new observations, and then stacking (using SE) the new one under the existing one – in a new file. SAS has fancier ways too, and others may do things differently! If your connection is too slow to view the photo of the first page of the Notebook, the title reads FAMILY HEIGHTS (add 60 inches to every entry in the Table) METHODS/RESULTS/COMMENTS: Categorize each father’s height into one of 3 subgroups (shortest 1/4, middle 1/2, tallest 1/4). Do likewise for mothers. Then, as Galton did [ Table III ], obtain the 2-way frequency distribution and assess whether “we may regard the married fold as picked out of the general population at haphazard”. Calculate the variance Var[F] and Var[M] of the fathers’ [F] and mothers’ [M] heights respectively. Then create a new variable consisting of the sum of F and M, and calculate Var[F+M]. Comment. Galton called this a “shrewder” test than the “ruder” one he used in 1. ( statistic-keyword VAR in PROC MEANS) When Galton first anayzed these data in 1885-1886, Galton and Pearson hadn’t yet invented the CORRelation coefficient. Calculate this coefficient and see how it compares with your impressions in 1 and 2. Create two versions of the transmuted mother’s heights, one using Galton’s and one using the modern-day (lazy-person’s, blackbox?) additive scaling [for the latter, use the observed difference in the average heights of fathers and mothers, which you can get by e.g., running PROC MEANS on the offspring dataset, either BY gender, or using gender as a CLASS variable]. In which version of the transmuted mothers’ heights is their SD more simlar to the SD of the fathers? ( statistic-keyword STD in PROC MEANS) Create the two corresponding versions of what Galton called the ‘mid-parent’ (ie the average of the height of the father and the height of the transmuted mother). Take mid-point to mean the half-way point (so in this case the average of the two) Create the corresponding two versions (additive and multiplicative scaling) of the offspring heights (note than sons’ heights remain ‘as is’). Address again, but now for daughters vs sons, the question raised at the end of 4. Merge the parental and offspring datasets created in steps 4 and 6, taking care to have the correct parents matched with each offspring (this is called a 1:many merge). Using the versions based on 1.08, round the offspring and mid-parent heighs to the nearest inch (or use the FLOOR function to just keep the integer part of the mid-parent height –you need not be as fussy as Galton was about the groupings of the mid-parent heights), and obtain a 2-way frequency distribution similar to that obtained by Galton [ Table I ]. Note that, opposite to we might do today, Galton put the parents on the vertical, and the offspring on the horizontal axis. ( The MOD INT FLOOR CEIL and ROUND functions can help you map observations into ‘bins’ ; we will later see a way to do so using loops) Galton called the offspring in the same row of his table a ‘filial array’. Find the median height for each filial array, and plot it, as Galton did, against the midpoint of the interval containing their midparent – you should have one datapoint for each array. Put the mid-parent values on the vertical, and the offspring on the horizontal axis. By eye, estimate the slope of the line of best fit to the datapoints. Mark your fitted line by ‘manually’ inserting two markers at the opposite corners of the plot. Does the slope of your fitted line agree with Galton’s summary of the degree of “regression to mediocrity”? [ Plate IX ] Note that Galton used datapoints for just 9 filial arrays, choosing to omit those in the bottom and top rows (those with the very shortest and the very tallest parents) because the data in these arrays were sparse. ( By using the binned parental height in the CLASS statement in PROC MEANS or PROC UNIVARIATE, directing the output to a new SAS dataset, and applying PROC PLOT to this new dataset, you can avoid having to do the plotting manually See more on this in the FAQ) Plot the individual unisex offspring heights (daughters additively transmuted) versus the mid-parent height (mothers transmuted). OVERLAY on it, with a different plotting symbol, the corresponding plot involving the multiplicatively transmuted offspring values (on the parent-axis, stay with Galton’s definition of a midparent). (see FAQ) Compare the two, and have a look at Galton’s fitted ellipse, corresponding to a bivariate normal distribution [ Plate X ]) {here, again, we would be more likely to plot the parents’ heights on the horizontal, and the offspring heights on the vertical axis}. For each of the following ‘offspring vs. mid-parent’ correlations, use the ‘mid-parent’ obtained using Galton’s multiplicative method. Calculate (a) the 2 correlations for the 2 unisex versions of the offspring data (b) the sex-specific correlations (i.e., daughters and sons separately) and (c) the single parent-offspring correlation, based on all offspring combined, and their untransmuted heights, ignoring the sex of the offspring. Comment on the correlations obtained, and on the instances where there are big disparities between them. [ a PLOT, with separate plotting symbols for sons and daughters, might help in the case of (c) ] Calculate the 4 correlations (i) father,son (ii) father,daughter, (iii) mother,son and (iv) mother,daughter. Comment on the pattern, and on why you think it turned out this way. Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday October 30. 16.11 Epidemics 16.12 Duplicate Birthdays 16.13 Lottery payoffs 16.14 Chevalier de Méré 16.15 Detecting a fake Bernoulli sequenece 16.16 Cell occupancy 16.17 Life Tables 16.18 Carrier Status (genetics) 16.19 Diagnostic and statistical tests "],
["dalite.html", "Chapter 17 DALITE 17.1 Aim 17.2 How it works", " Chapter 17 DALITE 17.1 Aim 17.2 How it works "],
["references-1.html", "References", " References "]
]
