[
["index.html", "Introduction to Statistical Analysis: a regression-from-the-outset approach Preface 0.1 Target 0.2 Topics/textbooks 0.3 Regression from the outset 0.4 Parameters first, data later 0.5 Let’s switch to “y-bar”, and drop “x-bar”. 0.6 Computing from the outset 0.7 Appendix:", " Introduction to Statistical Analysis: a regression-from-the-outset approach Sahir, Shirin and Jim 2020-06-07 Preface The order in which we are proceeding with the writing, and what we have completed this far We are proceeding somewhat non-linearly in the writing. After completing a first draft of Chapter 2 (an overview of Statistical parameters, parameter constrast and parameter functions) we began on the mother-of-all-regressions chapter for a single mean (ch. 5), and planned to move on the the corresponding ones for a single proportion (ch 6) and event-rate (ch 7). However, we realized that to get through these, students would need the concepts of interval estimates of parameters, and these in turn need other preliminaries. So we have completed 1st drafts of chapters on probability and random variables, but we are hoping to motivate the various statistical concepts and laws by simulation and to use the parallel sessions on computing in R to make these laws more concrete, and less as products of calculus and mathematical statistics. This way we hope to get to serious regression applications sooner than most introductory courses do. And we are also hoping to have the parsimony of a regression approach become evident much sooner than in the ‘traditional t-test chi-sq. regression’ sequence that only reaches regression 3 weeks before the end of the course. We have completed 1st drafts of computing chapters 1, 2 and 3, each with dual learning objectives – computing, and statistical concepts). Having completed the theoretical chapter on random variables, we are now preparing a 4th computing chapter that emphasizes simulations. Once these are done, we will move back to interval estimation, and then back to regression models. In practice, we plan to intruduce the regression early, and use the computing to reinforce/(?replace) the theoretical pre-requisite chapters that (traditionally) seem to hold up so many inytroductory statistics courses. So we see part II as a sidebar^ for part I – i.e., to be consulted as we go along. ^ secondary article accompanying a larger one in a newspaper,&quot; 1948, from side (adj.) + bar (n.1). ^ a typographically distinct section of a page, as in a book or magazine, that amplifies or highlights the main text. ^ a conference between the judge and lawyers out of the presence of the jury. ^ a subordinate or incidental issue, remark, activity, etc. Comments welcomed at any stage! 0.1 Target The target is graduate students in population health sciences in their first year. Concurrently, they take their first courses on epidemiologic methods. The department is known for its emphasis on quantitative methods, and students’ ability to carry out their own quantitative work. Since most of the data they will deal with are non-experimental, there is a strong emphasis on multivariable regression. While some students will have had some statistical courses as undergraduates, the courses start at the beginning, and are pitched at the Master’s level. In the last decade, the incoming classes have become more diverse, both in their backgrounds, and in their career plans. Some of those in the recently begun MScPH program plan to me consumers rather than producers of research; previously, the majority of students pursued a thesis-based Masters that involved considerable statistical analyses to produce new statistical evidence. 0.2 Topics/textbooks For the first term course 607, recent choices have been The Practice of Statistics in the Life Sciences by Baldi and Moore, and Stats by de Veaux, Velleman and Bock. Others that have been recommended are the older texts by Pagano and Gauvreau, and by Rosner. Some of us have also drawn on material in Statistics by Freedman, Pisani, Purves and Adkikari, and Statistical Methods in Medical Research, 4th Edition_ by Armitage,Berry, and Matthews. The newer books have tried to teach the topic more engagingly, by starting with where data come from, and (descriptively) displaying single distributions, or relationships between variables. They and the many others then typically move on to Probability; Random Variables; Sampling Distributions; Confidence intervals and Tests of Hypotheses; Inference about/for a single Mean/Proportion/Rate and a difference of two Means/Proportions/Rates; Chi-square Tests for 2 way frequency tables; Simple Correlation and Regression. Most include a (or point to an online) chapter on Non-Parametric Tests. They typically end with tables of probability tail areas, and critical values. Bradford Hill’s Principles of Medical Statistics followed the same sequence 80 years ago, but in black type in a book that measured 6 inches by 9 inches by 1 inch, and weighed less than a pound. Today’s multi-colour texts are 50% longer, 50% wider, and twice as thick, and weigh 5 pounds or more. The topics to be covered in the second term course include multiple regression involving Gaussian, Binomial, and Poisson variation, as well as (possibly censored) time-durations – or their reciprocals, event rates. Here is is more difficult to point to one modern comprehensive textbook. There is pressure to add even more topics, such as correlated data, missing data, measurement error etc. top the second statistics course. 0.3 Regression from the outset It is important to balance the desire to cover more of these regression-based topics with having a good grounding, from the first term, in the basic concepts that underlie all statistical analyses. The first term epidemiology course deals with proportions and rates (risks and hazards) and – at the core of epidemiology – comparisons involving these. Control for confounding is typically via odds/risk/rate differences/ratios obtained by standardization or Mantel-Haenszel-type summary measures. Teachers are reluctant to spend the time to teach the classical confidence intervals for these, as they are not that intuitive and – once students have covered multiple regression – superceded by model-based intervals. One way to synchronize with epidemiology, is to teach the six separate topics Mean/Proportion/Rate and differences of two Means/Proportions/Rates in a more unified way by embedding all 6 in a regression format right from the outset, to use generalized linear models, and to focus on all-or-none contrasts, represented by binary ‘X’ values. This would have other benefits. As of now, a lot of time in 607 is spent on 1-sample and 2-sample methods (and chi-square tests) that don’t lead anywhere (generalize). Ironically, the first-term concerns with equal and unequal variance tests are no longer raised, or obsessed about, in the multiple regression framework in second term. The teaching/learning of statistical concepts/techiques is greatly enriched by real-world applications from published reports of public health and epidemiology research. In 1980, a first course in statistics provided access to 80% of the articles in NEJM articles. This large dividend is no longer the case – and even less so for journals that report on non-experimental research. The 1-sample and 2-sample methods, and chi-square tests that have been the core of first statistics courses are no longer the techniques that underlie the reported summaries in the abstracts and in the full text. The statistical analysis sections of many of these articles do still start off with descriptive statistics and a perfunctory list of parametric and non-parametric 1 and 2 sample tests, but most then describe the multivariable techniques used to produce the reported summaries. [Laboratory sciences can still get by with t-tests and ‘anova’s – and the occasional ancova’; studies involving intact human beings in free-living populations can not.] Thus, if the first statistical course is to to get the same ‘understanding’ dividend from research articles as the introductory epidemiology course does, that first statistical course needs to teach the techniques that produce the results in the abstracts. Even if it can only go so far, such an approach can promote a regression approach right from week one, and build on it each week, rather than introduce it for the first time at week 9 or 10, when the course is already beginning to wind down, and assignments from other courses are piling up. 0.4 Parameters first, data later When many teachers and students think of regression, they imagine a cloud of points in x-y space, and the least squares fitting of a regression line. They start with thinking about the data. A few teachers, when they introduce regression, do so by describing/teaching it as an equation that connects parameters, constructed in such a way that the parameter-constrast of interest is easily and directly visible. Three such teachers are Clayton and Hills 1995, Miettinen1985, and Rothman 2012. In each case, their first chapter on regression is limited to the parameters and to undersatnding what they mean; data only appear in the next chapter. There is a lot to commend this approach. It reminds epidemiologists – and even statisticians – that statistical inference is about parameters. Before addressing data and data-summaries, we need to specify what the estimands are – i.e, what parameter(s) is(are) we pursuing. Fisher, when introducing Likelihood in 1922, was one of the earliest statisticians to distinguish parameters from statistics. he decried ‘the obscurity which envelops the theoretical bases of statistical methods’ and ascribed it to two considerations. (emphasis ours) In the first place, it appears to be widely thought, or rather felt, that in a subject in which all results are liable to greater or smaller errors, precise definition of ideas or concepts is, if not impossible, at least not a practical necessity. In the second place, it has happened that in statistics a purely verbal confusion has hindered the distinct formulation of statistical problems; for it is customary to apply the same name, mean, standard deviation, correlation coefficient, etc., both to the true value which we should like to know, but can only estimate, and to the particular value at which we happen to arrive by our methods of estimation. [R. A. Fisher. On the Mathematical Foundations of Theoretical Statistics. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, Vol. 222 (1922), pp. 309-368] It is easy and tempting to start with data, since the form of the summary statistic is usually easy to write down directly. It can also be used to motivate a definition: for example, we could define an odds ratio by its empirical computational form ad/bc. However, this ‘give me the answer first, and the question later’ approach comes up short as soon as one asks how statistically stable this estimate is. To derive a standard error or confidence interval, one has to appeal to a sampling distribution. To do this, one needs to identify the random variables involvced, and the parameters that determine/modulate their statistical distributions. Once students master the big picture (the parameter(s) being pursued), the task of estimating them by fitting these equations to data is considerably simplified, and becomes more generic. In this approach more upfront thinking is devoted to the parameters – to what Miettinen calls the design of the study object – with the focus on a pre-specified ‘deliverable.’ 0.5 Let’s switch to “y-bar”, and drop “x-bar”. The prevailing practice, when introducing descriptive statistics, and even to 1 and two sample procedures, is to use the term x-bar (\\(\\bar{x}\\)) for an arithmetic mean (one notable execption is de Veaux at al.) This misses the chance to prepare students for regression, where E[Y|X] is the object of interest, and the X-conditional Y’s are the random variables. Technically speaking, the X’s are not even considered random variables. Elevating the status of the Y’s and explaining the role of the X’s, and the impact of the X distributions on precision might also cut down on the practice of checking the normality of the X’s, even though the X’s are not random variables. They are merely the X locations/profiles at which Y was measured/recorded. When possible, the X distribution should be determined by the investigators, so as to give more precise and less correlated estimates of the parameters being pursued. Switching from \\(\\bar{x}\\) to \\(\\bar{y}\\) is a simple yet meaningful step in this direction. JH made this switch about 10 years ago. 0.6 Computing from the outset In 1980, most calculations in the first part of 607 were by hand calculator. Computing summary statistics by hand was seen as a way to help students understand the concepts involved, and the absence of automated rapid computation was not considered a drawback. However, doing so did not always help students understand the concept of a standard deviation or a regression slope, since these formulae were designed to minimize the number of keystrokes, rather than to illuminate the construct involved. For example, it was common to rescale and relocate data to cut down on the numbers of digits entered, to group values into bins, and use midpoints and frequencies. It was also common to use the computationally-economical 1-pass-through-the-data formula for the sample variance \\[s^2 = \\frac{ \\sum y^2 - \\frac{(\\sum y)^2}{n}}{n-1},\\] even though the definitional formula is \\[s^2 = \\frac{\\sum(y - \\bar{y})^2}{n-1}.\\] The latter (definitional) one was considered too long, even though having to first have to compute \\(\\bar{y}\\) and then go back and compute (and square) each \\(y - \\bar{y}\\) would have helped students to internalize what a sample variance is. When spreadsheets arrived in the early 1980s, students could use the built-in mean formula to compute and display \\(\\bar{y}\\), another formula to compute and display a new column of the deviations from \\(\\bar{y}\\), another to compute and display a new column of the squares of these deviations, another to count the number of deviations, and a final formula to arrives at \\(s^2.\\) The understanding comes from coding the definitional formula, and the spreadsheet simply and speedily carries them out, allowing to user to see all of the relevant components, and from noticing if each one looks reasonable. Ultimately, once students master the concept, they could move on to built-in formulae that hide (or themselves avoid) the intermediate quantities. Few teachers actually encouraged the use of spreadsheets, and instead promoted commercial statistical packages such as SAS, SPSS and Stata. Thus, the opportunity to learn to `walk first, run later’ afforded by spreadsheets was not fully exploited. RStudio is an integrated environment for R, a free software environment for statistical computing and graphics that runs on a wide variety of platforms. Just like spreadsheet software, one can use R not just as a calculator, but as a programmable calculator, and by programming them, learn the concepts before moving on to the built-in functions. There is a large user-community and a tradition of sharing information and ways of doing things. The graphics language contains primitive functions that allow customization, as well as higher-level functions, and is tighly integrated with the statistical routines amd data frame functions. R Markdown helps to foster reproducible research. Shiny apps allow interactivity and visualization, a bit like ‘what-ifs’ with a spreadsheet. It takes practice to become comfortable with R. Gor those less mathematical, it is somewhat more cryptic than, and not quite as intuitive as, other packages. For the last several years, the department has offered a 13 hour course introduction to R in first term. Initially the aim was to prepare students for using it in course 621 in second term, but in the Fall 2018 and 2019 offerings of course 607, computing with R and use of R Studio became mandatory. Just as the epidemiology material in the Fall is shared between 2 courses (601 and 602), the aim will be to also spread the statistics material over 607 and 613, and to integrate the two more tightly. As an example, the material on ‘descriptive’ (i.e., not model-based) statistics and graphical displays will be covered in 613, while 607 will begin with parameters and models. Rather than treat computing as a separate activity, exercises based on 607 material will be carried out as part of 613 classes/tutorials. The statistical material will be used to motivate the computer tasks. 0.7 Appendix: [Still rough] History of current introductory biostatistics courses The senior author first taught a 2-course sequence for first year graduate students in epidemiology in 1980, using Colton’s Statistics in Medicine as the text for the introductory course (607). He developed his own notes for the second course, which covered multiple regression for quantitative responses. Over the next 10 years, he continued to teach the first course – first from Colton, but latterly from Moore and McCabe (and undergraduate text) and with epi statistics from Armitage and Berry and some other fundamentals from Freedman (Statistics). Stan S taught the second (621 Data Analysis in the Health Sciences), mostly from Kleinbaum’s Applied Regression Analysis and Other Multivariable Methods. In the 1990s, Lawrence J taught 607, and Michal A 621. Neither used a required textbook. LJ developed an extensive set of written notes (still available on his website) (and contributed a chapter Introduction to Biostatistics: Describing and Drawing Inferences from Data book on Surgical Arithmetic) while MA used transparencies that were widely photocopied. 2000s Robert P 621? LJ 621 Meanwhile JH taught to summer students (mostly medical residents and fellows): 607 and a second course (678, Analysis of Multivariable Data). Both sets of content are available on JH’s website. He last taught the Fall version of 607 in 2001, when LJ was on sabbatical. 607: Tina 2006 - 201x Erica M; 20xx - Paramita SC. 2018, 2019 Sahir B 621: Aurelie Alexandra 2020 Shirin "],
["introduction.html", "Chapter 1 Introduction 1.1 Goals 1.2 Structure 1.3 Attitudes, etc….", " Chapter 1 Introduction 1.1 Goals Blah 1.2 Structure Blah Blah 1.3 Attitudes, etc…. Blah Blah Blah "],
["paras.html", "Chapter 2 Statistical Parameters 2.1 Parameters 2.2 Parameter Contrasts 2.3 Parameter functions 2.4 Phraseology to avoid 2.5 SUMMARY 2.6 Exercises 2.7 References", " Chapter 2 Statistical Parameters 2.1 Parameters The objectives of this chapter are to Define what a parameter is in a statistical context See examples of such parameters Understand the concept of a parameter relation or a parameter equation Be able to set up parameter equations that isolate and directly pinpoint parameter differences in both the absolute and relative scales, using a regression equation framework. Do so before fitting any such (regression) equations to data, so that we can focus on the research objects without having data get in the way. See the unity (generality) in what we will be doing in the course, by seeing the big picture, i.e., the forests, not the trees. We begin by defining is meant by the term parameter in a statistical context Parameter – A constant (of unknown magnitude) in a (statistical) model. [OSM2011, p60] In Statistics. A numerical characteristic of a population, as distinguished from a statistic obtained by sampling.[OED] Note that the term can mean other things in other contexts. For example, in clinical medicine, Parameter – any quantitative aspect/dimension of the client’s (patient’s) health, subject to measurement (by means of a test). (Example: systolic blood-pressure.)[OSM, Terms and Concepts of Medicine] The (statistical) parameters we will be concerned with \\(\\mu\\) The mean level of a quantitative characteristic, e.g. the depth of the earth’s ocean or height of the land, or the height / BMI / blood pressure levels of a human population. [One could also think of mathematical and physical constants as parameters, even though their values are effectively ‘known.’ Examples where there is agreement to many many decimal places include the mathematical constant pi, the speed of light(c), and the gravitational constant G. The speed of sound depends on the medium it is travelling through, and the temperature of the medium. The freezing and boiling points of substances such as water and milk depend on altitude and barometric pressure]. At a lower level, we might be interested in personal characters, such as the size of a person’s vocabulary, or a person’s mean (or minimum, or typical) reaction time. The target could be a person’s ‘true score’ on some test – the value one would get if one (could, but not realistically) be tested on each of the (very large) number of test items in the test bank, or observed/measured continously over the period of interest.     Later on we will address sitautions where the mean \\(\\mu\\) is not the best ‘centre’ of a distribution, and why we might want to take some other feature, such as the median, or some other quantile, instead. \\(\\pi\\) Prevalence or risk (proportion): e.g., proportion of the earth’s surface that is covered by water, or of a human population that has untreated hypertension, or lacks internet access, or will develop a new health condition over the next x years. At a lower level, we might be interested in personal proportions, such as what proportion of the calories a person consumes come from fat, or the proportion of the year 2020 the person spent on the internet, or indoors, or asleep, or sedentary. \\(\\lambda\\) The speed with which events occur: e.g., earthquakes per earth-day, or heart attacks or traffic fatalities per (population)-year. At a lower level, we might be interested in personal intensities, such as the mean number of tweets/waking-hour a person issued during the year 2020, or the mean number of times per 100 hours of use a person’s laptop froze and needed to be re-booted. Each of these three parameters refers to a characteristic of the overall domain, such as entire surface of the earth, or the entire ocean, or population. There are no indicators for distinguishing among subdomains, so they refer to locations / persons not otherwise specified. We will drill down later. Especially for epidemiologic research, and also more generally, one can think of \\(\\pi\\) and \\(\\lambda\\) as parameters of occurrence. [Although the word occurrence usually has a time element, it can also be timeless: how frequently a word occurs in a static text, or a mineral in a rock.] Prevalence is the proportion in a current state, and the 5-year risk is the expected proportion or probability of being in a new state 5 years from now. The parameter \\(\\lambda\\) measures the speed with which the elements in question move from the original to the other state. Even though the depths of the ocean, and blood pressures, are measured on a quantitative (rather than on all or none) scale, one can divide the scale into a finite number of bins/caterories, and speak of the prevalence (proportion) in each category. Conversely, one can use a set of descriptive parameters called quantiles, i.e, landmarks such that selected proportions, e.g., 0.05 or 5%, 25%, 50%, 75%, 95% of the distribution are to the left of (‘below’) these quantiles. Occurence Parameters are not constants of nature [OSM1995] It has been noted in the philosophy of science that any science is concerned with functional relations of its objects (Friend and Feibleman, 1937). This proposition is quite evidently tenable for epidemiologic objects of research. Parameters of occurrence, such as the incidence rate for a particular illness, are not constants of nature. Rather, their magnitudes generally depend on — are functions of — a variety of characteristics of individuals — constitutional, behavioral, and/or environmental. Such relations, even if only remotely credible, are generally the objects of medical occurrence research. For example, one is quite usually interested in learning whether the rate of occurrence of some particular illness depends on (is related to or is a function of) gender — regardless of whether there is any express reason to surmise that it might be. EXAMPLE 1.5 The prevalence of any given blood type based on the ABO antigen system,, while constant over gender and essentially constant over age, is not a constant of nature. It varies by ethnic groupings, for example. Thus the prevalence must be quantified in relation to—as a function of—ethnic group. EXAMPLE 1.6. For the occurrence of various values of blood pressure among people, one descriptive parameter is the median of the pressure. (This is a value such that the prevalence of its exceedance is 50%.) This parameter, again, is not a constant of nature but depends on age and other characteristics of individuals. For the quantitative nature of the age relation of systolic blood pressure, a rule of thumb used to be that it is, in mm Hg, 100 plus age in years.&quot; This rule expresses a regression model - a regression function - of the form P = A + B x Age. In this example, P, the occurrence parameter, is the median of systolic blood pressure, A = 100 mm Hg, and B = 1 mm Hg/yr. The characteristics on which the magnitude of an occurrence parameter depends (causally or otherwise) are determinants of the parameter. Thus, in the examples given above, ethnic grouping is a determinant of prevalence of any given blood type, and age is a determinant of the median of systolic blood pressure. “Determinant” has no implication as to causality in science — any more than in everyday locution: the current age of a person is “determined” by his/her year of birth (noncausally), just as the expected outcome of a disease is “determined” by the treatment that is used (causally). The relation of an occurrence measure to a determinant, or a set of determinants, is naturally termed an occurrence relation or an occurrence function. These relations are in general the objects of epidemiologic research. [Even though the general inconstancy of occurrence parameters leads to the consideration of occurrence relations, this latter outlook affords only a partial accommodation of the inconstancy, because occurrence relations the degree also vary according to the type of individual. In particular, measures of a relation (Appendix 2) have determinants of their own.] Before we start, a comment on terminology Before we go on, we need to adopt sensible terminology for referring generically to the states, traits, conditions or behaviours whose category-specific parameter values are being compared. Following OSM (see above) we will use the term `determinant’. It has several advantages over the many other terms used in different disciplines, such as exposure, agent, independent/explanatory variable, experimental condition, treatment, intervention, factor, risk factor, predictor. The main advantage is that it is broader, and closer to causally neutral in its connotaion. Exposure has environmental connotations, and technically refers to an opportity to injest or mentally take on board a substance or message. Agent has causal connotations. The term independent variable suggests the investigator has control over it in a laboratory setting. The term explanatory is ambiguous as to the mechanism by which the parameter value in the index category got to be different from the value in the index category. Not all contrasts are experimentally formed. The term factor, and thus the term risk factor, are to be avoided because the word factor derives from the Latin facere, (the action of) doing, making, creating. Predictor makes one think of the future. The term regressor (or its shorthand, the ‘X’ ) won’t be understood by lay people. While the word ‘determine’ can suggest causality (e.g., demand determines the price), it also refers to ‘fixing the form, position, or character of beforehand’: two points determine a straight line; the area of a circle is determined by its radius. There is considerable philosophical debate as to whether something ‘causes’ something else. Some would argue that the extent to which genetics determines one’s personality is a causal concept. Others argue that since one cannot cannot consider the alternative, ones biological sex or age can not be considered a causal determinant or a risk factor (in the strict causal meaning of the word). They prefer to refer to them as risk indicators. We now move on to the parameter relations we will be concerned with, beginning with the simplest type. 2.2 Parameter Contrasts In applied research, we are seldom interested in a single constant. Much more often we are interested in the contrast (difference) between the parameter values in different contexts/locations (Northern hemisphere vs Southern hemisphere), conditions/times (reaction times using the right versus left hand, or behaviour on weekdays versus weekends), or sub-domains or sub-populations (females vs males). Contrasts involving ‘persons, places, and times’ have a long history in epidemiology. In this section, we will limit our attention to ‘contrasts’: a compariosn of the parameter values between 2 contexts/locations/sub-populations. Thus (unlike in OSM’s example 1.6) the parameter function has just 2 possible ‘input’ values. The next section will address more general parameter functions. ‘Reference’ and ‘Index’ categories In many research contexts, the choice of ‘reference’ category (starting point, the category against which the other category is compared) will be obvious: it is the status quo (standard care, prevailing condition or usual situation, dominant hand, better known category). The ‘index’ category is the category one is curious about and wishes to learn more about, by contrasting its parameter value with the parameter value for the reference category.     In other contexts, it is less obvious which category should serve as the reference and the index categories, and the choice may be merely a matter of persepctive. If one is more famiar with the Northern hemisphere, it serves as a natural starting point (or ‘corner’ to use the terminology of Clayton and Hills, or reference category). The choice of reference category in a longevity contrast between males and females, or in-hospital mortality rates or motor vehicle fatality rates during weekends versus weekdays, might depend on what mechanism one wishes to bring out. Or one might close as the reference category the one with the larger amount of experience, or maybe the one with the lower parameter value, so that the ‘index minus reference’ difference would be a positive quantity, or the ‘index: reference ratio’ exceeds 1. 2.2.1 Parameter relations in numbers and words To make this concrete, we will use hypothetical (and very round) numbers and pretend we ‘know’ the true parameter values – in our example of the mean depth of the ocean in the Northern hemisphere (reference category) and Southern hemisphere (index category) – to be 3,600 metres (3.6Km) and 4,500 metres (4.5Km) respectively. Thus, the difference (South minus North) is 900 metres or 0.9Km. If we wished to show the two parameter values graphically, we might do so using the format in panel (a), which shows the 2 hemisphere-specific parameter values – but forces the reader to calculate the difference. Panel (b) follows a more reader-friendy format, where the difference (the quantity of interest) is isolated: the original 2 parameters are converted to 2 new, more relevant ones. Panel (c) encodes the relation displayed in panel (b) in a single phrase that applies to both categories: Onto the ‘starting value’ of 3.8Km, one adds \\(\\Delta \\mu\\) = 0.9 Km only if the resulting parameter pertains to the Southern hemisphere. The 0.9 Km is toggled off/on as one moves from North to South. 2.2.2 Parameter relations in symbols, and with the help of an index-category indicator Panels (a) and (b) in the following figure repeat the information in panels (a) and (b) in the preceding Figure, but using Greek letters to symbolically represent the parameters. Just to keep the graphics uncluttered, the labels North and South are abbreviated to N and S and used as subscripts. Also, for brevity, the expression \\(\\Delta \\mu\\) denotes \\(\\mu_S - \\mu_N\\). The relation encoded in a single phrase shown in the previous panel (c) has a compact form suitable for verbal communication. The representation can be adapted to be more suitable for computer calculations. (The benefit of doing this will become obvious as soon as you try to learn the parameter values by fiiting these models to actual data.) Depending on whether the hemisphere in question is the northern or southern hemisphere, the expression/statement ‘the specified hemisphere is the SOUTHERN hemisphere’ evaluates to a (logical) FALSE or TRUE. In the binary coding used in computers, it evaluates to 0 or 1, and we call such a 0/1 variable an ‘indicator’ variable.^ ^ In ‘better families’ we speak of INDICATOR variables, not DUMMY variables. The International Statistical Institute’s Dictionary of Statistical Terms objects to the name: the term is ‘used, rather laxly, to denote an artificial variable expressing qualitative characteristics …. [The] word ’dummy’ should be avoided.’ Miettinen’s Epidemiological Research: Terms and Concepts:- Indicator variate – A variate with 0 and 1 as its (only) realizations, with realization 1 indicating something particular. (Examples: Y = 1 indicating membership in the case series of person-moments and X1 = 1 indicating index category of the etio-genetic determinant in an etiogenetic study – in the logistic model for the object of study.) Dummy variate (synonym: indicator variate) – See ‘Indicator variate’ in section II – 2. Note: This term is a misnomer: there is nothing dummy about an indicator variate. We encourage you to use, in your coding, meaningful variable names such as i.South or i.Southern (where i stands for indicator of) or i.Male. Don’t use the name sex or gender, where the coding is not self evident. If you think i.Male is over doing it, then use Male. In panel (c) in the following figure, just to keep the graphics uncluttered, the name of the indicator variable SOUTHERN is abbreviated to S, and \\(\\mu_S\\) is shorthand for the \\(\\mu\\) cooresponding to whichever value (0 or 1) of \\(S\\) is specified (we could also write it as \\(\\mu | S\\), or \\(\\mu\\) ‘given’ \\(S\\). ) Thus, the symbol \\(\\mu_0\\) refers to the \\(\\mu\\) when \\(S=0\\), or in longerhand, to \\(\\mu \\ | \\ S = 0\\). What does the equation in panel (c) remind you of? Probably the equation of a line. In high school you may have learned it in the form \\(A + B \\times X\\) that Miettinen used to describe the relation between median blood pressure and age. Today, in statistics, these equations are referred to as regression equations, and the statistical model is called a regression model. The term ‘regression’ is unfortunate, since it bears little relation to the original application. It concerned the phenomenon of ‘reversion’ first described by Charles Darwin. Following his first studies of the sizes of the ‘daughter’ seeds of sweet peas, his nephew Francis Galton, described the tendency: offspring did not tend to resemble their parent seeds in size, but to be always more mediocre (‘middling’, or closer to the mean) than they — to be smaller than the parents, if the parents were large; to be larger than the parents, if the parents were very small (Galton 1886) One of the first ‘regression’ lines fitted to human data is Galton’s line depicting the ‘rate of regression on hereditary stature’ where, using the term ‘deviate’ where today we would use ‘Z-scores.’ The Deviates of the Children are to those of their Mid-Parents as 2 to 3. (Galton, 1886) Because he used z scores (so the means in the parents and in the children were both 0) the equation of the line simplified to \\[\\mu(\\textrm{Z-score in children of parents with mean z}) = 0 + (2/3) \\times z\\] But don’t we need a cloud of points to have a regression line? Although many courses and textbooks introduce regression concepts this way, the answer is NO. There is nothing in the regression formulation that specifies at which ‘X’ values the mean Y values at these X values are to be determined. Unlike many textbboks that start with Xs on a ‘continuous’ scale, and then later have to deal with a 2-point (binary) X, we are starting with this simplest case, and will move ‘up’ later. We are doing this for a few reasons: in epidemiology, the first and simplest contrasts involve just two categories, the reference category and the index category; a simple subtraction of 2 parameter values is easier to do and to explain to a lay person; and there is no argument about how the function behaves at the values between 0 and 1. There are no parameter values at Male = 0.4 or Male = 1.4, they are only at Male=0 and Male=1. In addition, it is easier to learn the fundamental concepts and principles of regression if we can easily ‘see’ what exactly is going on. Fewer blackbox formulae mean more transparency and understanding. Once we see how to represent parameter values in two determinant-categories, we can easily extend it to more than two, such as the ethnic groups in example 1.5 above. As we will see later on, when we have a value for a dental health parameter (eg the mean number of decayed, missing and filled DMF teeth) at X = 0 parts per million of fluoride in the drinking water, and another parameter value at X = 1 parts per million, we can only look at these 2 parameter values. If this is not enough, we would need to have (obtain) parameter values at the intermediate fluoride levels, or levels beyond 1 ppm, to trace out the full parameter relation, namely how the mean-DMF varies as a function of fluoride levels. If we have large numbers of observations at each level, then the DMF means will trace out a smooth curve. If data are limited, and the trace is jumpy/wobbly, we will probably resort to a sensible smooth function, the coefficients of which will have to be estimated from (fitted to) data. This discussion leads on naturally to situations where the parameter varies over quantitative levels of a determinant - a topic considered in the next section. But meantime, we need to answer this question: why limit ourselves to subtraction? why not consider the ratio of the two parameters, rather than their difference? Relative differences (ratios) – in numbers first A ratio can be more helpful than a difference, especially if you are don’t have a sense of how large the parameter value is even in the reference category. As an example, on average, how many more red blood cells do men have than women? or how much faster are gamers’ reaction times compared with nongamers? Recall our hypothetical mean ocean depths, 3.6 Km in the oceans in the Northern hemisphere (reference category) and 4.5Km in the oceans of the Southern hemisphere (index category). Thus, the S:N (South divided by North) ratio is 4.5/3.6 or 1.25. Panel (a) leaves it to the reader to calculate the ratio of the parameter values. In panel (b) the ratio (the quantity of interest) is isolated: again, the original 2 parameters are converted to 2 new, more relevant ones. Again, panel (c) shows a single master-equation that applies to both hemispheres by togging off/on the ratio of 4.5/3.6. Relative differences (ratios) – expressed in symbols and with the help of the index-category indicator To rewrite these numbers in a symbolic equation suitable for a computer, we again convert the logical ‘if South’ to a numerical Southern-hemisphere-indicator, using the binary variate \\(S\\) (short for Southern) that takes the value 0 if the Northern hemisphere, and 1 if the Southern hemisphere. But go back to some long-forgotten mathematics from high school to be able to tell the computer to toggle the ratio off and on. Recall ‘powers’ of numbers, where, for example, ‘\\(y\\) to the power 2’, or \\(y^2\\) is the square of \\(y\\). The two powers we exploit are 0 and 1. ‘\\(y\\) to the power 1’, or \\(y^1\\) is just \\(y\\) and ‘\\(y\\) to the power 0’, or \\(y^0\\) is 1. We take advantage of these to write \\[\\mu_S = \\mu \\ | \\ S \\ = \\mu_0 \\ \\times \\ \\Big\\{ \\frac{\\mu_{South}}{\\mu_{North}}\\Big\\}^S = \\mu_0 \\ \\times Ratio \\ ^ S.\\] You can check that it works for each hemisphere by setting \\(S=0\\) and \\(S=1\\) in turn. Thus, \\[\\log(y^S) = S \\times \\log(y)\\] Although this is a compact and direct way to express the parameter relation, it is not well suited for fitting these equations to data. However, in those same high school mathematics courses, you also learned about logarithms. For example, that \\[\\log(A \\times B) = \\log(A) + \\log(B); \\ \\ \\log(y^x) = x \\times \\log(y).\\] Thus, we can rewrite the equation in panel (c) as \\[\\log(\\mu_S) = \\log(\\mu \\ | \\ S) \\ = \\underbrace{\\log(\\mu_0)} \\ + \\underbrace{\\log(Ratio)} \\times \\ S.\\] This has the same ‘linear in the two parameters’ form as the one for the parameter difference: the parameters are \\(\\underbrace{\\log(\\mu_0)}\\) and \\(\\underbrace{\\log(Ratio)}\\) and they are made into the following ‘linear compound’ or ‘linear predictor’ (see Remarks below) : \\[\\log(\\mu_S) = \\log(\\mu \\ | \\ S) \\ = \\underbrace{\\log(\\mu_0)} \\times \\ 1 \\ + \\underbrace{\\log(Ratio)} \\times \\ S.\\] The course is concerned with using ‘regression’ software to ‘fit’/‘estimate’ these 2 parameters from \\(n\\) depth measurements indexed by \\(S\\). 2.3 Parameter functions A very simple example of a function that describes how parameter values vary over quantitative levels of a determinant is the straight line shown in the upper right panel of the next figure. Here the determinant has the generic name X, and the equation is of the \\(A + B \\times X\\) or \\(B_0 + B_1 \\times X\\) or \\(\\beta_0 + \\beta_1 \\times X\\) straight line form. Miettinen used the convention that the upper case letters \\(A\\) and \\(B\\) are used to denote the (true but unknown) coefficient values, whereas the lower case leters \\(a\\) and \\(b\\) are used to denote their empirical counterparts, sometimes called estimated coefficients or fitted coefficients. This sensible and simple convention also avoids the need, if one uses Greek letters for the theoretical coefficient values, to put ‘hats’ on them when we refer to their empirical counterparts, or ‘estimate/fit’ them. Fortunately, journals don’t usually allow investigators to use ‘beta-hats’; but this means that the investigators have to be more careful with their words and terms. As we go left to right in the following grid, the models become more complex. The simplest is the one of the left, in column 1, the one JH refers to as ‘the mother of all regression models.’ It refers to a single or overall situation/population/domain, so \\(X \\equiv 1\\), it takes on the value 1 in/for every instance/member. So the parameter equation is \\(\\mu_X = \\mu \\ | \\ X \\ = \\mu \\times 1.\\) In column 2, there are 2 subdomains, indexed by the 2 values of the determinant (here generically called ‘X’), namely \\(X = 0\\) and \\(X = 1\\). In the 3rd column, the number of of parameters is left unspecified, since the numbers of coefficients to specify a line/curve might vary from as few as 1 (if we were describing how the volume of a cube dependeds or, was is function of, its radius) to 2 (for a straight line that did not go through the origin, or for a symmetric S curve) to more than 2 (e.g., for a non-symmetric S curve, or a quadratic shape). A few more remarks on the panels in this Figure The 3 rows refer to the 3 core parameters we have given examples of above. All 3 are governed by the same principles, although there are more possibilities of different possible scales for some parameters. In setting (column) 1: there is just 1 parameter (value shown as a dot) corresponding to the ‘overall’ population or the entire domain. You can think of it as the limiting or ‘degenerate’ case of the columns to its right. One can still write it in a ‘regression’ model. It is of the form P(arameter) = \\(B\\), involving no indicators for distinguishing among subdomains of the referent domain of the distribution, say adults not otherwise specified. [MSH2018, p63] It is sometimes referred to as a null or ‘incercept only’ regression model.     We will exploit this idea to take a more holistic/general and economical approach to this introductory course. Many textbooks/courses do not mention regression models until quite late, and spend a lot of time on ‘1-sample’ (and even ‘2-sample’) problems without pointing out that these are merely sub-cases of regression models. This ‘silos’ practice of promoting/learning a separate software routine for dealing with a 1 sample problem, when one can get the same answer from a regression routine, leads to dead ends and wastes time.     Once we get to fitting/estimating a mean (or proportion or rate) parameter to/from data, we will encourage doing so within a regression framework. In setting (column) 2, there are 2 parameter values, one for the reference category and one for the index category of the determinant. As we have seen, how they relate to each other can be can be expressed in a number of different ways. A common and useful way is via a parameter equation that contains a parameter for the reference category and a comparative parameter (some measure of the difference between the two parameters) – the latter is often of most interest. In setting (column) 3, the parameter equation traces the parameter over a continuum of possible values of the determinant, using as many coefficients as are needed. In this particular diagram, the values of the determinant (X) are shown starting at X = 0, but this does not have to be. In data analysis, one often shifts the X origin, so that the ‘intercept’ makes more sense. For example, if one was plotting world temperatures, or ice-melting dates (see Chapter on Computning) against calendar year, it would be better to have the incercept refer to the fitted temperature for when the series begins, rather than when our current Western calendat begins (at the year 0 AD). Likewise, if we were describing the relation between ideal weight and height it is good to start near where people’s heights are. Thus, ‘100 pounds for a height of 5 feet, with five additional pounds for each added inch of height’ for women, and ‘106 pounds for a height of 5 feet, and six additional pounds for every added inch of height.’ for men. Of course, if you wish, for women you could use the mathematically equivalent ‘-300 pounds for a height of 0 feet, with five additional pounds for each added inch of height’ but it is not that easy to remember, and doesn’t apply for much of the (unspecified) height range! A few remarks on associated terminology Instead of ‘regression models’, some textbooks and courses refer to ‘linear models’ : Linear model: Formulation of the mean/‘expectation’ of (the distribution of) a random variate (Y) as a linear compound of a set \\(B_0 , B_1 , B_2 , \\dots\\) of parameters: as \\(B_0 + B_1 X_1 + B_2 X_2 + \\dots\\) [Miettinen 2011, p54] The meaning of ‘linear’ in the appellation of this model has nothing to do with straight lines; it refers to the mathematical concept of ‘linear compound’: given quantities Q\\(_1\\), Q\\(_2\\), etc., a linear compound of these is the sum C\\(_1\\)Q\\(_1\\) + C\\(_2\\)Q\\(_2\\) + …, where C\\(_1\\) etc. are the ‘coefficients’ that define a particular linear compound of the set of quantities constituted by the Qs. So, the ‘general linear model’ is linear in the sense that the dependent parameter, M, is formulated as a linear compound of the independent parameters B0, B1, etc., the coefficients in this linear compound being 1, X1, etc. The model is, in this way, ‘linear in the parameters.’ [MSH2018, p65] Statistics courses in the social sciences, the biological laboratory sciences, and other experimentally-based sciences, typically move on from 1- and 2-sample procedures (unfortunately, mainly focusing on statistical tests) to ‘analysis of variance’ models Miettinen explains an ’analysis of variance models this way: In the ‘analysis of variance model,’ the random variate at issue – Gaussian – has a mean whose value depends on a nominal-scale determinant, a nominal scale being characterized by discrete categories without any natural order among them. The names of the (nominal) categories, some N in number, could be Category 1, Category 2, … , Category N. The term for the model is a misnomer. For, at issue is not analysis but synthesis of data, and the synthesis is not directed to learning about the variance of the random variate; it focuses on the mean, the relation of the mean to the (nominal-scale) determinant of it. A simple example of these models might address the mean of systemic blood-pressure – defined as the weighted average of the diastolic and systolic pressures with weights 2/3 and 1/3, respectively – in relation to ethnicity, represented by three categories. An ‘analysis-of- variance’ model would define a random variate (Y) as representing the numerical value of the pressure (statistical variates inherently being numerical) and having a Gaussian distribution with means M1, M2, and M3 in those ethnicity categories 1, 2, and 3, respectively, with the variance of the distribution invariant among them. The random variate (Y) is the ‘dependent’ variate in the meaning that the value of its mean depends on ethnicity; and the ethnicity categories are represented in terms of suitably-defined ‘independent’ – non-random – variates (Xs). The form of the ‘analysis-of-variance’ model in this simple example is: \\(M = B_0 + B_1 X_1 + B_2 X_2\\), where \\(M\\) is the mean of \\(Y\\) and the two independent variates are indicators of two particular ones of the three ethnic categories. One possibility in this framework is to take \\(X_1\\) and \\(X_2\\) to be indicators of Category 2 and Category 3, respectively – an indicator variate being one that takes on the value 1 for the category it indicates, 0 otherwise. In terms of this model, \\(B_0\\) is the value of \\(M\\) when \\(X_1 = X_2 = 0\\), that is, for Category 1 (i.e., \\(B_0 = M_1\\)); and for Category 2 and Category 3 the values of \\(M\\) are represented by \\(B_0 + B_1\\) and \\(B_0 + B_2\\), respectively (i.e., \\(M_2 = B_0 + B_1\\), and \\(M_3 = B_0 + B_2\\)). Thus, the difference between \\(M_1\\) and \\(M_2\\) is represented by \\(B_1\\); \\(B_2\\) represents the difference between \\(M_1\\) and \\(M_3\\); and the difference between \\(M_2\\) and \\(M_3\\) is the difference between \\(B_1\\) and \\(B_2\\). In this ‘analysis-of-variance’ framework it is feasible to accommodate, jointly, whatever number of nominal-scale determinants of the magnitude of the mean of the dependent variate. A simple example of this is the addition of the two categories of gender for consideration jointly with the three categories of ethnicity. These two determinants jointly imply a single nominal-scale determinant with six categories (as each of the three categories of ethnicity is split into two subcategories based on gender). When involved in the definition of the independent variates is only a single determinant, the model is said to be for ‘one-way analysis of variance’; with two determinants the corresponding term (naturally) is ‘two-way analysis of variance’; etc. 2.4 Phraseology to avoid It is quite common to hear a regression coefficient (fitted or theoretical) interpreted this way: “For every 1 unit increase in X, the ‘Y’ parameter increases by \\(\\beta_X\\) units.” or as follows “As (when) you increase X by 1 unit, you increase the Y parameter by \\(\\beta_X\\) units.” We pick up this terminology very early, maybe even back in high school, and from other people around us. But, in interpreting the B = 1 mm Hg/yr in Miettenen’s example (100 plus age in years), should we use such phrases? Or, since you don’t know the source of, or the data behind this rule, you can take a look at the distributions of some anthropometric chacacteristics (height, weight, forced expiratory volume, FEV) measured cross-sectionally, in different populations – Busselton, Australia and rural Southwest Ethiopia – in 1972 and 1992. By eye, try to estimate the slope you would get if you regressed the age-and sex-specific means or medians on the ages. and then summarize the gradient across age. Remember that these these subjecsts aren’t aging or going anywhere, and nobody was watching them age. It is more accurate to say: People who were aged a+1 years at the time of the survey had heights/weights/FEVs that were t.tt units higher/lower than people who were aged a years. or The mortality rate was u.uu units higher/lower (or u.u times higher/lower) in the experience in the index category than the reference category. This way, you are telling the reader that this is a static source, and not a dynamic situation where conditions are being manipulated by the investigators, or the subjects being watched as their ages go up [for many readers, the word ‘increased’ implies that some human force deliberately changed the dial, and turned the X up or down, as one could do with temperature or humidity in a laboratory.] One of JH’s favourite examples of people being misled into thinking that a cross-sectional dataset allows you to say that ‘as people get older, they …’ is the McGill epidemiology department’s studies, in the 1960s, on the health of the more than 10,000 millers and miners of asbestos. These workers were born between 1890 and 1920. In cross-sectional studies, there were gradients in mean height across attained age. It would be easy to give them a ‘as people get older, they shrink in height’ or they ‘gain in height’ interpretation. It is easy to overlook the fact that some of these were children and adolescents during the Depression. Next Chapter The next chapter will begin in the upper left corner of the grid, and address situations where the estimand (the parameter to be estimated) is \\(\\mu.\\) It will describe how we ‘estimate’ / ‘fit’ a single \\(\\mu\\) parameter from/to a finite number of observations, and how we quantify and report how far off the target our method of estimation can/might be. 2.5 SUMMARY 2.6 Exercises So far, we have only dealt with equations involving a difference and the ratio of two \\(\\mu\\) parameters. Extend the graphs and the equations (for the difference of means and the ratio of means) to the \\(\\pi\\) parameter. Use as an example the proportions of the surfaces of the Northern (reference category) and Southern hemisphere (index category) covered by water, i.e, \\(\\pi_{North}\\) and \\(\\pi_{South}.\\) Use the hypothetical values \\(\\pi_{North} = 0.65\\) and \\(\\pi_{South} = 0.75.\\) Instead of focusing on the proportions covered by water, focus on the proportions covered by land. How does the difference of the two proportions relate to the difference calculated in 1? How does the ratio of the two proportions relate to the ratio calculated in part 1? i.e., is one the reciprocal of the other? Can you think of different scale, where the ratio when the focus is land IS just the reciprocal of the ratio when the focus is water.? If you can, show that the log of the ratio when the focus is land IS just the negative of the log of the ratio when the focus is water.? Extend the graphs and the equations (for the difference of means and the ratio of means) to the \\(\\lambda\\) parameter. Use as an example the mean number of earthquakes per year in the Northern (reference category) and Southern hemisphere (index category) , i.e, \\(\\lambda_{North}\\) and \\(\\lambda_{South}.\\) Use the hypothetical values \\(\\lambda_{North} = 5.0\\) and \\(\\lambda_{South} = 7.5\\) 2.7 References Olli S. Miettinen. Theoretical epidemiology: Principles of Occurrence Research in Medicine. Wiley, New York, 1985. Chapter 1: The study of occurrence patterns in medicine. Introduction. David Clayton and Michael Hills. Statistical Models for Epidemiology. Oxford University Press, 1993. Chapter 22: Introduction to regression models. Kenneth J. Rothman. Epidemiology: An introduction. Oxford University Press, 2012. Chaper 12: Using regression models in epidemiologic analysis. Olli S. Miettinen, Johann Steurer, Albert Hofman. Clinical Research Transformed. Springer, 2019. Chapter 7: The Logistic Regression Model (The Precursors of the General Linear Model; The General Linear Model; The Generalized Linear Model; The Logistic Regression Model) "],
["inference.html", "Chapter 3 Statistical Inference 3.1 The Bayesian Approach 3.2 Frequentist approach 3.3 Does the approach matter?", " Chapter 3 Statistical Inference The objectives of this chapter are to Understand what is meant by statistical inference Appreciate the fundamental difference between the Bayesian and Frequentist approaches In the Bayesian approach, the parameter is the subject of the statement. Statements are of the form: if, before observing the data, the parameter \\(\\theta\\) was thought to have this probability distribution, then post-data, \\(\\theta\\) should be thought of as having THIS probability distribution. In the Frequentist approach, the statements are indirect. They are (conditional) probabilistic statements about the data and about the performance of the procedure used to bracket parameter values. A variant ranks the possible parameter values according to how probable the observed data would be under each of these, but does not make direct probabilistic statements about the parameter values themselves. See several examples of both, and understood how the numerical results are arrived at Learn to be statisticallly/politically correct in interpreting these results Appreciate that in practice, the purely numerical differences between the results of the two approaches are often small. Google gives the following definition for statistical inference The theory, methods, and practice of forming judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling. The Oxford English Dictionary defines it as The drawing of inferences about a population based on data taken from a sample of that population; an inference drawn in this way; the branch of statistics concerned with this procedure. Gelman and Hills start their regresssion textbook this way: The goal of statistical inference for the sorts of parametric models that we use is to estimate underlying parameters and summarize our uncertainty in these estimates. We discuss inference more formally in Chapter 18; here it is enough to say that we typically understand a fitted model by plugging in estimates of its parameters, and then we consider the uncertainty in the parameter estimates when assessing how much we actually have learned from a given dataset. Gelman and Hills 2007. We would add to these numerical statements about unknown (and unknowable) constants, as well as the mechanism or process that generated the limited data you got/get to observe. David Cox begins his book Principles of Statistical Inference by taking the family of models as given and aims to: give intervals or in general sets of values within which the parameter (or parameter set) is in some sense likely to lie; assess the consistency of the data with a particular parameter value predict as yet unobserved random variables from the same random system that generated the data; use the data to choose one of a given set of decisions, requiring the specification of the consequences of various decisions. He refers to the first two of these as interval estimation and measuring consistency with specified values of the parameter(s). For orientation, before we get to parameters involved in multi-parameter regression models, here are some simple examples of parameters we might like to quantify probabilistically – some scientific, some more personal or particularistic – Whether a potential hemophilia carrier is in fact a carrier a particular email is malicious a person committed the crime they are accused of a person has been infected with a certain virus The proportion of thumbtacks that land on their back when tossed your time that you are being productive the earth’s surface that is covered by water your driving time that you are on the phone your time (over the entire year) that you spend inside patients whose disease would respond to a medication people who would volunteer for a demanding survey or long-term research study The numerical value for the density of the Earth,relative to water the age of a person whom you have just met your cholesterol level the mean depth of the ocean the 20th percentile of the depths of the ocean the median age of a population To address the uncertainties involved in the judgements/inferences, some use of probabilities is required. When considering the first objective, namely providing intervals or sets of values likely in some sense to contain the parameter of interest, Cox tells us There are two broad approaches, called frequentist and Bayesian, respectively, both with variants. Alternatively the former approach may be said to be based on sampling theory and an older term for the latter is that it uses inverse probability. In their preamble to their chapter on inferemce, Clayton and Hills tell us that There are two radically different approaches to associating a probability with a range of parameter values, reflecting a deep philosophical division amongst mathematicians and scientists about the nature of probability. We shall start with the more orthodox view within biomedical science. Clayton and Hills completed their book in 1993. Since then, propelled by greater computer power, and by people like Clayton’s Cambridge colleague David Spiegelhalter, whose book we will start with, the Bayesian approach to ’associating a probability with a range of parameter values‘has become more common. It has not yet reached the status of ’customary or conventional, as a means or method; established.’ that the dictionaries give as the meaning of orthodox. In any case, we should not take Clayton and Hills’ use of the phrase ‘more orthodox’ to describe the frequentist approach to mean that the Bayesian approach ‘does not conform to the approved form of analysis’ or is in some sense ‘wrong.’ The first-established of the two ‘schools’ (or ‘churches’) of statistical inference** makes direct probabilistic statements about the possible parameter values. This approach goes back at least as far as the mid-1700’s essay ‘A method of calculating the exact probability of all conclusions based on induction’; ironically the author was a Presbyterian minister. The developments since then are nicely told in the very readable book The Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy by Sharon Bertsch McGrayne, and in her Microsoft lecture and her Google lecture. Maybe, by calling it the more ‘orthodox’, all that Clayton and Hills mean is that the frequentist approach is more popular method today. It got a slow start, and dates from the early 20th century. (In one of our sampling exercises, we will try to determine the relative frequencies of the two approaches in the epidemiology and medical literature). Interestingly, if you use Google Books Ngram Viewer you get a different sense. Maybe this is because the majority don’t need to justify the methods they use! Frequentist statements are indirect. They are (conditional) probabilistic statements about the data and about the performance of the procedure used to bracket the parameter values. A variant on it ranks the various possible parameter values according to how probable the observed data would be under each of these, but does not make direct probabilistic statements about the parameter values themselves. Because it is indirect, conditional, the results are often interpreted incorrectly. We begin with the direct method, one that studies tell us we are born with, and use throughout our lives, both consciously and subconsciously, to continue to learn/update. When we learn a new motor skill, such as playing an approaching tennis ball, both our sensors and the task possess variability. […] We show that subjects internally represent both the statistical distribution of the task and their sensory uncertainty, combining them in a manner consistent with a performance-optimizing bayesian process. The central nervous system therefore employs probabilistic models during sensorimotor learning. Bayesian integration in sensorimotor learning leading to this New York Times headline Subconsciously, Athletes May Play Like Statisticians 3.1 The Bayesian Approach to probability statements concerning parameter values. Bayesian inference refers to statistical procedures that model unknown parameters […] as random variables. […] Bayesian inference starts with a prior distribution on the unknown parameters and updates this with the likelihood of the data, yielding a posterior distribution which is used for inferences and predictions. Gelman and Hills, 2007, page 143. This paragraph, taken from this chapter An Overview of the Bayesian Approach in the book Bayesian Approaches to Clinical Trials and Health-Care Evaluation by David Speigelhalter et al, describes it well: The standard interpretation of probability describes long-run properties of repeated random events (Section 2.1.1). This is known as the frequency interpretation of probability, and standard statistical methods are sometimes referred to as ‘frequentist’. In contrast, the Bayesian approach rests on an essentially ‘subjective’ interpretation of probability, which is allowed to express generic uncertainty or ‘degree of belief’ about any unknown but potentially observable quantity, whether or not it is one of a number of repeatable experiments. For example, it is quite reasonable from a subjective perspective to think of a probability of the event ‘Earth will be openly visited by aliens in the next ten years’, whereas it may be difficult to interpret this potential event as part of a ‘long-run’ series. Methods of assessing subjective probabilities and probability distributions will be discussed in Section 5.2. Section 3.1 SUBJECTIVITY AND CONTEXT emphasizes that ‘the vital point of the subjective interpretation is that Your probability for an event is a property of Your relationship to that event, and not an objective property of the event itself.’ Moreover, ‘pedantically speaking, one should always refer to probabilities for events rather than probabilities of events, and the conditioning context used in Section 2.1.1 includes the observer and all their background knowledge and assumptions.’ That there is ‘always a context’ goes along with what we read in Alan Turing’s recently de-classified essay The Applications of Probability to Cryptography. Under section 1.2 (‘Meaning of probability and odds’) he starts out I shall not attempt to give a systematic account of the theory of probability, but it may be worth while to define shortly probability and odds. The probability of an event on certain evidence is the proportion of cases in which that event may be expected to happen given that evidence. For instance if it is known the 20% of men live to the age of 70, then knowing of Hitler only Hitler is a man we can say that the probability of Hitler living to the age of 70 is 0.2. Suppose that we know that Hitler is now of age 52 the probability will be quite different, say 0.5, because 50% of men of 52 live to 70. Not all context is subjective. We will start with a context where the initial (starting out, pre-new-data) probability is objective. Just before we do, we include this passage from Clayton and Hills, in subchapter 10.2 Subjective probablity, which they denote as optional material. The second approach to the problem of assigning a probability to a range of values for a parameter is based on the philosophical position that probability is a subjective measure of ignorance. The investigator uses probability as a measure of subjective degree of belief in the different values which the parameter might take. With this view it is perfectly logical to say that there is a probability of 0.9 that the parameter lies within a stated range. Before observing the data, the investigator will have certain beliefs about the parameter value and these can be measured by a priori probabilities. Because they are subjective every scientist would be permitted to give different probabilities to different parameter values. However, the idea of scientific objectivity is not completely rejected. In this approach objectivity lies in the rule used to modify the a priori probabilities in the light of the data from the study. This is Bayes’ rule and statisticians who take this philosophical position call themselves Bayesians. Bayes’ rule was described in Chapter 2, where it was used to calcu- late the probabilities of exposure given outcome from the probabilities of outcome given exposure. Once we are prepared to assign probabilities to parameter values, Bayes’ rule can be used to calculate the probability of each value of a parameter (\\(\\theta\\)) given the data, from the probability of the data given the value of the parameter. The argument is illustrated by two tree diagrams. Fig. 10.2 illustrates the direction in which probabilities are specified in the statistical model — given the choice of the value of the parameter, \\(\\theta\\), the model tells us the probability of the data. The probability of any particular combination of data and parameter value is then the product of the probability of the parameter value and the probability of data given the parameter value. In this product, the first term, Pr(\\(\\theta\\)), represents the a priori degree of belief for the value of \\(\\theta\\) and the second term, Pr(Data | \\(\\theta\\) ), is the likelihood. Fig. 10.3 reverses the conditioning argument, and expresses the joint probabilityas the product of the overall probability of the data multiplied by the probability of the parameter given the data. This latter term, Pr( \\(\\theta\\) | Data), represents the posterior degree of belief in the parameter value once the data have been observed. Since the joint probability of data and parameter value is the same no matter which way we argue, so that \\[Pr(\\theta) \\times Pr(Data | \\theta) = Pr(Data) \\times Pr(\\theta | Data),\\] so that \\[Pr(\\theta | Data) = \\frac{Pr(\\theta) \\times Pr(Data | \\theta)}{Pr(Data)}\\] Thus elementary probability theory tells us how prior beliefs about the value of a parameter should be modified after the observation of data. Their 2 figures nicely show that the difference is in the directionality or conditioning, i.e. is the object \\[Pr(\\theta | data) \\ \\ \\ OR \\ \\ \\ Pr(data | \\theta) \\ \\ ?\\] Figure 3.1: From Chapter 10.2 of Clayton and Hills Without getting into the details of the calculations, we will apply the Bayesian approach to the first example in each of the parameter genres listed above. The point is to illustrate how direct and unambigous the answer is in each case. To avoid being distracted by the details, you might wish to skip the detailed mathematical calculations involved, as well as the asides/tangents. 3.1.1 Example: parameter is 2-valued: yes or no In the first genre, the parameter is personal or particular. In each of the examples, the true state is binary. The potential hemophilia carrier is a hemophia carrier or is not; the particular email is either malicious or is not; the person in question either committed the crime ot did not. So, there are just two possible parameter values: yes or no, is or is not. From the outset, just like in Turing’s example, there is a given context. For example, suppose a woman’s brother is known to have haemophilia. hemophilia: a medical condition in which the ability of the blood to clot is severely reduced, causing the sufferer to bleed severely from even a slight injury. The condition is typically caused by a hereditary lack of a coagulation factor, caused by a mutation in one of the genes located on the X chromosome. - Google Just knowing this, the probability that the woman is a hemophilia carrier is 50% or 1/2. Today, genetic testing of the carrier can help determine whether the woman is a carrier. But when JH first taught 607, the only time to learn more about her carrier status (and move her probability to 1, or towards 0) was after the births of her sons: their status was knowable virtually immedediately. If it is determined that the first son has hemophilia, it establishes that she IS a carrier, thereby moving the probability up to 1. If he does not, it moves the probability down to 1/3: in other words, among ‘women like her’, i.e, potential carriers who also have had 1 son who turned out to be Normal (NL), 1/3 of the sons are the sons of carriers, and 2/3 are the sons of non-carriers. The continued updating as the women with a NL son gave birth to a second son, and so on, is shown in the diagram below, with C used as shorthand for ‘Is a Carrier.’ Technically speaking, each sequential P[C] should indicate that it is ‘conditioned on’ – and thus reflects the information in – the history up to that point. In other words, the 1/5 probability refers to P[C | both sons are NL], where “|” stands for ‘given that’, or – to use Turing’s phrase – ‘on the evidence that’. Figure 3.2: At the outset, each woman had a 50:50 chance of being a haemophilia carrier. Accumulating information from the hemophilia status of the sons increasingly ‘sorts’ or segregates the women by moving their probabilities of being a carrier TO 1 (100%) or FURTHER TOWARDS 0 (0%). It ‘updates’ the probablity of being a carrier, P[C]. For brevity, the ‘| data’ in each P[C | data] is omitted. Before moving on the the next type of parameter, a few points In the hemophilia (and if interested, the transgenic mice) example, the ‘starting’ probability is objective and the post-data probabilities have a ‘long-run’ or ‘in large numbers of similar instances’ interpretation. One could make a diagram that shows the expected numbers ‘in every 100 women like this.’ There is nothing special about the ‘starting out’ probability P[C] of 0.5. Before a pregnancy test, or a pre-natal diagnostic test, for example, the probability of the target Condition/state of interest/concern would be a function of many other factors, and could in theory take on any value between 0 and 1. The (starting out, pre-filter) proportion of malicious emails would depend on which of a person’s email accounts it was. In the language of diagnostic tests, each ‘Son as a test of the mother’s carrier status’ has 50% sensitivity and 100% ‘specificity’. For sensitivity, this puts it on par with the Pap test for cervical cancer: the main problem withe latter is in the sampling. For specificity, it is better than most tests. The shiny app From Pre-test to Post-test Probabilities shows how the initial average (pre-test) probability is segregated into 2 post-test probabilities by the 2 possible test results, and the role of the 2 error-probabilities (just about all tests are fallible) in how well they push them out. The ‘starting out’ probability could be subjective. For example it could be one’s impression (before getting to see up close how wrinkled their face is) as whether the person is a smoker, or one’s assessment of the probability that the accused is guilty (before getting to hear the DNA expert, or lie detection report) based on how credible the accused appears to be, and all of the other evidence to date. The posterior distribution for the parameter merges (adds) two sources of information. But the focus is less on where we start, and more on how much we have learned from the new data. Bayes’ rule: Guide (at several levels) ref: https://arbital.com/p/bayes_rule/?l=1zq 3.1.2 Example: parameter is a proportion In theory, in this genre, the true parameter value could in theory lie anywhere between 0 and 1, But again, just like in Turing’s example, we seldom start from complete ignorance, or with – in the title of pscychologist Stephen Pinker’s book – a blank slate. Even if you have never seen thumbtacks tossed onto on a surface, you can reason informally, and indicate what proportions are unlikely and likely, and where along the (0,1) scale you would ‘put your money’. You could do the same when asked what proportions of your time that you are being productive, or on the phone, or sedentary, or indoors. Mind you, you might be ‘way off’ with your claims, but the nice thing is that — and this is the point of this course – you can generate data to narrow down the true proportion. The other nice thing with the Bayesian approach in particular is that – no matter whether you believe the proportion is low or medium or high, we can work out what your post-data beliefs should be. It is a matter of mathematics. If, before collecting any new data, we have ‘no idea’ – a common phrase among todays’s generation, one that, if it is uttered with empahsis on the ‘no’, irks JH to no end – what the true parameter value is, that is easily handled. Moreover, enough valid data will (or should!) trump the pre-data beliefs. On a side note: Dick Pound, a former chancellor of McGill University, and first president of the World Anti-Doping Agency is a staunch advocate of strict drug testing for athletes. Discussing the National Hockey League in November 2005, Pound said, ‘you wouldn’t be far wrong if you said a third of hockey players are gaining some pharmaceutical assistance.’ Pound would later admit that he completely invented the figure. Both the NHL and NHLPA have denied the claims, demanding Pound provide evidence rather than make what they term unsubstantiated claims. Since his comments were made, some NHL players have tested positive for banned substances, including Bryan Berard, José Théodore, and two of 250 players involved in Olympic testing. As of June 2006, there had been 1,406 tests in the program jointly administered by the league and the union, and none has come up with banned substances under NHL rules. Pound remained skeptical, claiming the NHL rules were too lax and unclear, as they do not test for some banned substance, including certain stimulants. In an interview with hockey blogger, B. D. Gallof, of Hockeybuzz on December 19, 2007, Pound was asked to expand on the 30% comment and subsequent reaction, expounded that stimulants was ‘the NHL’s drug of choice’. He also cited that the NHL will have no credibility on a drug policy if it, and other sports, continue to run things ‘in-house’. https://en.wikipedia.org/wiki/Dick_Pound and https://www.cbc.ca/sports/hockey/dick-pound-slams-nhl-s-drug-policy-1.557993 Even before studying/asking them, investigators would have some sense of the proportions of patients whose disease would respond to a medication, or people who would volunteer for a survey or research study. These iimpressions would probably be based on previous analogous situations, and the ‘literature’, but would vary from pundit to pundit. But ultimately, they could be much improved and narrowed (and even replaced entirely) by new-data-based ones. The proportion of the earth’s surface that is covered by water is easy to determine: just look up a reputable source. But what if you weren’t able to, but did have access to the database of 933 million recordings in the SRTM30PLUS database. It has altitude/depth measurements for 43,200 x 21,600 = 933,120,000 locations. This database is so large that you would have to sample from it. From a thousand randomly chosen loactions, you would be able to ‘trust’ the first decimal in your estimate; from a million you should be able to trust the second – and maybe the third. Since we already know/remember from high school ‘roughly’ what the proportion is, we will leave it for an exercsie in another chapter. In this chapter, following the advice of master-teacher Fred Mosteller, we use examples where the correct answer is not known with any precision. The proportion of these we probably know the least about is the thumbtack one. However, it has fewer personal benefits than knowing what proportion of your time you are being productive. Moreover, we have a nice written account of how you might go about learning this personal proportion. Worked example (productivity) In his book Elementary Bayesian Statistics, Gordon Antelman informally introduces and illustrate a Bayesian analysis of an uncertain proportion with a slightly modified version of a novel and useful application of work sampling discussed by Fuller (1985). We have changed his notation for the proportion of your time spent in productive work, and called it \\(\\pi\\), and also modified some of his words. Suppose you, as a good up-to-date manager practicing continuous quality and productivity improvement, have some ideas on improving your own productivity. To see if these ideas have any merit, you would like to compare some ‘before’ measure of productivity with a comparable ‘after’ measure of productivity. For now — we shall come back to this example several times — let us focus on just a ‘before’ measure. The measure to be used is the proportion of your time spent in productive work, call it \\(\\pi\\), as opposed to time spent doing something that would not have needed doing if things had been done right the first time. Examples of the latter might include searching for a misplaced document, recreating a deleted computer file, following up on a customer’s complaint, or waiting past a scheduled time for a meeting to start. [Today, we would add being on social media, or browsing the web for non-work-related matters] Rather than saying \\(\\pi\\) is not (precisely) ‘known’, it is better to say that ‘\\(\\pi\\) is uncertain’; from your job experience, you would really know quite a lot about p. For example, you might be almost certain that it is greater than 0.50, less than 0.90, and you might assess your odds that \\(\\pi\\) is between, say, 0.60 and 0.80 to be about 9 to 1. A precise statement of these beliefs will be your prior distribution for \\(\\pi\\). You would probably feel uncomfortable — most people do — about assessing this prior distribution, especially since there are an infinite number of states; viz., all of the values between zero and one. But, without any real loss, you can bypass the infinite-number problem by rounding the values of \\(\\pi\\) to the nearest 5% or 10%, making the problem discrete. Then you have a contemplatable Bayes’ theorem, like those discussed in Chapter 4, with the finitely many \\(\\pi\\)-values as the possible “states”. (When we reconsider this example later in this chapter, you will see that, with a little theory, the infinite number of \\(\\pi\\)-values can almost always be handled very neatly and more easily.) PRIOR BELIEFS For illustration, he supposes you choose just five possible values for \\(\\pi\\), and assess your prior distribution. Since R does not allow Greek symbols, we will refer to it by uppercase P and assess your prior distribution. This possible prior distribution, shown below, would reflect, for example, that your judgment is that there is only about one chance in 20 that \\(\\pi\\) rounds to 0.50, about one chance in 20 that \\(\\pi\\) rounds to 0.90, about one chance in four that it rounds to 0.60, a little more than one chance in three that it rounds to 0.70, and a little less than one chance in three that it rounds to 0.80. Figure 3.3: Prior Probabilities for the parameter P, the proportion of time that I am being productive. DATA: Suppose you are fitted with a beeper set to beep at random times; when the beeper beeps, you classify the task being worked on as W — for ‘productive Work’, or F — for ‘Fixing’ (or today we mght say ‘Fiddling’ or ‘Fooling around’ or wasting time). Although we will skip the technicalities, it is important that the experiment be designed so that the trials are independent. Beeps should be unpredictable so you do not arrange, possibly subconsciously, to be doing productive work at the beep. They probably also should not be too close together to make the independence assumption more reasonable. Suppose the first four trials give the data \\(F_1, F_2, W_3\\), and \\(F_4\\). Below is a picture showing the effects of the data FFWF on the prior distribution. Three F’s in four trials increase your probabilities for the two smaller values of P and decrease your probabilities for the three larger ones. Figure 3.4: Prior Probabilities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior probabilities, after observing that in n = 4 randomly sampled occasions, I was actually productive in only 1 of the 4. The sample alone most strongly supports a value for P of 0.25 (one W in four trials); had the prior included a value of P of 0.25, the (relative) increase in going from prior to posterior would have been greatest for that value. For the assumed prior, in which only p’s of 0.50, 0.60, 0.70, 0.80, or 0.90 are considered, the sample evidence FFWF in favor of a P near 0.25 can only push up the posterior probabilities for the nearest possible values - 0.50 and 0.60. (The seemingly harder consideration of all possible p’s between zero and one will handle this kind of situation more logically.) Below we show the ‘continuous P ’ version Antelman refers to. To make this, we calculated the mean and variance of his discrete (5-point) prior distribution, and converted them to the 2 parameters, \\(a\\) and \\(b\\), of the beta distribution with the same mean and variance. Conveniently, the posterior density is also a beta distribution, but with parameters \\(a+1\\) and \\(b+3\\). Figure 3.5: Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 4 randomly sampled occasions, I was actually productive (W) in only 1 of the 4. Before moving on the the next type of parameter, a few points: The beta distribution nicely shows how the prior information/impression and the new data get combined. The \\(a\\) and \\(b\\) parameters of the prior distribution are 14.8 and 6.2. Together, they determine the mean, \\(a/(a+b)\\), the median, the mode, \\((a-1)/(a+b-2)\\), and the variance, \\(mean(1-mean)/(a+b+1)\\) of the prior distribution. Their conterparts in the data-likelihood are 1 and 3. The ‘\\(a\\)’ and ‘\\(b\\)’ parameters of the posterior distribution are 15.8 and 9.2: the \\(a\\)’s add, and the \\(b\\)’s add. In other words, the distribution of one’s pre-data beliefs is the distribution one would have after ‘seeing’ 14.8 W’s and 6.2 F’s; the distribution of one’s post-data beliefs is the distribution one would have after ‘seeing’ 15.8 W’s and 9.2 F’s. The (synthetic) experience-equivalent of the numbers of Ws and F’s in the prior are added to the actual (observed) numbers of Ws and F’s in the data to arrive at the (new) posterior distribution. You are probably wondering what the posterior distribution would look like with more data. Here is what it would look like after observing 7 out of 25 or 13 out of 25. The modes of the posterior distributions are still somewhat influenced by the prior – as they are still well above P=7/25 = 0.28 and P=13/25 = 0.52. If the data were 70/250 or 13/250, the modes would be closer to P= 0.28 or P = 0.52; in other words, the data would ‘swamp’ – or ‘trump’ – the prior. Figure 3.6: Prior probability densities for the parameter P, the proportion of time that I am being productive, together with the corresponding posterior densities, after observing that in n = 25 randomly sampled occasions, I was actually productive (W) in only 7 of the 25, or 13 of the 25. Be thinking about your prior for the proportion of thumbtacks that land on their back, and the proportion of the Earth’s surface that is covered by water, or [these words written on March 30, 2020, before any trial data] the proportion of patients with mild symptoms of covid-19 who would benefit from chloroquine. Think about how you might elicit a prior distribution. You might want to Google ‘tools for eliciting prior distributions’ – or consult Chapter 5 of Spiegelhalter’s book. We now move on to last parameter genre we will consider. 3.1.3 Examples: parameter is a personal number or population mean We will start with a discrete version of a commonly-wondered-about parameter, the age of a person whom you have just met, or seen a photo of. We will then go on to full numerical examples: your average cholesterol or blood pressure level, a baseball player’s batting average Example 1 How old (or what age – if you prefer to avoid speaking of ‘old’) do you think this IBC2006 attendee was when this photo was taken? This is the parameter of interest. Figure 3.7: An attendee at the International Biometrics Conference, held at McGill in July 2006 He tells you he got his PhD 32 years earlier. Based on the distribution of ages at which people get their PhD (shown in grey below), that puts his current age somewhere in the blue distribution. Figure 3.8: Current ages of persons who obtained PhD 32 years earlier. This is a somewhat unusual example, as the blue distribution is very wide – partly because we could not find age-at-graduation data specific to PhD graduates in statistics in 1974. We suspect that that specific distribution is a good deal narrower than the one shown. In the next chapter, you will see that other indirect measures of age are a good bit tighter than this. Nevertheless, it emphasizes that, depending on what impression you got form the photo alone, you may now wish to revise your estimate of the person’s age. We suspect that many of you would have initially though he looks like he was in mid 50s, and so would have made guesses like those shown in red in the top panel. If you are one of those, then you will want to revise the age upwards, as we do in the green in the bottom panel. Figure 3.9: If he looked like he was in mid 50s If you initially thought he looked like he was ‘around 60’ you would have made guesses like those shown in red in the top panel. If you did, then you will want to revise upwards a little, as is shown in green, and now put hime him somewhere around 60, or a bit above. Figure 3.10: If If he looked like he was around 60. If you initially thought he looked like he was ‘in his mid 60s’ your estimate is more in line with the age-at-PhD data, and so you would not revise as much. You might bet a bit more on the ‘around 60’ age-bracket. Figure 3.11: If If he looked like he was in his mid 60s. As we noted, the PhD data have too much of a right tail, and so it is driving up the estimates. If you are now curious as how keen your ‘age-estimation’ skills are, here is a link to the Google Scholar page of the statistician whose age we have been trying to determine. Age estimation via face images (image-based age estimation) is a growing research area, with many possible applications. We now describe 2 more classical examples Example 2 Spiegelhalter et al. address this in their Example 3.4: ‘Suppose we are interested in the long-term systolic blood pressure (SBP) in mmHg of a particular 60-year-old female.’ We take two independent readings 6 weeks apart, and their mean is 130. We know that SBP is measured with a standard deviation \\(\\sigma = 5.\\) What should we estimate her SBP to be? They then go on to give the frequentist (‘standard’, ‘orthodox’) 95% confidence interval, of 123.1 to 136.9, centered on the measured value of 130 [we will come later to how they calculated this]. They continue, … However, we may have considerable additional information about SBPs which we can express as a prior distribution. Suppose that a survey in the same population revealed that females aged 60 had a mean long-term SBP of 120 with standard deviation 10. This population distribution can be considered as a prior distribution for the specific individual. The posterior distribution, computed from the combination of the 130 measured on the woman, and the prior, is centered on 128.9 and the 95% interval is 122.4 to 135.4. This posterior distribution reveals some ‘shrinkage’ towards the population mean, and a small increase in precision from not using the data alone. Intuitively, we can say that the woman has somewhat higher measurements than we would expect for someone her age, and hence we slightly adjust our estimate to allow for the possibility that her two measureshappened by chance to be on the high side. As additional measures are made, this possibility becomes less plausible and the prior knowledge will be systematically downgraded. Before going on to example 3, we emphasize that the 128.9 is a compromize between the personal mean of 130, and the (prior) poulation mean of 120: it is a weighted average, with (relative) weights that are the reciprocals of the squares of the 2 standard deviations, the reciprocals of \\(\\frac{5^2}{2}\\) and \\(10^2,\\) i.e., \\(\\frac{2}{25} = 0.08\\) and \\(\\frac{1}{10^2} = 0.01.\\) The 128.9 is 8/9ths closer to the measured 130 than it is to the population mean of 120. It is slightly ‘shrunk’ toawards the population. This is why physicians might not believe that the 130 is correct, and might ask for more measurements before putting this woman on blood pressure-lowering drugs. [The already cited ‘Bayesian integration in sensorimotor learning’ illustrates how as humans, we automatically combine estimates of different precisions into one more precise estimate, and do so using the same mathematical laws that are used in the Bayesian approach!] Example 3 One article that does go into some detail about a similar situation is the very nice medically-useful – and didactic article Estimating an Individual’s True Cholesterol Level and Response to Intervention by Les Irwig, Paul Glasziou, Andrew Wilson and Petra Macaskill. It begins with a single measurement, before dealing with an average of several measurements on the same person. It also gives separate charts for persons of different ages, and deals not just with point and interval estimates, but also derives probability statements for the possibility that the person’s true cholesterol is above some threshold that should trigger intervention. The appendix is a nice tutorial for combining information. Their abstract begins: An individual’s blood cholesterol measurement may differ from the true level because of short-term biological and technical measurement variability. Using data on the within-individual and population variance of serum cholesterol, we addressed the following clinical concerns: Given a cholesterol measurement, what is the individual’s likely true level? The confidence interval for the true level is wide and asymmetrical around extreme measurements because of regres-sion to the mean. Of particular concern is the misclassification of people with a screening measurement below 5.2 mmol/L who may be advised that their cholesterol level is ‘desirable’ when their true level warrants further action. The first half of the paper, which deals with two related topics, (a) Estimating the True Cholesterol Level, and (b) assesing the Probability of Misclassification shows the primary elements, and these notes will focus on the highlights. [after these, extensive excerpts will be included] The results for (a) and (b) were presented as 2 Figures. The first gave the (posterior) credible interval for a person’s true cholesterol level based on either 1, or an average of 3, measurements, using on the horizontal axis the measured value, and on the vertical one the point and 95% credible interval. Using a graph (rather than a formula) allows the clinician to use it for all possible ‘what if’s. Below, we will illustrate it using one specific example, a person whose measured value was 7.15. The second uses the (posterior) credible interval to calculate the probability that someone with a specific measured value has a true level that is above a certain threshold level used in treatment guidelines. Thus, the key tool is the posterior distribution itself, and so we give the statistical basis for this. Reasons to take a Bayesian approach The reason this problem arises in the first place is because of short term biological variability in the quantity of interest in the person in question. If we were measuring a person’s height, we could do so carefully at just one time-point: it would not be different a week or month from now; it remains quite stable over several years. [it does vary slightly over the day, but, be keep it simple, we could speak of one’s height at mid-day]. The same is not the case for a person’s cholesterol level: even if we measured it very carefully at one time, it would be genuinely different a week or month later, even in the absence of any intervention of lifestyle change. (The same applies, more strikingly, for other blood levels such as C-reactive protein (CRP), which is a marker of inflammation). Thus any single measurement, or any average of a finite number of determinations, is imprecise. So what’s new? Don’t we meet this issue all the time in statistics? The point of Irwig’s article is that we should not rely solely on the estimate based on the person’s measurements, but rather should combine it with an estimate based on outside information. The same reasoning is at work when a physician repeats a measurement that seems extreme. In so doing, (s)he is not relying only on the point or interval estimate provided by the measurement itself: rather (s)he is also using knowledge of how this measurement behaves in other similar persons! And what we know about others, even if collectively, can help us with an individual. All of the technical details are available in these notes, prepared for bios601 Here we will skip to the ‘botom line’ diagrams. Figure 3.12: A Person’s estimated true cholesterol level from one measurement for a population of individuals where the mean is 5.2 mmol/L (i.e, at the time of the article, men less than 35 and women less than 45 years old). The thicker solid line indicates the estimated true level, the thinner solid lines are 80% confidence intervals. The dotted diagonal line is an equivalence line. The arrow shows the amount by which a single measurement of 7.15 mmol/L is shrunk towards the population mean, to a rcorrected value of 6.8 mmol/L. Below, we show the detailed calculations for a person with a single measurement of 7.15 mmol/L. For now, just focus on the items shown in red (the prior distribution for \\(\\theta\\), blue (the 7.15 for the individual, and the associated likelihood function) and green (the posterior distribution of \\(\\theta\\)). All the calculations are on the log scale. Figure 3.13: Worked example, for a person with a measured value of 7.15 combine estimates of different precisions into one more precise estimate, and do so using the same mathematical laws that are used in the Bayesian approach!] Example 4 We end with a strking example of the limitations of using one-at-a-time frequentist intervals, each in isolation from the next. It is based on the example in this classic article. The task is to predict each player’s batting average over the remainder of the season using only the data from the first 4 weeks of the season. Since it proved difficult to replicate their selection croteria, we used instead the first 40 of these players. The numbers of at bats per player up to mid April ranged from 44 to 73 (median 60 ). Efron and Morris used a constant 45. The numbers from mid April to the end of September ranged from 511 to 611 (median 542 ), or about 9 times as many as in the initial sample. Clearly, the extreme results in these first 4 weeks were poor estimates of the perfornmances during the rest of the season. They ‘shrunk’ quite a bit. Figure 3.14: Batting averages of selected Major League baseball players, 2019 Across the performances up to mid April, the variance is a lot more than the variances in their ‘real’ (longer-term, remaining) performances. Thus, only about portion of it reflects the variance of the player-specific true (long-term) success rates. Will will quantify these later, below. The individual binomial-based point estimates and frequentist confidence intervals we could construct (see next section) do not use the knowledge that individual season-long averages are confined to a narror band between say 0.225 and 0.325. Thus, many of them will not come close to or ‘trap’ the eventual individual longer-term batting averages. The correct way to make better frequentist intervals is to consider the family of players as an ensemble. If we label an randomly selected player as ‘rs’ say, then the long-run average \\(\\pi_{rs}\\) is thought of (modelled) as drawn from the distribution (family) of long-run averages. A slightly simplified (for display and didactic purposes) version of this is shown in this figure. Figure 3.15: Simplified Distribution, for learning purposes, of Long-term Batting averages of Major League baseball players. Mean = 0.290; SD = 0.040. How then should we think of (model) rs’s short-term average? To make it concrete, suppose the number of at bats was \\(n_{rs}\\) = 60, and that we observed 21/60 hits. A natural way is to view the 21/60 as the end-result of (of having come to us in) two stages, each involving a random draw (i) \\(\\pi_{rs}\\) was randomly drawn from the above distribution and then (ii) the number of hits was drawn from a binomial(\\(n\\)=60) distribution with the probability/proportion parameter \\(\\pi_{rs}\\). Symbolically, we would describe the provenance of the \\(y\\) (= 21 for example) as \\[(i) \\ \\pi_{rs} \\sim distribution \\ in \\ previous \\ figure ; \\ \\ (ii) \\ \\ y | \\pi_{rs} \\sim Binomial(n=60, \\pi = \\pi_{rs}).\\] provenance : The fact of coming from some particular source or quarter; origin, derivation. The history of the ownership of a work of art or an antique, used as a guide to authenticity or quality; a documented record of this. A distinction is sometimes drawn between the ‘origin’ and the ‘provenance’ of an article, as in ‘A Canterbury origin is probable, Canterbury provenance is certain.’ Forestry. The geographic source of tree seed; the place of origin of a tree. Also: seed from a specific location. Oxford English Dictionary The bottom part of this next figure shows the frequencies of the possible end-results (possible observed short term averages). The colours show their ‘provenance’. The top part shows the provenance of the ‘21/60’ short-term averages in particular. Note the direction of the arrow, showing the reverse-engineering, from what we observe (the 21/60) back to where (who) it could have arisen from. The use of reverse probabilities is at the heart of the Bayesian approach. In his worked example, when Bayes wrote about ‘the exact probability of all conclusions based on induction’, his conclusions referred to the origin (original location, unknown) of a billiard ball, and his data referred to the locations where it came to rest in a number of trials. Figure 3.16: The bottom frequency distribution shows the expected frequencies of Short-term Batting averages from n = 60 at bats by types of players shown in previous figure. For each possible short-term average, the colours of their components show their ‘provenance’, i.e., which types of player this average arises from, and the relative contributions from each type. As an example, highlighted, within the column enclosed by a dashed line, is the provenance of the short-term averages that equal 21/60 = 0.350. In a real application, after n=60, IF ALL ONE KNEW WAS THE 21/60, one would not know which type of player, i.e., which colour, one was observing. Nevertheless, the theoretical calculations show that a short-term average of 21/60 = 0.350 is more likely to have been produced by a player who bats 0.320 or 0.300 or even lower, that one who bats 0.350 or or 0.360 or higher. Much more of the probability distribution lies to the left of the 0.350 than to the right of it! So we need to ‘shrink’ the observed 0.350 towards the 0.290. Efron and Morris give a specific formula for merging the initial information (the ‘centre’ of 0.290) with the new information (the 21/60 = 0.350). the ‘shrunken’ estimate of the long-term average is a compromise, a weighted average, with the weights determined by the precision of the 21/60 (determined mainly by the 60) and the narrowness of the ‘parent’ distribution. We don’t need to get into these at this stage. But, we will address a question some of you may already have concerning the batting average example, compared with the cholesterol example. In the cholesterol example, the ‘prior’ (or population) distribution of individuals’ true levels would be reasonably well known already. But in the baseball example, even though they had easy access to all of players’ previous years of data, Efron and Morris did not use any historical data. Instead, they use a ‘contemporaneous’ prior: they inferred what the location and spread of the distribution shown in the second of our three baseball figures, just from the March to Aril 15, 2019 numbers shown along the top of the first figure. They did this by subtraction. In the simplest case, if all the short-term averages were based on the same number (\\(n\\)) of at bats (Efron and Morris used \\(n\\) = 45), we have the following variance (V) formula \\[V[short.term.averages] = V[long.term.averages] + \\ V[binomial.proportion \\ based \\ on \\ n.]\\] They got to measure the left hand quantity empirically from their data on April 26 / May 3, 1970, and they knew from mathematical statistics that the rightmost quantity is of the order of \\(0.3 \\times 0.7 \\ / \\ 45\\) = 0.0047. Thus they could deduce the spread of the long-term averages. In our 2019 data, the numbers of early-season at bats vary from player to player, but we will simplify the math by taking a typical \\(n\\). Across the performances up to mid April, the (rounded) variance of the individual batting averages, (i.e., the left-hand \\(V\\) in the above equation) is 0.004640. The binomial variation that you would see with \\(n\\)’s of the order of 60 is approximately 0.003570. Player-specific true proportions vary from maybe 0.225 to 0.325. [for example, at \\(\\pi\\) =0.30 and \\(n\\) = 60, the binomial variance is 0.30 * 0.70 / 60 = 0.003500.] Thus, by solving the equation \\[0.004640 = V[long.term.averages] + \\ 0.003500\\] we can back-calculate that the variance of the player-specific true (long-term) success rates is only about 0.001070. [The square root of this, sd = 0.033, broadly fits with the spead we see in the performances during the remainder of the season: the averages range from 0.224 to 0.326.] To distinguish it from the typical Bayes approach, this double use of the data is sometime referred to as an ‘Empirical Bayes’ approach. Empirical Bayes methods are procedures for statistical inference in which the prior distribution is estimated from the data. This approach stands in contrast to standard Bayesian methods, for which the prior distribution is fixed before any data are observed. Despite this difference in perspective, empirical Bayes may be viewed as an approximation to a fully Bayesian treatment of a hierarchical model wherein the parameters at the highest level of the hierarchy are set to their most likely values, instead of being integrated out. Empirical Bayes, also known as maximum marginal likelihood, represents one approach for setting hyperparameters. Wikipedia, 2020.04.15 3.1.4 The Bayesian Bottom Line As the examples all show, the Bayesian approach allows for direct-speak about parameters, without any of the ‘legalese’ we will have to use when using the frequentist approach. Because the end-product is a probability distribution for the parameter (\\(\\theta\\) say), we can directly talk about the (pre-) and post-data probability that, say, \\(\\theta &gt; 0,\\) or (for any prespecified values \\(A\\) and \\(B\\)) \\(A &lt; \\theta &lt; B\\), or \\(\\theta &gt; A\\) or \\(\\theta &lt; B\\), or, as in the case of hemophilia carriage, \\(\\theta = Yes\\) or \\(\\theta = No.\\) If \\(\\theta\\) = batting average for the remainder of season, Prob(a 21/60 player will have \\(\\theta \\ge\\) .350) = 0.11 [see Fig] Likewise, if we fix a desired probability \\(P\\), we can calculate \\(\\theta\\) values \\(\\theta_{lower}\\) and \\(\\theta_{upper}\\) such that Prob( \\(\\theta &gt; \\theta_{lower}) = P\\), or Prob( \\(\\theta &lt; \\theta_{upper}) = P\\), or Prob( \\(\\theta_{lower} &lt; \\theta &lt; \\theta_{upper}) = P\\). Again, if \\(\\theta\\) = batting average for the remainder of season, then, in the case of a 21/60 player, Prob(0.245 &lt; \\(\\theta\\) &lt; 0.377) = 0.95. Contrast this with the 95% frequentist confidence interval, based only on the 21/60, and not on the knowledge that (historically, or even by back-calculation just from the early 2019 data) most players’ batting averages for a year are between 0.225 and 0.325, \\(\\theta_{lower}\\) = 0.231 ; \\(\\theta_{upper}\\) = 0.484) = 0.95. Can this shrinkage be done without ‘priors’? 3.2 Frequentist approach In contrast to the seamless and unified Bayesian approach, the Frequentist approach is more fragmented, and is typically taught in two parts: tests of null hypotheses concerning a parameter or process, and confidence intervals (CI’s). Whereas we – like others – wish to downplay tests and elevate CI’s, it is easier to start with tests. Don’t take this to mean they are more scientifically relevant – in most applications they are not. But it is difficult to learn the principle behind frequentist CIs without knowing what tests of null hypotheses are. In their chapter 11, Null hypotheses and p-values, Clayton and Hills tell us With most probability models there is one particular value of the parameter which corresponds to there being no effect. This value is called the null value, or null hypothesis. For a parameter \\(\\theta\\) we will denote this null value by \\(\\theta_0\\). In classical statistical theory, considerable emphasis is placed on the need to disprove (or reject) the null hypothesis before claiming positive findings, and the procedures which are used to this end are called statistical significance tests. However, the emphasis in this theory on accepting or rejecting null hypotheses has led to widespread misunderstanding and misreporting in the medical research literature. In epidemiology, which is not an experimental science, the usefulness of the idea has been particularly questioned. Undoubtedly the idea of statistical significance testing has been overused, at the expense of the more useful procedures for estimation of parameters which we have discussed in previous chapters. However, it remains useful. A null hypotheses is a simplifying hypothesis and measuring the extent to which the data are in conflict with it remains a valuable part of scientific reasoning. In recent years there has been a trend away from a making a straight choice between accepting or rejecting the null hypothesis. Instead, the degree of support for the null hypothesis is measured, for example using the log likelihood ratio at the null value of the parameter. Clayton and Hills, p96. [Their illustration involved an investigation of the possible linkage of the HLA locus to nasopharyngeal cancer susceptibility] In Chapter 2.4, Classical hypothesis testing, Gelman and Hill tell us The possible outcomes of a hypothesis test are ‘reject’ or ‘not reject.’ It is never possible to ‘accept’ a statistical hypothesis, only to find that the data are not sufficient to reject it. . Comparisons of parameters to fixed values and each other: interpreting confidence intervals as hypothesis tests . The hypothesis that a parameter equals zero (or any other fixed value) is directly tested by fitting the model that includes the parameter in question and examining its 95% interval. If the interval excludes zero (or the specified fixed value), then the hypothesis is rejected at the 5% level. Testing whether two parameters are equal is equivalent to testing whether their difference equals zero. We do this by including both parameters in the model and then examining the 95% interval for their difference. As with inference for a single parameter, the confidence interval is commonly of more interest than the hypothesis test. For example, if support for the death penalty has decreased by 6% \\(\\pm\\) 2.1%, then the magnitude of this estimated difference is probably as important as that the change is statistically significantly different from zero. The hypothesis of whether a parameter is positive is directly assessed via its confidence interval. If both ends of the 95% confidence interval exceed zero, then we are at least 95% sure (under the assumptions of the model) that the parameter is positive. Testing whether one parameter is greater than the other is equivalent to examining the confidence interval for their difference and testing for whether it is entirely positive. In section 2.5, Problems with statistical significance, they warn that A common statistical error is to summarize comparisons by statistical significance and to draw a sharp distinction between significant and nonsignificant results. The approach of summarizing by statistical significance has two pitfalls, one that is obvious and one that is less well known. First, statistical significance does not equal practical significance. For example, if the estimated predictive effect of height on earnings were $10 per inch with a standard error of $2, this would be statistically but not practically significant. Conversely, an estimate of $10,000 per inch with a standard error of $10,000 would not be statistically significant, but it has the possibility of being practically significant (and also the possibility of being zero; that is what “not statistically significant” means). . The second problem is that changes in statistical significance are not themselves significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary — for example, only a small change is required to move an estimate from a 5.1% significance level to 4.9%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying variables. They illustrate the application of basic statistical methods with a story, in which, a couple of years before, they received a fax, entitled HELP!, from a member of a residential organization: Last week we had an election for the Board of Directors. Many residents believe, as I do, that the election was rigged and what was supposed to be votes being cast by 5,553 of the 15,372 voting households is instead a fixed vote with fixed percentages being assigned to each and every candidate making it impossible to participate in an honest election. Gelman and Hill devote 3 pages to their analysis, which led them to conclude that the intermediate vote tallies are consistent with [the null hypothesis of] random voting. As we explained to the writer of the fax, opinion polls of 1000 people are typically accurate to within 2%, and so, if voters really are arriving at random, it makes sense that batches of 1000 votes are highly stable. This does not rule out the possibility of fraud, but it shows that this aspect of the voting is consistent with the null hypothesis. Cox acknowledges a large and ever-increasing literature on the use and misuse of significance tests. This centres on such points as: Often the null hypothesis is almost certainly false, inviting the question why is it worth testing it? Estimation of \\(\\theta\\) is usually more enlightening than testing hypotheses about \\(\\theta\\). Failure to ‘reject’ \\(H_0\\) does not mean that we necessarily consider \\(H_0\\) to be exactly or even nearly true. If tests show that data are consistent with \\(H_0\\) and inconsistent with the minimal departures from \\(H_0\\) considered as of subject-matter importance, then this may be taken as positive support for \\(H_0\\) , i.e., as more than mere consistency with \\(H_0\\). With large amounts of data small departures from \\(H_0\\) of no subject-matter importance may be highly significant. When there are several or many somewhat similar sets of data from different sources bearing on the same issue, separate significance tests for each data source on its own are usually best avoided. They address individual questions in isolation and this is often inappropriate. The p-value is not the probability that \\(H_0\\) is true. He continues Discussion of these points would take us too far afield. Point 7 addresses a clear misconception. The other points are largely concerned with how in applications such tests are most fruitfully applied and with the close connection between tests and interval estimation. The latter theme is emphasized below. The essential point is that significance tests in the first place address the question of whether the data are reasonably consistent with a null hypothesis in the respect tested. This is in many contexts an interesting but limited question. The much fuller specification needed to develop confidence limits by this route leads to much more informative summaries of what the data plus model assumptions imply. Using some classic examples as illustrations, we now begin now with our own home-grown explanations 3.2.1 (Frequentist) Test of a Null Hypothesis Use: To assess the evidence provided by sample data in favour of a pre-specified claim or ‘hypothesis’ concerning some parameter(s) or data-generating process. As do confidence intervals, tests of significance make use of the concept of a sampling distribution. Example 1 The Mathematics of a Lady Tasting Tea. Whether the sensory experiment was actually carried out has been the subject of subsequent articles. In 2002, the phrase ‘Lady Tasting Tea’:’ was used as the title, and How Statistics Revolutionized Science in the Twentieth Century the subtitle of this book which you can borrow from the McGill Librray. Here is a link to an online copy of Fisher’s 1935 book Design of Experiments Fisher continued It is usual and convenient for experimenters to take 5 per cent. as a standard level of significance, in the sense that they are prepared to ignore all results which fail to reach this standard, and, by this means, to eliminate from further discussion the greater part of the fluctuations which chance causes have introduced into their experimental results. No such selection can eliminate the whole of the possible effects of chance coincidence, and if we accept this convenient convention, and agree that an event which would occur by chance only once in 70 trials is decidedly “significant,” in the statistical sense, we thereby admit that no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon; for the one chance in a million&quot; will undoubtedly occur, with no less and no more than its appropriate frequency, however surprised we may be that it should occur to us. In order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result. In this example, the result was the most extreme that it could be. But, Fisher asked, ’what should be our conclusion if, for each kind of cup, her judgments are 3 right and 1 wrong? It is obvious, too, that 3 successes to 1 failure, although showing a bias, or deviation, in the right direction, could not be judged as statistically significant evidence of a real sensory discrimination. For its frequency of chance occurrence is 16 in 70, or more than 20 per cent. Moreover, it is not the best possible result, and in judging of its significance we must take account not only of its own frequency, but also of the frequency for any better result. In the present instance “3 right and 1 wrong” occurs 16 times, and “4 right” occurs once in 70 trials, making 17 cases out of 70 as good as or better than that observed. The reason for including cases better than that observed becomes obvious on considering what our conclusions would have been had the case of 3 right and 1 wrong only 1 chance, and the case of 4 right 16 chances of occurrence out of 70. The rare case of 3 right and 1 wrong could not he judged significant merely because it was rare, seeing that a higher degree of success would frequently have been scored by mere chance. JH would add: if the experiment involved 200 cups of each kind, and she only got 106 right? The probability of exactly this number is just 4% whereas the probability of at least this number is 14% Example 2 Preston-Jones vs. Preston-Jones (1946-1951), excerpted from here. The facts were as follows: The husband (the appellant) was married to the wife (the respondent) on April 14, 1941, at Brymbo in the county of Denbigh. Jean, a child of the marriage, was born on April 15, 1942. On August 13, 1946, a second child was born to the wife. The husband, alleging that this child was not his child and founding on the circumstances of his birth a charge of adultery against the wife, on November 8, 1946, presented a petition to the High Court for the dissolution of his marriage. He alleged that the said child was conceived some time between August 29, 1945, and February 13, 1946, during the whole of which period of time [he] was absent from the United Kingdom and . . . the [wife] was resident in the United Kingdom and that the child must therefore have been conceived as the result of an act of adultery. The petition was first heard by His Honour Commissioner Burgis, who dismissed it. The husband appealed to the Court of Appeal and that court allowed the appeal, […] set aside the order of the commissioner and directed the petition to be reheard before another judge. The petition was reheard on March 24, 1949, by Mr. Commissioner Blanco White, K.C. He concluded: I am not satisfied beyond a reasonable doubt that adultery has been committed. … The question reduces itself to the simple question as to whether the court’s judicial knowledge of natural laws enables it to say without any satisfactory evidence that 360 days between coition and birth is an impossible period and whether, if that period elapsed, a woman, who is, so far as the rest of the evidence goes, a respectable woman, is to be convicted of adultery. I think not. He dismissed the petition. The husband appealed to the Court of Appeal, which on July 22, 1949, set aside the commissioner’s order and ordered the cause to be reheard by a judge of the High Court if possible. The husband appealed to the House of Lords, asking that a decree for dissolution of the marriage should be made. The wife cross-appealed, asking that the petition should be dismissed. The final paragraph of the lengthy of opinions of the five Lords at the end of 1950: your text My Lords, I do not think it open to doubt that a time must come when, with the period far in excess of the normal, the court may properly regard its length as proving the wife’s adultery beyond reasonable doubt, and decree accordingly. But, as has so often been pointed out, the difficulty is to know where to draw the line. The reported cases naturally tend to creep up on each other and there is little sound guidance to be gleaned from authority. If a line has to be drawn I think it should be drawn so as to allow an ample and generous margin, for it may be as difficult for the wife to prove a freak of nature as for anyone else; and it need hardly be added that, before acting on the length of the period, due regard must be paid to any other relevant evidence. But I do not find it possible to go further and lay down any hard and fast rule capable of general application. In my opinion there is nothing in the testimony here, or in the facts of which judicial notice may be taken, to justify fixing a specific period and holding that, in the absence of evidence to the contrary, anything more would, but anything less would not, prove adultery beyond reasonable doubt. If, in the light of a full and exhaustive inquiry, the line could be thus fixed it might well be that the period of 360 days would come near or even cross it. But, however that may be, I am not prepared to decide this case merely on the fact that a period of that duration has been established. I prefer to relate my conclusion to the evidence as a whole and, for the reasons already given, I think it should have satisfied the learned commissioner beyond reasonable doubt. I would therefore allow the appeal and dismiss the cross- appeal. Appeal allowed. The following is from this ‘bible’ for those who became parents in the 1970s What Are the Chances of Delivering on Time? In over 17,000 cases of pregnancy carried beyond the twenty seventh week, 54 per cent delivered before 280 days, 4 percent had their babies either the week before or the week after the calculated date, and 74 per cent within a two-week period before or after the anticipated day of birth. On the basis of these data one can calculate the likelihood which the average woman faces when carrying a single infant, not twins, of having her baby, during each week after the twenty-seventh week from the first day of her last menstrual period. The author then presents a frequency distribution in 19 rows and 3 columns, with the headers Weeks (27, 28, … 45, 46+), Days (189-196, 196-203, … 308-315, 315+) and Approximate Chance (1:625, 1:625, 1:525, … 1:140, 1:140). We have converted it to a graphic. The author does on to say that Another reliable study has shown that 40 per cent of women go into labor within a ten-day period — five days before and five days after the calculated date, and nearly two thirds within plus or minus ten days of the expected time. Altman, writing in the BMJ in 1980, gives a graph based on 1970 data from England. In that (pre-ultrasound) era, some of the variation was dues to dating errors. But a 2013 article, based on carefully collected daily urine samples found that even with the precise measuremet of the timing of pregranct that these samples provides the length of human pregnancies can vary naturally by as much as five weeks Normally, women are given a date for the likely delivery of their baby that is calculated as 280 days after the onset of their last menstrual period. Yet only four percent of women deliver at 280 days and only 70% deliver within 10 days of their estimated due date, even when the date is calculated with the help of ultrasound. Now, for the first time, researchers in the USA have been able to pinpoint the precise point at which a woman ovulates and a fertilised embryo implants in the womb during a naturally conceived pregnancy, and follow the pregnancy through to delivery. Using this information, they have been able to calculate the length of 125 pregnancies. “We found that the average time from ovulation to birth was 268 days – 38 weeks and two days,” said Dr Anne Marie Jukic, a postdoctoral fellow in the Epidemiology Branch at the National Institute of Environmental Health Sciences (Durham, USA), part of the National Institutes for Health. “However, even after we had excluded six pre-term births, we found that the length of the pregnancies varied by as much as 37 days. ”We were a bit surprised by this finding. We know that length of gestation varies among women, but some part of that variation has always been attributed to errors in the assignment of gestational age. Our measure of length of gestation does not include these sources of error, and yet there is still five weeks of variability. It’s fascinating.&quot; The possibility that the length of pregnancies can vary naturally has been little researched, as it is impossible to tell the difference between errors in calculations and natural variability without being able to measure correctly the gestational age of a developing fetus. Previous studies conducted as long ago as the 1970s and 1980s had used the slight rise in a woman’s body temperature at waking as a way of detecting when ovulation occurred. This is an inexact measurement and cannot be used to detect when the embryo actually implants in the womb. Science Daily. But even if the variation is still (naturally) large, the original (1849) ruling in the Preston-Jones case seems extreme. Altman asks us to look at the distribution of length of gestation in his Fig 1, ‘which the judges apparently did not do.’ Altman himself thought that most people would feel that the husband was hard done by. Even If this case were heard now, where would you draw the line on the basis of (Altman’s) fig 1 ? This case illustrates a failure to use statistical methods when they ought to have been used, a fairly common occurrence. Saying that an event is possible is quite different from saying that it has a probability of, say, one in 100,000. Although not an example from medical research, this case concerne essentially the same difficulty as in many more frequentl encountered problems, such as denning hypertension or obesity Everything varies; it is in trying to draw lines between good and bad, high and low, likely and unlikely, and so on, that many problems arise. Although statistics cannot answer a given question, they can often shed considerable light on the problem. Altman, 1980. Today, JH would add that we should also consider, as those who heard the case did, the other evidence and testimony presented in the case. (We will come back to this when discussing the prosecutor’s fallacy.) 3.2.2 Ingredients and methods of procedure in a statistical test The bullet point following each item is specific to the Preston-Jones case. A claim about a parameter (or about the shape of a distribution, or the way a lottery works, etc.). Note that the null and alternative hypotheses are usually stated using Greek letters, i.e. in terms of population parameters, and in advance of (and indeed without any regard for) sample data. Parameter (unknown): Date of Conception CLAIMS by husband (h) and by wife (w) H\\(_0\\)(w): Date of Conception &lt; HUSBAND LEFT H\\(_A\\)(h): Date of Conception &gt; HUSBAND LEFT A probability model (in its simplest form, a set of assumptions) which allows one to predict how a relevant statistic from a sample of data might be expected to behave under H\\(_0\\). A probability model for statistic; examples: Gaussian, Empirical (common wisdom, and expert testimony at the time). In the 1949 hearing, the Commissioner referred to the ’court’s judicial knowledge of natural laws A probability level or threshold or dividing point below which (i.e. close to a probability of zero) one considers that an event with this low probability ‘is unlikely’ or ‘is not supposed to happen’ or ‘just doesn’t happen’. This pre-established limit of extreme-ness is referred to as the \\(\\alpha\\) level of the test. Fisher, in his treatment of the tea-tasting experiment, allowed that ‘It is open to the experimenter to be more or less exacting in respect of the smallness of the probability he would require before he would be willing to admit that his observations have demonstrated a positive result.’ Hovever, he is in part responsible for the 0.05 probability threshold commonly used today. None of those who pronounced on the Preston-Jones case gave a value for this. Denning, L.J., ‘thought that, according to the ordinary knowledge of mankind, a 360-day normal baby is impossible; and the courts should not assert that it is possible unless there is direct medical evidence to that effect.’ A sample of data, which under H\\(_0\\) is expected to follow the probability laws in (2) \\(y\\) = 360 days between when husband left and baby was born [the House of Lords account mentions 3 different departure dates, August 13, 17, and 29, but the Lords all spoke of 360 days in their deliberations] The most relevant statistic (e.g. \\(\\bar{y}\\) if interested in inference about the parameter \\(\\mu\\)) 360 - 280 = 80 days (here \\(n\\) = 1, so \\(y\\) and \\(bar{y}\\) are the same) The probability of observing a value of the statistic as extreme or more extreme (relative to that hypothesized under H\\(_0\\)) than we observed. This is used to judge whether the value obtained is either ‘close to’ i.e. ‘compatible with’ or ‘far away from’ i.e. ‘incompatible with’, H\\(_0\\). The ‘distance from what is expected under H0’ is usually measured as a tail area or probability and is referred to as the ‘P-value’ of the statistic in relation to H\\(_0\\). P = Prob[ 360 or 361 … | \\(\\mu\\) = 280]. Although it was not quantified, it was taken to be quite small. IN PASSING: This example re-inforces Fisher’s explanation as to why it is the tail area (the probablities of outcomes AS extreme OR MORE extreme than the observed one that is relevant, not the probability of the observed one. Even though day 280 is close to the mode of the distribution, fewer than 5% of babies are born on day 280, and only about 4% on day 285. This ‘just 4%’ does not make being ‘5 days late’ suspicious: more than 25% of babies are born this late or later. A comparison of this ‘extreme-ness’ or ‘unusualness’ or ‘amount of evidence against H\\(_0\\)’ or P-value with the agreed-on ‘threshold of extreme-ness’. If it is beyond the limit, H\\(_0\\) is said to be ‘rejected’. If it is not-too-small, H0 is ‘not rejected’. These two possible decisions about the claim are reported as ‘the null hypothesis is rejected at the P = \\(\\alpha\\) significance level’ or ‘the null hypothesis is not rejected at a significance level of 5%’. Mr. Commissioner Blanco White, K.C., who heard the petition in 1949, didn’t tell us his threshold, but his use of the word ‘impossible’ suggests it might have been zero. Bucknill, L.J. of the Court of Appeal noted that the commissioner in his judgment said it appeared to be a pure question of law whether 360 days is an impossible period without any satisfactory scientific evidence to that effect.‘Bucknill saw the case ’not in that light but rather as one where the court must balance the probabilities one against the other. On the one hand there is the very great improbability, I will not put it any higher, that a woman can, 360 days after coitus, give birth to a normal full-time child of 8 and a quarter pounds without any ifficult labour, and after a pregnancy which, according to the wife, was not unduly prolonged, but, on the contrary, was cut short. As against this high improbability there is the fact that the wife has put forward the case of a visit by the husband to her in November, a case which the commissioner has not accepted. If the matter rested there I should be in favour of allowing this appeal [siding wth the husband]. [if interested, see the origibal for why he ordered a re-hearing.] Denning, L.J., of the same Court of Appeal dissented, saying that ’the irresistible inference was that it was conceived in adultery. […] according to the ordinary knowledge of mankind, a 360-day normal baby is impossible; and the courts should not assert that it is possible unless there is direct medical evidence to that effect.’ In his summary, Lord Simonds said that since ‘The result of a finding of adultery in such a case as this is in effect to bastardize the child’, it ’is a matter in which from time out of mind strict proof has been required. But that does not mean that a degree of proof is demanded such as in a scientific inquiry would justify the conclusion that such and such an event is impossible. In this context at least no higher proof of a fact is demanded than that it is established beyond all reasonable doubt: see Head v. Head (18). To prove that a period of so many days between fruitful coition and the time of conception is in a scientific sense impossible is itself, I suppose, a scientific impossibility. The utmost that a court of law can demand is that it should be established beyond all reasonable doubt that a child conceived so many days after a particular coitus cannot be the result of that coitus. Other Examples: Quality Control (it has given us the terminology) Taste-tests (see exercises ) Jumping the gun in athletics After having had 3 NL sons, is potential-carrier mother a carrier? Adding water to milk..seeM&amp;M2Example6.6p448 Water divining..seeM&amp;M2exercise6.44p471 Randomness of U.S. Draft Lottery of 1970.. see M&amp;M2 Example 6.6 p105-107, and 447- Births in New York City after the “Great Blackout” John Arbuthnot’s “argument for divine providence” US Presidential elections: Taller vs. Shorter Candidate. Gelman and Hill, HELP. 50,000 votes Laplace average depth of the oceans is of the same order as the average height of the continents. Under the null hypothesis of the independence of the row and column factors, the expected value of the chi-square statistic calculated from a 2 \\(\\times\\) 2 table is 4 - 1 = 3 (said Karl Pearson in 1900) or (2 - 1)\\(\\times\\) (2 - 1) = 1 (said Ronald Fisher, and G Udny Yule in 1922). The same arguments which explode the Notion of Luck may, on the other side, be useful in some Cases to establish a due comparison between Chance and Design. We may imagine Chance and Design to be as it were in Competition with each other for the production of some sorts of Events, and may calculate what Probability there is, that those Events should be rather owing to one than to the other… From this last Consideration we may learn in many Cases how to distinguish the Events which are the effect of Chance, from those which are produced by Design. Abraham de Moivre: ‘Doctrine of Chances’ (1719) THE FIRST RECORDED P–VALUE??? by a physician no less!! “AN ARGUMENT FOR DIVINE PROVIDENCE, TAKEN FROM THE CONSTANT REGULARITY OBSERV’D IN THE BIRTHS OF BOTH SEXES.” John Arbuthnot, 1667-1735 physician to Queen Anne Arbuthnot claimed to demonstrate that divine providence, not chance, governed the sex ratio at birth. To prove this point he represented a birth governed by chance as being like the throw of a two-sided die, and he presented data on the christenings in London for the 82-year period 1629-1710. Under Arbuthnot’s hypothesis of chance, for any one year male births will exceed female births with a probability slightly less than one-half. (It would be less than one-half by just half the very small probability that the two numbers are exactly equal.) But even when taking it as one-half Arbuthnot found that a unit bet that male births would exceed female births for eighty-two years running to be worth only (1/2)82 units in expectation, or 1 4 8360 0000 0000 0000 0000 0000 a vanishingly small number. “From whence it follows, that it is Art, not Chance, that governs.” STIGLER : HISTORY OF STATISTICS nificance levels can correspond to small, nonsignificant changes in the underlying variables. 2 separate issues Ho and intervals (treated together in Bayesian) • Statistical Quality Control procedures [for Decisions] • Sample survey organizations: Confidence intervals • Statistical Tests of Hypotheses Unlike Bayesian inference, there is no quantified pre-test or pre- data “impression”; the ultimate statements are about data, conditional on an assumed null or other hypothesis. Thus, an explanation of a p-value must start with the conditional “IF the parameter is … the probability that the data woul 3.3 Does the approach matter? semantics "],
["CI.html", "Chapter 4 Parameter Intervals 4.1 ‘100% confidence’ intervals 4.2 More-nuanced intervals 4.3 SUMMARY", " Chapter 4 Parameter Intervals The objective of this first section is to provide simple examples of reverse engineering that show some of the logic behind statistical ‘confidence intervals’ for parameters. We begin with ‘100% confidence’ intervals, and then, in section 2, we explain why we have to move to ‘less-than-100% confidence’ intervals, where things get a bit more nuanced. In both sections, we emphasize the reverse engineering, i.e, by using as our limits the worst-case or almost-worst-case scenarios involving the (unknown) values of the parameter that is being estimated. 4.1 ‘100% confidence’ intervals Example 1 Consider a very ‘particularistic’ parameter, the height of a particular building. There is nothing ‘scientific’ about the parameter, except maybe that we use tools of mathematical science (of trignometry) to measure it. Nevertheless, we will sometimes refer to it one of the generic symbols for a parameter, namely \\(\\theta.\\) Suppose you measure the height of this building by standing a known horizontal distance (e.g. 100 metres) from the bottom of the building and using an instrument to measure the angle between the horizontal and the top of the building. Suppose, as shown in the left panel of the Figure below, that the instrument gives a reading of 70 degrees. Remembering from trigonometry that the tangent of a 70 degree angles is 2.75, the angle of 70 degrees suggests that the height of the building is \\(\\hat{\\theta}\\) = 275 metres. The ‘hat’ is statistical shorthand for ‘estimate of.’ Since it is sometimes referred to as a ‘point estimate’ of \\(\\theta\\), we display the value using a dot or point. After calculating this, you learn that the measuring instrument only displays the angle is to the nearest 10 degrees. This means that the true angle is somewhere between 65 and 75 degrees. [This is the same range you would get if it was dark and you used a laser pointer or flashlight attached to a wheel that rotates in fixed 10-degree steps, i.e., 5 degrees, 15 degrees, 25 degrees, etc. At 65 degrees, the light is visible on the building, but at 75 degrees, it goes above the building and shines into the sky.] So you cannot say that the true height is exactly 275 metres. What can you say? And with what certainty? You can put limits on the true height by asking what are the minimum and maximum heights that could have produced the observed reading of 70 degrees? To do this you need to take the limits one at a time. The minimum angle that could have given the (rounded) readout of 70 degrees is 65 degrees, and this corresponds to a minimum height (lower limit) height of \\(\\theta_L\\) = 214 metres. The maximum angle that could have given the readout of 70 degrees is 75 degrees, and this corresponds to a maximum height (upper limit) of \\(\\theta_U\\) = 373 metres. Thus, assuming that the instrument is measuring the angle correctly, and then doing what you are told it does, you are 100% confident that the true height lies in the interval (214, 373). As is clear in the graph, this does not have the typical 275 \\(\\pm\\) a single-number (or in sybols, \\(\\hat{\\theta} \\pm\\) ME [‘margin of error’]) that we typically see in reports. Figure 4.1: Estimating the height of an building by measuring subtended angles. The ‘70’ in the left panel signifies that the real angle was somewhere between 65 and 75 degrees; thus the real height lies between the L and U limits of 214 and 373 metres. In the righ panel, the interval shown by the thicker black segment to the right of the 3 individual intervals is the set of parameter values common to all 3. More data The panel on the right shows how, by obtaining 3 measurements at 3 different distances, and finding the interval they have in common (the overlap), you can narrow the interval within which the true height must lie. What allows us to be 100% confident in the parameter interval The reason is the limited error range. How wide the error range is, and how many measurements one makes, determine how wide the parameter interval is. Example 2 This one is less artificial, and indeed is motivated by a real court case in the late 1990s in Quebec, where a defendant’s age (which would determine wheter he was tried in an adult or a juvenile court) was in doubt. He was adopted, while still a young child, from another country. Official birth records were not available, and his adoptive parents were able to get a cheaper airfare by claiming that he was under age 2 at the time. Bone age, and Tanner Staging, also known as Sexual Maturity Rating (SMR), an objective classification system used to track the development and sequence of secondary sex characteristics of children during puberty, were other pieces of information used by the judge. For more on this topic of determining chronological age, see this article, entitled Many applications for medical statistics and thos one, entitled People smugglers, statistics and bone age, by UCL statistics professor and child growth expert, Tim Cole. Again, the person’s correct chronological age is a particularistic parameter, one that had nothing to do with science, or universal laws of Nature. But it can be estimated by using the laws of mathematics and statistics. For didactic purposes, we will simplify matters, and assume that ‘our’ indirect method gives estimates such that if many of them were averaged, they would give the correct chronological age of the person (in statistical lingo, statisticians say that the method/estimator is ‘unbiased’). However, as is seen below, the individual measurements vary quite a bit around the correct age. They can be off by as much as 25% (1/4th) in either direction. [In practice, a measuring instrument with this much measurement error would not be useful – unless it was fast, safe, inexpensive, non-invasive, easily repetaed, and so on – but we make the measurement variations this large just so we can see the patters more clearly on the graph!]. Another unrealistic feature of our ‘measurement model’ is that the ‘error distribution’ has a finite range. The shape of the error distribution doesn’t come into the 100% ‘confidence intervals’ below, but it will matter a little bit – but not a whole lot unless the sample size is small – later on when we cut corners. Consider first a single indirect measurement of chronological age, that yielded a value of 17.6 years. Given what you know about the sizes of the possible errors, you cannot say that the true age is exactly 17.6 years What can you say? And with what certainty? You can put limits on the true age by asking what are the minimum and maximum ages that could have produced the observed reading of 17.6 years. To do this you need to consider the limits one scenario at a time. The minimum age that could have given the estimate of 17.6 years is / 1.125 = 15.7 years. The maximum age that could have produced this reading is 17.6 / 0.875 = 20.1 years. Thus (assuming the error model is correct!) you are 100% confident that the true age lies in the interval (15.7 , 20.1) years. Again, as is clear in the graph, this does not have the typical 17.6 \\(\\pm\\) a single-number margin of error that we typically see in reports. Rather, it is 17.6 - 2.6 and 17.6 + 4.4 ! But, you can’t arrive at these directly; get there this way. You have to try on various limits, until \\[ LowerLimit + margin = 17.6 \\ = \\ UpperLimit - margin \\] Figure 4.2: 100% Confidence Intervals for a person’s chronological age when error distributions (that in this example are wider at the older ages) are 100% confined within the shaded ranges. Left: based on n = 1 measurement; right: based on n = 4 independent measurements. More data The panel on the right shows how, by obtaining 4 independent measurements, and finding the interval they have in common, you can narrow the interval containing the true age. Can we narrow the interval more, maybe by first averaging the 4 measurements? Should the mean of 4 measurements give us more information, ie., a tighter interval, that the one based on the overlap? The sad fact is that, as long we insist on 100% confidence in our interval (or our procedure), we can not: the mean of the 3 measurements can still – theoretically – be anywhere in the same 0.75 \\(\\times\\) True Age to 1.25 \\(\\times\\) True Age range – just as a single measurement can. The only way to narrow the interval is to take a chance, cut corners, and accept a lower confidence level. To do this, we need to know a bit more about where the pattern (shape) of the error distribution (**up to now we didn’t use the shape, just the boundaries). In other words, we need to know how much of the error distribution is in the corners, so that we can cut them! In the next section, we will stick for now with Daniel Bernoulli’s error distribution, but cut some corners. (Later on, we will cut some corners on Laplace’s and Gauss’s error distributions, but with the same standard deviation as in Bernoulli’s error curve.) 4.2 More-nuanced intervals We will cut 5% from each corner of the distribution, and focus on the middlemost 90%. From the formula for its mathematical shape, we can calculate that this measurement range is from -1 \\(\\times\\) the radius of the semi-ellipse to +1 \\(\\times\\) the radius. There is only a 5% probability of observing a measurement below (to the left of) this interval, and a 5% probability of observing a measurement above (to the right of) the interval. After we observe our single measurement, we ‘try it on’ against all possible true-age-scenarios. We retain only those true-age-scenarios in which the observed measurement would fall within this central (90%) range. We discard (‘rule out’) those age scenarios in which the measurement would be at one extreme or the other extreme, in one of the two excluded or ‘cut’ corners. The left panel shows the (now narrower, and more nuanced) range of true-ages (rahe of parameter values) that is compatible with the observed measurement of 13.1 years. In all other age-scenarios, the 13.1 would have been too extreme, and so these scenarios are discarded. We can think of the ‘ruled in’ range as our (nuanced, compromise) parameter interval. Note again the method of constructing this non-symmetric parameter interval, namely one boundary at a time. It does not fit the \\(\\pm\\) mold. It does, however, give a way of talk about such an interval: The observed measurement (point-estimate) may be an underestimate of the parameter: it might have fallen short of the true parameter value. Or, it may be an underestimate: it might have overshot the true parameter value. The plus and the minus amounts are the almost-maximal amounts by which our shot might have been off-target. (as we will see later, the maximal error can be infinite, so we have to put some probalistic limits on the error if we are to narrow the interval). Q: Does this procedure for constructing intervals have a 90% success rate, if used up and down all of the ages, say from 10 to 30 years? We could try it out with people of known ages. [answer by simulation] You will discover in your simulations that it might matter whether you simulate the same number of 16 year olds as 10 years, i.e., what the mix of real ages is. This does not matter in the 100% intervals, but it might if you are more nuanced. For example,instead of estimating age by an indirect method, pretend you were were estimating a person’s height indirectly, by just measuring their arm span (at each height, the mean armspan is very close to the height, but there is a spread of armspans (pardon the pun!)). And (just like in our example 2 where the spread increases with the mean) the spread of armspans is larger in people who are 6 feet tall than it is in people 5 feet tall. BUT, there aren’t as many people 5 feet and 6 feet tall as there are people 5 feet 6 inches. So, the distribution of heights in people with a span of 5 feet 11 might have a different shape than that in people with a span of 5 feet 6, or 5 feet. Simulations (or even some diagrams) could settle the issue as to whether the height-mix (or, in example 2, the age mix) matters. What is your intuition as to whether it affects the perfornace of your nuanced parameter estimates? The point is that your method needs to have the same claimed performance (say 90%) at any age you throw at it. Figure 4.3: 90% Confidence intervals for Chronological Age when only 90% of the error distributions lie within the shaded ranges. When we have \\(n = 3\\) observations (right panel), it is not so easy to say how confident we should be about the overlap of the 3 intervals. Instead, we would be bettter off taking the mean of the 3 measurements, and ‘trying on’ this single mean against the various sampling distributions of the means of 3 independent measurements from a semi-circular error distribution. Again, since the range remains the same, we would again have to cut corners. We will illustrate this when we now consider the easier-to-work-with Gaussian error distribution. A more realistic error distribution Although it makes it easier to demonstrate ‘100% confidence’ intervals, Daniel Bernoulli’s error distribution is both mathematically unfamiliar, and a bit unrealistic. And it isn’t that easy to see how to use it for situations where you wiull want to take an average of several independent indirect measurements. So, we now switch to a more familiar and more realistic bell-shaped error distribution. It is often called the Gaussian distribution, even if Gauss was not the first to publish it. Variations of it were alraedy discovered by deMoivre many decades earlier, and Gauss’ competotor, Laplace , publihed it well before Gauss did. Gauss claimed he was using it for many years before he published it. Laplace also used a different (more spiked) error distibution that today is called after him. In any case, lets switch to bell-shaped distributions of the age measurements at each true age, but let’s keep the ‘average squared deviation’ or its square root, the standard deviation, the same as it was in the Bernoulli model. on 10 Mark note NEXT we deliberately took non-symmetric situations where it is not +/- Now will show sitations where the error doesnt get bigger of smaller with the context, and where the ‘trying-on’ is faster. 4.3 SUMMARY what a parameter interval is If an error distribution is bounded, we can be 100% confident in our parameter interval, and we can narrow it by taking more measurements. Moreover, we don’t need to specify the exact shape of the error distribution. All that matters are its bounds. I don’t think you should take for granted that students (or even professors!) will know what an error distribution is. So I think a first point should be to describe what you mean by an error distribution. With unbounded error distributions, a 100% parameter interval may be unacceptably wide, even if we take many measurements. Thus, we have to ‘give up something’ (in certainty) in order to ‘get something’ (a narrower interval). Moreover, we need to either (a) specify a model for the shape of the error distribution, or (b) use data-intensive techniques, such as re-sampling, to be able to ‘cut the corners.’ Either way, a logical way to determine parameter intervals is to have them consist of all the parameter-value scenarios in which the observed measurement (or summary measurement) is ‘plausible’. The upper limit for the parameter is the scenario in which the measurement would be probalistically near the bottom of the corresponding sampling distribution; the lower limit is the scenario in which the measurement would be near the top of the corresponding sampling distribution. If the error (or sampling) distributions have differing spreads at different parameter values, then the parameter interval will not be symmetric about the point estimate. If the error (or sampling) distributions have the same spreads at different parameter values, then the parameter interval will be symmetric about the point estimate, and thus, easier to calculate. it would be good to give a concrete example here.e.g. repeat the age example –&gt; It’s harder to accurately guess the true age of someone who is older. perhaps also a good time to mention that in this situation, the +/- formula will fail you It is not correct to view the parameter as ‘falling’ on one or other side of the measurement. The true parameter values is fixed, and isn’t moving or falling anywhere. Rather, it is the observed measurement (point-estimate) that may have fallen to the left of (fallen short of), and thus provided an underestimate of, the true parameter value: Or, it may have overshot the true parameter value, and thus overestimated it. This point also explains why the +/- formula fails us Missing from the above summary points is a direct answer to the question or criticisms we are likely to face: “Why do we care about Wilson when the +/- gives you the same answer?” The answer to this question is given indirectly in points 4) and 5). But I think we need to be explicitly clear about this, since for me at least, is the main motivation for this chapter. I also think the objective of the chapter needs to be revised. Based on our discussions, the point of the chapter is to explain: 1) Why the +/- formula fails us in the interpretation of a CI and when the spread of the error distribution is non-constant and 2) Wilson’s idea to remedy the situation. If you agree with those two objectives, the next question to answer is how ‘100% confidence intervals’ and ultimately ‘cutting corners’ helps you explain the points 1) and 2). By the way, a simplified version of the Wilson plot you made will be a great help in visualizing point 3: Another unrealistic feature of our ‘measurement model’ is that the ‘error distribution’ has a semi-circular (or rather semi-eliptical) shape. In statistics, it is called the ‘Wigner’ semicircle distribution. First written about by Daniel Bernoulli, predates the error distributions of Laplace and Gauss. If the archer makes innumerable shots, all with the utmost possible care, the arrows will strike sometimes the first band next to the mark, sometimes the second, sometimes the third and so on, and this is to be understood equally of either side whether left or right. Now is it not self-evident that the hits must be assumed to be thicker and more numerous on any given band the nearer this is to the mark? If all the places on the vertical plane, whatever their distance from the mark, were equally liable to be hit, the most skilful shot would have no advantage over a blind man. That, however, is the tacit assertion of those who use the common rule in estimating the value of various discrepant observations, when they treat them all indiscriminately. In this way, therefore, the degree of probability of any given deviation could be determined to some extent a posteriori, since there is no doubt that, for a large number of shots, the probability is proportional to the number of shots which hit a band situated at a given distance from the mark. Moreover, there is no doubt that the greatest deviation has its limits which are never exceeded and which indeed are narrowed by the experience and skill of the observer. Beyond these limits all probability is zero; from the limits towards the mark in the centre the probability increases and will be greatest at the mark itself. [Note: developers and teachers of statistical methods have long made use of archery, gunnery, darts, and aiming at targets. Indeed, the word stochastic, which refers to randomnees, has its roots in the Greek word stokhastikos, meaning able to guess, with the root stokhos meaning a target. Klein (1997) writes of ‘men reasoning on the likes of target practice’ and describes how this imagery has pervaded the thinking and work of natural philosophers and statisticians ] The foregoing give some idea of a scale of probabilities for all deviations, such as each observer should form for himself. It will not be absolutely exact, but it will suit the nature of the inquiry well enough. The mark set up is, as it were, the centre of forces to which the observers are drawn; but these efforts are opposed by innumerable imperfections and other tiny hidden obstacles which may produce in the observations small chance errors. Some of these will be in the same direction and will be cumulative, others will cancel out, according as the observer is more or less lucky. From this it may be understood that there is some relation between the errors which occur and the actual true position of the centre of forces; for another position of the mark the outcome of chance would be estimated differently. So we arrive at the particular problem of determining the most probable position of the mark from a knowledge of the positions of some of the hits. It follows from what we have adduced that one should think above all of a scale (scala) between the various distances from the centre of forces and the corresponding probabilities. Vague as is the determination of this scale, it seems to be subject to various axioms which we have only to satisfy to be in a better case than if we suppose every deviation, whatever its magnitude, to occur with equal ease and therefore to have equal probability. Let us suppose a straight line in which there are disposed various points, which indicate of course the results of different observations. Let there be marked on this line some intermediate point which is taken as the true position to be determined. Let perpendiculars expressing the probability appropriate to a given point be erected. If now a curve is drawn through the ends of the several perpendiculars this will be the scale of the probabilities of which we are speaking. If this is accepted, I think the following assumptions about the scale of probabilities can hardly be denied. Inasmuch as deviations from the true intermediate point are equally easy in both directions, the scale will have two perfectly similar and equal branches. Observations will certainly be more numerous and indeed more probable near to the centre of forces; at the same time they will be less numerous in proportion to their distance from that centre. The scale therefore on both sides approaches the straight line on which we supposed the observed points to be placed. The degree of probability will be greatest in the middle where we suppose the centre of forces to be located, and the tangent to the scale for this point will be parallel to the aforesaid straight line. If it is true, as I suppose, that even the least-favoured observations have their limits, best fixed by the observer himself, it follows that the scale, if correctly arranged, will meet the line of the observations at the limits themselves. For at both extremes all probability vanishes and a greater error is impossible. Finally, the maximum deviations on either side are reckoned to be a sort of boundary between what can happen and what cannot. The last part, therefore, of the scale, on either side, should approach steeply the line on which the observations are sited, and the tangents at the extreme points will be almost perpendicular to that line. The scale itself will thus indicate that it is scarcely possible to pass beyond the supposed limits. Not that this condition should be applied in all its rigour if, that is, one does not fix the limits of error over-dogmatically. If we now construct a semi-ellipse of any parameter on the line representing the whole field of possible deviations as its axis, this will certainly satisfy the foregoing conditions quite well. The parameter [radius] of the ellipse is arbitrary, since we are concerned only with the proportion between the probabilities of any given deviation. However elongated or com-pressed the ellipse may be, provided it is constructed on the same axis, it will perform the same function; which shows that we have no reason to be anxious about an accurate description of the scale. In fact we can even use a circle, not because it is proved to be the true scale by mathematical reasoning, but because it is nearer the truth than an infinite straight line parallel to the axis, which supposes that the several observations are of equal weight and probability, however distant from the true position. This circular scale also lends itself best to numerical calculations; meanwhile it is worth observing in advance that both hypotheses come to the same whenever the several observations are considered to be infinitely small. They also agree if the radius of the auxiliary circle is supposed to be infinitely large, as if no limits were set to the deviations. Thus if the deviation of an observation from the true position is thought of as the sine of a circular arc, the probability of that observation will be the cosine of the same arc. Let the auxiliary semicircle, which I have just described, be called the controlling semicircle (moderator). Where the centre of this semicircle is located, the true position, which fits the observations best, is to be fixed. Admittedly our hypothesis is, to some extent, precarious, but it is certainly to be preferred to the common one, and will not be hazardous to those who understand it, since the result that they will arrive at will always have a higher probability than if they had adhered to the common method. When by the nature of the case a certain decision cannot be reached, there is no other course than to prefer the more probable to the less probable. I will illustrate this line of argument by a trivial example. The particular problem is the reconciliation of discrepant observations; it is therefore a question of difference of observations. Now if a dice-thrower makes three throws with one die so that the second exceeds the first by one and the third exceeds the second by two, the throws may arise in three ways, viz. 1,2,4 or 2,3,5 or 3,4,6. None of these throws is to be preferred to the other two, for each is in itself equally probable. If you prefer the one in the middle, viz. 2,3,5, the preference is illogical. The same sort of thing happens if you choose to consider observations which, so far as you are concerned, are accidental, whether they are astronomical or of some other kind, as equally probable. Now suppose the thrower produces the same result by throwing a pair of dice three times. There will then be eight different ways in which he would obtain this result, viz. 2,3,5; 3,4,6; 4,5,7; 5,6,8; 6,7,9; 7,8,10; 8,9,11 and 9,10,12. But they are far from being all equally probable. It is well known that the respective probabilities are proportional to the numbers 8, 30, 72, 100, 120, 80, 40 and 12. From this known scale I have better right to conclude that the fifth set has happened than that any other has, because it has the highest probability; and so the three throws of a pair of dice will have been 6, 7 and 9. No-one, however, will deny that the first set 2, 3 and 5 might possibly have happened, even though it has only a fifteenth part of the probability corresponding to the fifth set. Forced to choose, I simply choose what is most probable. Although this example does not quite square with our argument, it makes clear what contribution the investigation of probabilities can make to the determination of cases. Now I will come more to grips with the actual problem. First of all, I would have every observer ponder thoroughly in his own mind and judge what is the greatest error which he is morally certain (though he should call down the wrath of heaven) he will never exceed however often he repeats the observation. He must be his own judge of his dexterity and not err on the side of severity or indulgence. Not that it matters very much whether the judgement he passes in this matter is fitting or somewhat flighty. Then let him make the radius of the controlling circle equal to the aforementioned greatest error; let this radius be r and hence the width of the whole doubtful field = 2r. If you desire a rule on this matter common to all observers, I recommend you to suit your judgement to the actual observations that you have made: if you double the distance between the two extreme observations, you can use it, I think, safely enough as the diameter of the controlling circle, or, what comes to the same thing, if you make the radius equal to the difference between the two extreme observations. Indeed, it will be sufficient to increase this difference by half to form the diameter of the circle if several observations have been made; my own practice is to double it for three or four observations, and .to increase it by half for more. Lest this uncertainty offend any one, it is as well to note that if we were to make our controlling semicircle infinite we should then coincide with the generally accepted rule of the arithmetical mean; but if we were to diminish the circle as much as possible without contradiction, we should obtain the mean between the two extreme observations, which as a rule for several observations I have found to be less often wrong than I thought before I investigated the matter. We must wonder whether its was that this eminent mathematician (who also, incidentally contributed to the epidemiological debate about smallpox vaccination) was unable to think of a formula for an error distribution that did not end abruptly, and that would instead ‘flow out’ further in both directions. Or was it that he wasn’t bothered by having to find the roots of 5th degree polynomials to find the best ‘centre’ of his semicircukar distribution? As it turns out, the mathematics involved in finding (what we now call today) the Maximun Likelihood Estimator for Laplace’s and Gauss’s (infinitely wide) error distributions is much simpler. But the, we have the benefit of hindsight. The key ideas – the semi-ellipse and the fixed (assumed known) – are highlighted. "],
["paraMu.html", "Chapter 5 The ‘mean’ parameter \\(\\mu\\) 5.1 Two genres 5.2 Fitting these to data / Estimating them from data", " Chapter 5 The ‘mean’ parameter \\(\\mu\\) [and other location (and spread and shape) parameters] The objectives of this chapter are to Although few textbooks do so, we think it is worth distinguishing two contexts. 5.1 Two genres The first is where, if there were no measurement issues, we would ‘see’ / ‘get’ / ‘observe’ the same constant every time we made a determination, but where, because of unavoidable measurement variations, there is a statistical distribution of ‘measurements’ around that constant. Examples include measurements of constants such as the speed of light, or of physical constants, such as a standard weight (e.g. 1 Kg) or of a fixed distance measured by a smart phone app or a fixed number of steps measured by a step-counter. Examples of ‘personal’ constants that are constant – at least in the short term – but not easily or reproducibly measured might be the size of a person’s vocabulary, or a person’s mean (or minimum, or typical) reaction time. Or, the target could be a person’s ‘true score’ on some test – the value one would get if one (could, but not realistically) be tested on each of the (very large) number of test items in the test bank, or observed/measured continously over the period of interest.     Starting with this simpler ‘measurement variation only’ context makes it easier to master the statistical laws that govern the variation of values derived from a combination of measurements, the variation of statistics. The second is where the variation is primarily (or in a few deluxe cases where there are no measurement issues, entirely) due to genuine – e.g., biological – variation. Examples of such (often effectively infinite in size) biological distributions include the depths of the ocean, or the heights or weights or blood pressures of a specific population.     In this context, the distribution is less likely to display the symmetry observed when the variation is entirely due to measurement variations around some constant. Thus, there may be several possible choices of the ‘centre’ of the distribution. So, in addition to pursuing the ‘mean’ parameter \\(\\mu\\) we will also pursue other numerical parameters for the centre. No matter which of the two genres we are dealing with, it may be important to quantify the spread (and maybe the shape) of the distribution. Even if this aspect may be of secondary interest, it has a bearing on what we can say about how far off the target (off the parametr) our estimators might be. We will begin with the first genre, where, if there were no measurement issues, we would ‘see’ / ‘get’ / ‘observe’ the same constant every time we made a determination, but where, because of unavoidable measurement variations, there is a statistical distribution of ‘measurements’ around that constant. Because its estimation involves the same statistical laws as when the distribution/variation is biological, we will refer to this elusive ‘constant’ as \\(\\mu.\\) 5.2 Fitting these to data / Estimating them from data Experiments to Determine the Density of the Earth. By Henry Cavendish, Esq. F.R.S. and A.S. Philosophical Transactions of the Royal Society of London, Vol. 88. (1798), pp. 469-526. http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Mean-Quantile/Cavendish1798.pdf The following Table contains the Results of the Experiments density = c( 5.50, 5.61, 4.88, 5.07, 5.26, 5.55, 5.36, 5.29, 5.58, 5.65, 5.57, 5.53, 5.62, 5.29, 5.44, 5.34, 5.79, 5.10, 5.27, 5.39, 5.42, 5.47, 5.63, 5.34, 5.46, 5.30, 5.75, 5.68, 5.85) round(mean(density),2) ## [1] 5.45 lm.fit = lm(density ~ 1) print(summary(lm.fit),digits=1) ## ## Call: ## lm(formula = density ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57 -0.15 0.01 0.16 0.40 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.45 0.04 133 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2 on 28 degrees of freedom round(confint(lm.fit),2) ## 2.5 % 97.5 % ## (Intercept) 5.36 5.53 library(mosaic) bootstrap.fits &lt;- do(1000) * lm( resample(density) ~1) round(confint(bootstrap.fits$Intercept),2) ## 2.5% 97.5% ## percentile 5.37 5.53 round(sd(bootstrap.fits$Intercept),2) ## [1] 0.04 Metrics (criteria) for measuring (best) fit "],
["paraPi.html", "Chapter 6 The (proportion) parameter 6.1 Example one 6.2 Example two", " Chapter 6 The (proportion) parameter 6.1 Example one etc 6.2 Example two etc "],
["paraLambda.html", "Chapter 7 The (event rate) parameter 7.1 Etc 7.2 ETC", " Chapter 7 The (event rate) parameter 7.1 Etc 7.2 ETC "],
["contrast2Muparas.html", "Chapter 8 Contrast: 2 mean parameters 8.1 Estimand, estimator, estimate", " Chapter 8 Contrast: 2 mean parameters 8.1 Estimand, estimator, estimate "],
["contrast2Piparas.html", "Chapter 9 Contrast: 2 proportion parameters 9.1 Estimand, estimator, estimate", " Chapter 9 Contrast: 2 proportion parameters 9.1 Estimand, estimator, estimate "],
["contrast2Lambdaparas.html", "Chapter 10 Contrast: 2 speed parameters 10.1 Estimand, estimator, estimate", " Chapter 10 Contrast: 2 speed parameters 10.1 Estimand, estimator, estimate "],
["probability.html", "Chapter 11 Probability 11.1 Objectives 11.2 Probability Scales 11.3 Basic rules for probability calculations 11.4 Conditional probabilities, and (in)dependence 11.5 Changing the Conditioning: the direction matters 11.6 Summary Slides 11.7 Exercises:", " Chapter 11 Probability 11.1 Objectives The objectives of this chapter are to Understand the meanings, uses and ways of arriving at, numerical values for probabilities. Appreciate the context for a probability, and that context (the conditioning, the information relied on) matters. See the basic rules for calculating probabilities in a few examples, but appreciate that calculating non-standard probabilities should be left to experts. In practice, most of our probability values come from some pDistribution function, and we will encounter some of the ‘off the shelf’ ones in the next chapter. Be warned against calculating the wrong probability. Beware of intuition, especially with ‘after the fact’ calculations for non-standard situations. Probability Meaning Long Run Proportion Estimate of (Un)certainty Amount prepared to bet Use Communicate one’s (un)certainty about the value of a parameter Describe the randomness of data Measure how far data are from some hypothesized model How Arrived At Subjectively Intuition, Informal calculation, consensus Empirically Experience (actuarial, …) Pure Thought Elementary Statistical Principles; if necessary, by breaking complex outcomes into simpler ones Advanced Statistical Theory, e.g.. Gauss’ Law of Errors Figure 11.1: In this older cartoon, these experts qualify their probability statement. What is the probability of/that ? … Death | Taxes | Rain tomorrow | Cancer in your lifetime | Win lottery in single try | Win lottery twice | Get back 11/20 pilot questionnaires | If treat 14 patients, get 0 successes | [Duplicate Birthdays(http://www.medicine.mcgill.ca/epidemiology/hanley/c323/excel/birthdays_607.pdf) | Canada will use $US before the year 2030 | another new/different pandemic by 2030 | OJ Simpson murdered his wife | DNA from a random (innocent) person would match blood found at crime scene Often, we work in other scales that spread out the probability scale. The odds scale runs from 0 to in finity, while the logit and probit scales – important for regression modeling – span the entire real line, from minus infinity to infinity. You will need to become confortable with these wider scales. 11.2 Probability Scales Figure 11.2: The probability scale, along with 3 equivalent scales derived from it. The probit scale transforms P into a Z score: thus P = 0.025 becomes Z = -1.96, and P = 0.975 becomes Z = +1.96; to reverse direction, if Z = qnorm(P), then P = pnorm(Z). To convert a LOGIT back to an odds, ODDS = anti-log(LOGIT) = exp(LOGIT), and to convert an odds back to a probability, P = ODDS/(1+ODDS). • 50 year old has colon ca • 50 year old with +ve haemoccult test has colon ca • child is Group A Strep B positive • 8 yr old with fever &amp; v. inflamed nodes is Gp A Strep B positive • There is life on Mars References • WMS5, Chapter 2 • Moore &amp; McCabe Chapter 4 •Colton, Ch 3 • Freedman et al. Chapters 13,14,15 •Armitage and Berry, Ch 2 • Kong A, Barnett O, Mosteller F, and Youtz C. “How Medical Professionals Evaluate Expressions of Probability” NEJM 315: 740-744, 1986 • Death and Taxes • Rain tomorrow • Cancer in your lifetime • Win lottery in single try • Win lottery twice • Get back 11/20 pilot questionnaires • Treat 14 patients get 0 successes • Duplicate Birthdays • Canada will use $US before the year 2010 11.3 Basic rules for probability calculations Figure 11.3: Basic Probablity Axioms and Rules. ‘B | A’ means ‘B GIVEN A’ or ‘B CONDITIONAL ON A’. 11.4 Conditional probabilities, and (in)dependence It is surprising how few textbooks use ‘trees’ (such as shown above) to explain conditional probabilities. Probability trees make it easy to see the direction in which one is preceeding, or looking, where simply (and often arbitrarily chosen) algebraic symbols like A and B can not; trees make it easier to distinguish ‘forward’ from ‘reverse’ probabilities. Tip: try to order letters so it is \\(A\\rightarrow B\\), rather than \\(B\\rightarrow A.\\) Trees show that – no matter whether the events are independent or dependent – the probability of a particular sequence is always a fraction of a fraction of a fraction … . Moreover we start with the full probability of 1 at the single entry point on the extreme left, so we need at the right hand side to account for all of this (i.e., the same ‘total’) probability. Just like the law of the conservation of mass, there is also a law on the conservation of probability; it cannot be lost or destroyed along the way. This first example highlights the difference between independent events (left) and non-independent events (right). Figure 11.4: Gender Distribution in a sample of size \\(n\\)=2. LEFT: 2 independent Births; RIGHT: Randomly sampling 2 persons from 10 persons (5M 5F), without replacenent. In both examples, the terminal probabilities are obtained by multiplication. In the second example, the second probability depends on the outcome of the first selection. With independence, one doesn’t have to look over one’s shoulder to the previous event to know which probability to use for the second portion of the product. In the example on the gender composition of 2 independent births, when one comes to the second component (birth) in the probability product, Prob(2nd birth is a male) is the same whether one has got to there via the ‘upper’ path, or the ‘lower’ one. ‘Contitioning on’ or ‘knowing the result of’ the first event does not alter the second probability. With non-dependence, one does have to look at where one has come from. In the example on the gender composition of a sample of 2 persons sampled (without replacement) from a pool of 5 males and 5 females, the Prob(2nd person selected is a male) is differs depending on which path you have already taken. The multiplication rules also apply to pairs of events that are not strictly serial in nature: the pairs can be assembled in other ways, such as in this second example. Figure 11.5: Another example of the difference between ‘independent’ and ‘related’ results. At issue is the composition of a make-female pair with respect to the number of sampled persons (out of \\(n\\) = 2) that smoke (0, 1, or 2). Left: the pair are formed at random, the Male selected randomly from all adult males, and the Female selected randomly from all adult females. Prob that F smokes is not realted to whether M does or does not. Right: a (related) M:F couple is selected from among all M:F couples, ‘F, M+’ denotes a Female whose M partner does smoke, and ‘F, M-’ denotes a Female whose M partner does not. These probabilities that these 2 types of Females smoke will differ from each other. 11.5 Changing the Conditioning: the direction matters Example: Medical Diagnostic Tests The left side of this next panel is an example where, when evaluating the performance of a new diagnostic test, we test selected numbers of persons known to have and not have the medical condition in question. To get reliable data on both the detection rate (or ‘sensitivity’, here assumed to be 90%), and the false positive rate (here 1%, making the ‘specificity’ 99%), we might (artificially) test roughly equal numbers of persons with and without the condition (here 40% and 60% of the total tested, making the ‘prevalence’ 40%). The right hand side reverses the process, so that it mimics clinical practice, where the test result is known, but the presence/absence of the condition is not. When the test result is positive, say, the question then, naturally, is the probability that the patient with the positive result has the condition of concern. If the prevalence were truly 40%, then we would have the numbers shown in the right hand panel. Clearly, in this setting, \\[Prob[Disease \\ is \\ present \\ | \\ Test \\ is \\ Positive] \\ne Prob[Test is Positive | Disease is present]\\] Figure 11.6: A first example of two very different sets of probabilities: LEFT The probabilities that a potential diagnostic test would be positive in persons who, respectively, do and do not have the medical condition of concern. RIGHT The probabilities that a medical condition of concern is present in persons whose diagnostic test is, respectively, positive and negative. It is assumed that the overall prevalence of the condition is 40%, just like in the population mix (left) it was evaluated on. Prob[Test is positive given that the Condition is Present] = 80%, but Prob[Condition is Present given that the Test is positive] is 87%. In the following Figure, the detection rate and false positive rates are as before, but the prevalence of the condition being tested for is now only half as high, ie., 12.5%. Figure 11.7: A second example of two differing different sets of probabilities: the detection rate and false positive rates are as before, but the prevalence of the condition being tested for is now only half as high. Prob[Test is positive given that the Condition is Present] = 80%, but Prob[Condition is Present given that the Test is positive] is 74%. The 74% is called the ‘positive predictive value’ of the (positive) test. Sometimes the 74% is referred to as post-test (or more accurately the post-positive-test) probability. The positive predictive value is lower still if the prevalence of the condition being tested for is lower again. Example: The Etiologic Study This ‘hockey-epidemiology’ example is based on data from 51 goals allowed by Canadiens goalie Patrick Roy in NHL games palyed again the Boston Bruins, and on the headline of the La Presse article: ‘Pour battre Roy, mieux lancer bas’ [to beat Roy, it’s better to shoot low] Since we don’t have full data, the left side of this next panel is theoretical, and uses symbols for the unknown probabilities. We would like to know the difference between Prob[Goal | Shoot Low] and Prob[Goal | Shoot Low], but if worst comes to the worst, we would settle for the ratio Prob[Goal | Shoot Low] : Prob[Goal | Shoot High], The RIGHT hand side illustrates the modern conception of THE etiologic study, using a case series (in this example, the series is 51 goals) and a base series (100 random shots selected from the videotapes of the games in question: it would take too long to examine and classify all 100% of the shots as low/high). Figure 11.8: A first example of the difference between two very different sets of probabilities. The LEFT panel allows one to estimate (in the 2nd column) the exposure-specific rates (incidence densities) of developing lung cancer, The RIGHT panel begins from a case series of size n, and couples it with a base (denominator) series K times that size that estimates the distribution of smoking histories in (from a representative sample of) the base – the experience from which the cases arose. The exposure-specific numerators (numbers of cases) and estimated exposure-specific relative-denominators allow for the estimation of an incidence density ratio. The difference between ‘forward’ and ‘reverse’ probabilities distinguishes frequentist p-values (probabilities) from Bayesian posterior probabilities. \\[Probability[ data \\ | \\ Hypothesis ] \\neq Probability[ Hypothesis \\ | \\ data ]\\] or, if you prefer something that rhymes, \\[Probability[ \\ data \\ | \\ theta ] \\neq Probability[ \\ theta \\ | \\ data ].\\] Example: The Prosecutor’s Fallacy Here, in the context of criminal trials involving DNA evidence, are two striking – and worrysome – examples of misunderstandings about them. This misunderstanding is often referred to as the ‘Prosecutor’s Fallacy’. In the 1995 OJ Simpson trial - the subject of this recent tv series – the (very very small) probability that the blood of a randomly-selected innocent person would match that found at the crime scene was mistaken as (confused with) the probability that the person accused of murder was innocent. It is worth repeating an excerpt from JH’s bios601 notes, just cited. In this case the statistician Peter Donnelly opened a new area of debate. He remarked that . forensic evidence answers the question: What is the probability that the defendant’s DNA profile matches that of the crime sample, assuming that the defendant is innocent? . while the jury must try to answer the question . What is the probability that the defendant is innocent, [in the light of ALL of the OTHER EVIDENCE and] assuming that the DNA profiles of the defendant and the crime sample match? The OJ Simpson case also features a well-known Harvard law professor Alan Dershowitz– who was in the news again in the Trump impeachment case – whose insunuations and misuse of probabilites spurred the renowned and respected statistician IJ Good (who and worked with Alan Turing) to write this article in the journal Nature. It begins: SIR - Alan M. Dershowitz, who advises the defence lawyers in the 0. J. Simpson trial, stated on US television in early March that only about a tenth of 1% of batterers actually murder their wives. His statement, though presumably true, is highly misleading for the woman in the street. A probability of greater relevance for legal purposes would be based on the knowledge that the woman was both battered by her husband and also murdered by somebody. An approximate estimate of this probability will now be made, based on Dershowitz’s statement. Some statistical articles, having to do with lie detection tests, have also mixed up the probability of innocence given data with the (much easier to calculate) probability of data given innocence. Naive Measures of diagnostic/prognostic accuracy Many claims of high accuracy of diagnostic or screening tests rely on artificially-high rates of the target condition, and/or combine the ‘sensitivity’ and ‘specificity’ performance measures into one number. 11.6 Summary Slides Probablities can be viewed as long-run frequencies, or as degrees of belief. We often map probabilities into, and work with, other scales. The ‘calculus’ of probabilities relies on additions and subtractions, and multiplications. There is always a context for a probability. Trees can help to keep things (and directions!) straight. When things are more complex, simulations can help. It is easy to confuse contexts, and to calculate the wrong probability. \\[Probability[ data \\ | \\ Hypothesis ] \\neq Probability[ Hypothesis \\ | \\ data ]\\] \\[Probability[ \\ data \\ | \\ theta ] \\neq Probability[ \\ theta \\ | \\ data ].\\] Don’t fall into the ‘prosecutor’s fallacy’ trap, and mistake a p-value for the probability that the (null) hypothesis is true. Be suspicious of small (extreme) probabilities: they may well have been incorrectly calculated, or not all that revevant. 11.7 Exercises: Convert the proportion of male births to a sex ratio, and to the log of the sex ratio. If, because of natural year to year variations, the proportion varies from 0.49 to 0.53, what does the ratio vary from? the log of the ratio? What if proportion were 3/4. scales SDs See John Arbuthnot’s data. In a recent report, the focus was on the proportion of persons who would detect added (spiked) alcohol in a drink. The authors fitted a logistic S curve, and arrived at the equation \\[ logit[proportion \\ if \\ X \\ = \\ x] \\ = \\ x.x - y.y \\ Dose.\\]. From this, calculate the fitted proportion if X = zzz. The authors alos fitted a probit-based S curve, and arrived at the equation \\[ probit[proportion \\ if \\ X \\ = \\ x] \\ = \\ w.w - q.q \\ Dose.\\]. From this, calculate the fitted proportion if X = zzz. In an older report, the focus was on the proportion of persons who had reached menarche by a given age. The authors fitted a logistic S curve, and arrived at the equation \\[ logit[proportion \\ if \\ X \\ = \\ x] \\ = \\ x.x - y.y \\ Age.\\]. From this, calculate the fitted proportion if X = zzz. The authors also fitted a probit-based S curve, and arrived at the equation \\[ probit[proportion \\ if \\ X \\ = \\ x] \\ = \\ w.w - q.q \\ Age.\\]. From this, calculate the fitted proportion if X = zzz. Suppose you measure the heights of a randomly chosen adult male and a randomly chosen adult female, and record where each one is relative to the (a) Q\\(_{25}\\) (b) Q\\(_{50}\\) (median) and (c) Q\\(_{75}\\) value of their same-sex peers, and mark the result on a 3 x 3 grid (like the grid Galton used). For each of the 9 cells, what is the probability that they will be that cell? “Clustering” of Cardiovascular Risk Factors ? A Santé Quebec survey found the prevalence of 4 heart disease risk factors in a certain age-sex group to be: smoking: 32%; family history: 32%; SBP&gt;155mmHg: 12%; diabetes: 5%. If risk factors are distributed independently of each other, what is the proportion of the age-sex group with (a) 4 risk factors (b) 0 risk factors (c) 1 or more risk factors?. A tree diagram may help. Suppose that the probability that HIV will be passed from an infected person to an uninfected person during a single sexual contact is 0.01. Suppose that there are 50 such contacts. Show how to calculate (or obtain via software) the probability that HIV will be passed on in at least one of the 50 contacts. Refer to a (current) lifetable calculated from (made with) with recent mortality data, For example, Table 3.9 in this Quebec report. Why is the average (expected) age at death of 20 years old males 81.2 years while that for 30 years olds is 81.6? and 84.8 and 84.9 for females? Why did William Farr, in the Fifth Report of the Registrar General 1843 say that a strong case may no doubt be made out on behalf of those young, but early-dying Cornets, Curates, and Juvenile Barristers, whose mean age at death was under 30! It would be almost necessary to make them Generals, Bishops, and Judges — for the sake of their health. [The answer is in the page before] continued Why did the International Journal of Epidemiology report of these Danish investigators generate the headline ‘Soldyrkere lever meget længere’ – ‘Sun worshipers live much longer’ and the subtitle: ’New research among 4.4 million Danes shows that sun worshipers on average live six years longer? An auto insurance company notes whether drivers under 21 years old have had a driver’s education course. Some 40% of its policyholders under 21 have had a driver’s education course and 5% of this subset have an accident in a one-year period. Of those under 21 who have not had a driver’s education course, 10% have an accident within a one-year period. A 20-year-old takes out a policy with this company and within one year has an accident. What is the probability that the person did not have a driver’s education course? [a probability tree may help] 607 midtern 2001 Twins: [Real story from a real statistician] Depict Efron’s calculations using a probability tree. Here is his story&quot; Here is a real-life example I used to illustrate Bayesian virtues to the physicists. A physicist friend of mine and her husband found out, thanks to the miracle of sonograms, that they were going to have twin boys. One day at breakfast in the student union she suddenly asked me what was the probability that the twins would be identical rather than fraternal. This seemed like a tough question, especially at breakfast. Stalling for time, I asked if the doctor had given her any more information. ‘Yes’, she said, ‘he told me that the proportion of identical twins was one third’. This is the population proportion of course, and my friend wanted to know the probability that her twins would be identical. Bayes would have lived in vain if I didn’t answer my friend using Bayes’ rule. According to the doctor the prior odds ratio of identical to nonidentical is one-third to two-thirds, or one half. Because identical twins are always the same sex but fraternal twins are random, the likelihood ratio for seeing ‘both boys’ in the sonogram is a factor of two in favor of Identical. Bayes’ rule says to multiply the prior odds by the likelihood ratio to get the current odds: in this case 1/2 times 2 equals 1; in other words, equal odds on identical or nonidentical given the sonogram results. So I told my friend that her odds were 50-50 (wishing the answer had come out something else, like 63-37, to make me seem more clever.) Incidentally, the twins are a couple of years old now, and ‘couldn’t be more non-identical’ according to their mom. [Bradley Efron] Screening for HIV: Can we afford the false positive rate? Represent the information they use in their Meaning of Postive Tests section (starting on page 239, second column) as a tree. Then present the same information in a different tree, with data [test result] on left, and hypothesis on the right (rather than the conventional \\(\\theta \\rightarrow\\) data’ direction) . The Economist article Problems with scientific research: HOW SCIENCE GOES WRONG has a very nice graphical explanation of why some many studies get it wrong, and cannot be reproduced – the topic of the Reproducibility Project in Psychology referred to on same page. One reason is that even if all studies were published, regardless of whether the p-value was less than 0.05 (a common screening/filtering criterion) or greater than 0.05, of all the hypotheses tested, only a small percentage of the hypotheses are ‘true’. Thus many or most of the ‘positive’ tests (published results) will be false positives. It is just like when using mammography to screen for breast cancer: in maybe 4 of every 5 women referred for biopsy, the biopsy will come back negative. Represent the information in their Figure as a tree. Then present the same information in a different tree, with data on left, and hypothesis on the right (rather than the conventional \\(\\theta \\rightarrow\\) data’ direction) – as JH has done in a few instances above. What percentage of positive tests would be correct/not if, instead, 1 in 2 of the hypotheses interesting enough to test were true? Come up with a general formula for what in medicine is called the ‘positive predictive value’ of a positive medical test. Try to simplify it so that the characteristics of the test(\\(\\alpha \\ and \\ \\beta\\) are isolated in one factor, and the testing \\underline{context (the 1 in 10 or 1 in 2, etc.) is in another. Hint: use odds rather than probabilities, so that you are addressing the ratio of true positives to false positives, and the ratio of true hypothesis to false hypotheses. And use the Likelihood Ratio On the same Resources web page is another (but longer) attempt to explain these concepts graphically to left brain and right brain doctors. JH was impressed with this, and wanted to share it with the Court for Arbitration in Sport, when explaining the interpretation of positive doping tests. But he found that the ‘teaser’ sentence immediately following the title, ‘’Can you explain why a test with 95% sensitivity might identify only 1% of affected people in the general population?’’ is misleading, and so he make his own diagram (available on request). Exercise: Revise this misleading phrase. and see this shiny app. Wald : CF screening OJ p-value Adult soccer ball.. wrong probability Vietnam deaths NHL birthdays Medical School admission rates, by gender. Vietnam war deaths, by month of birth NHL success, by month of birth John Snow, cholera deaths South London, by source of drinking water "],
["randomVariables.html", "Chapter 12 Random Variables/Variation 12.1 Objectives 12.2 Random Variables 12.3 Expectation (mean) of a Random Variable 12.4 Expected value of a FUNCTION of a random variable 12.5 Variance (and thus, SD) of a random variable 12.6 Variance and SD of a FUNCTION of a random variable 12.7 Sums/means/differences of RVs 12.8 Linear combinations of RVs (regression slopes) 12.9 Exercises 12.10 Summary Slides", " Chapter 12 Random Variables/Variation 12.1 Objectives This central chapter addresses a fundamental concept, namely the variance of a random variable. It gives the laws governing the variance of a sum of 2, or (especially) \\(n\\) random variables – and even more importantly – the laws governing the variance of a difference of two random variables. The latter is central, not just to simple contrasts involving just 2 sample means or proportions, but also in the much wider world of regression, since the variance (sampling variability) of any regression slope can be viewed as the variance of a linear combination of random ‘errors’, or random deviations, or random variables. So, if there is one ‘master’ formula to pay attention to and to ‘own’, it is the one for the variance of a linear combination of random variables. All others are special cases of this. So, the specific objectives are to truly understand the concept of a random variable. the concept of the (expectation and) variance of a random variable. why it is that, when dealing with the sum of two or more independent random variables, it is not their standard deviations that sum (add), but rather their variances. likewise, when dealing with the difference of two independent random variables, or some linear combination of \\(n\\) independent random variables involving positive and negative weights, why it is that the component variances add, and with what weights. 12.2 Random Variables Textbook definitions of RANDOM VARIABLE (Note: JH has changed the usual \\(X\\) to \\(Y\\)) A variable (\\(Y\\)) whose value is a number determined by the outcome of an experiment A variable (\\(Y\\)) whose value is a numerical outcome of a random phenomenon Textbook definitions: DISCRETE Random Variable A random variable that assumes only a finite (or countably infinite) number of distinct values Discrete random variables have “a finite or countably infinite number of possible values, each with positive or zero probability. A discrete random variable \\(Y\\) has a finite number of possible values Random Variable: DISCRETE or CONTINUOUS ? How long you have to wait for bus / elevator / surgery/ download to complete the blood group of n = 1 randomly selected person how many tries before pass a course how many of n = 20 randomly selected persons will return questionnaire in pilot study length of song on a CD mean cholesterol level in sample of n = 30 randomly selected persons how hot it is going to be today how much snow we will get next winter time someone called (on answering machine) value of test-statistic if 2 populations sampled from have the same mean how much ice McDonalds puts in soft drink how many calories in hamburger how many numbers you get correct in 6/49? where roulette wheel stops how many “wrong number” calls received how many keys you have to try before get the right one how much water consumed by 100 homes PROBABILITY DISTRIBUTION \\(p(y)\\) associated with (Discrete) Random Variable \\(Y\\) The ordered pairs {y, Probability(\\(Y\\) = \\(y\\)) } , where \\(y\\) ranges over the possible values of \\(Y\\) Probability(\\(Y\\) = \\(y\\)) is often shortened to Prob(\\(Y\\) = \\(y\\)) or P(\\(Y\\) = \\(y\\)) Can display distribution as formula, table, graph, etc. EXAMPLES (Note: the only reason we start with lotteries is that we can give the exact, mathematically-known, probabilities of each possible result. In most of the other examples, the probabilities will be context-specific, and seldom knowable with full exactness.) Y = Winnings on a $1 wager on la Quotidienne 3 (same distribution for exact order and any order) ....... ($) ........ y .. Probability(Y = y) ........ 0 ............... 0.999 ...... 450 ............... 0.001 .......................... ===== ...... AL................. 1.000 Randomly select n = 1 person Y = the person’s Blood Group .... y Probability(Y = y) .... A .............. p(A) .... B .............. p(B) .... AB ............ p(AB) .... O .............. p(O) .................... ===== ....ALL ............ 1.000 In a 1911 census, of all households with at least 1 child aged 12 or under, the proportions of households that had 1, 2, … children. [We ignore for now any errors in recording ages] ## n.children proportion.of.households ## [1,] 1 0.335 ## [2,] 2 0.245 ## [3,] 3 0.180 ## [4,] 4 0.126 ## [5,] 5 0.072 ## [6,] 6 0.031 ## [7,] 7 0.009 ## [8,] 8 0.002 Proportions of (currently) married women aged 45 and older who, in a 1911 census, reported this “number of children born alive to the present marriage” ## n.children proportion.of.women ## [1,] 0 0.243878 ## [2,] 1 0.047704 ## [3,] 2 0.058265 ## [4,] 3 0.067908 ## [5,] 4 0.072704 ## [6,] 5 0.078010 ## [7,] 6 0.077653 ## [8,] 7 0.074133 ## [9,] 8 0.072245 ## [10,] 9 0.061939 ## [11,] 10 0.055357 ## [12,] 11 0.034031 ## [13,] 12 0.026071 ## [14,] 13 0.014031 ## [15,] 14 0.007959 ## [16,] 15 0.004337 ## [17,] 16 0.001837 ## [18,] 17 0.001020 ## [19,] 18 0.000561 ## [20,] 19 0.000306 ## [21,] 20 0.000051 Choose a random word and count how many characters (letters) it contains Y = Number of characters in word .... y . Probability(Y = y) .... 1 .............. p(1) .... 2 .............. p(2) .... etc. ........... etc. .................... ===== ... ALL ............ 1.000 Choose a random hospital admission and count how many days it lasted Y = Length of Stay (LOS) .... y ..... Probability(Y = y) .... 1 day ............... p(1) .... 2 days .............. p(2) .... etc. ................ etc. ......................... ===== .... ALL ................ 1.000 Note 1: The probabilities (or if you prefer to think of them as proportions) add to 1. We use probabilities or fractions as relative frequencies (like a histogram with an infinite number of entries) Note 2: typically, the random quantity is obtained from an aggregate of elements e.g. a sum, mean , proportion, regression slope MORE EXAMPLES OF PROBABILITY DISTRIBUTIONS (Some are on a continuous scale, but for convenience, the possible values are shown ‘binned’ into intervals) To pilot-test what the return rate of a mail survey is going to be, you mail the questionnaire to n = 20 randomly selected persons. Y = the number who will return questionnaire ...... y Probability(Y = y) ...... 0 ............. p(0) ...... 1 ............. p(1) ...... 2 ............. p(1) ....... .............. .... ..... 19 ............ p(19) ..... 20 ............ p(20) ..................... ===== ..... ALL ........... 1.000 People call out their birthdays: Y = when get 1st duplicate ...... y Probability(Y = y) ...... 2 ............. p(2) ...... 3 ............. p(3) ....... ................... .... 366 ........... p(366) .................... ===== ..... ALL .......... 1.000 Y = Winnings on a $1 wager on la Quotidienne 4 (same distribution for exact order and any order) ....... ($) ........ y .. Probability(Y = y) ........ 0 ............... 0.999 ..... 4500 ............... 0.001 .......................... ===== ..... ALL ................ 1.000 Y.bar = Mean cholesterol level in randomly selected sample of n = 30 persons .... y.bar .. Probability(y.bar would be...) .. &lt; 4.5 mmol/L ............. p( . ) .... 4.5 - 4.6 .............. p( . ) .... 4.6 - 4.7 .............. p( . ) .... 4.7 - 4.8 .............. p( . ) .... 4.9 - 4.9 .............. p( . ) .... 4.9 - 5.0 .............. p( . ) .... 5.0 - 5.1 .............. p( . ) .... 5.1 - 5.2 .............. p( . ) .... etc .... 5.6 - 5.7 .............. p( . ) .... 5.7 - 5.8 .............. p( . ) .... 5.8 - 5.9 .............. p( . ) .... 5.9 - 6.0 .............. p( . ) ........ &gt; 6.0 .............. p( . ) ............................. ===== ..... ALL ................... 1.000 Y = the value of the test- statistic if 2 populations sampled from had the same population mean .... test.statistic (Z) .. Probability(Z would be...) .. &lt; -2 ........................... 0.028 .... -2 to -1 ..................... 0.136 .... -1 to 0 ...................... 0.341 ..... 0 to +1 ..................... 0.341 .... +1 to +2 ..................... 0.341 .........&gt; +2 ..................... 0.028 ................................... ===== ..... ALL ......................... 1.000 In 6/49 lottery, player selects 6 distinct numbers on a grid showing the numbers 1 to 49. 49 otherwise identical balls, but numbered 1 to 49, are thoroughly mixed in an urn 6 balls are drawn without replacement. . Y = how many of the balls drawn show numbers that match the numbers selected by player ......... y .. Probability(Y = y) ......... 0 .......... 0.4359650 ......... 1 .......... 0.4130195 ......... 2 .......... 0.1323780 ......... 3 .......... 0.0176504 ......... 4 .......... 0.0009686 ......... 5 .......... 0.0000184 ......... 6 .......... 0.0000001 ...................... ========= ....... ALL .......... 1.0000000 http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/banco.pdf http://www.medicine.mcgill.ca/epidemiology/hanley/c323/lotteries/Keno(20-spot).pdf 12.3 Expectation (mean) of a Random Variable DEFINITION If \\(Y\\) takes on the DISCRETE values … \\(y_1\\) with probability \\(p_1\\), … \\(y_2\\) with probability \\(p_2\\), … \\(y_3\\) with probability \\(p_3\\), … etc … \\(y_k\\) with probability \\(p_k\\), then the expected value of \\(Y\\) (written ‘E(\\(Y\\))’ ) is \\[E[Y] = y_1 \\times p_1 + y_2 \\times p_2 + \\dots + y_k \\times p_k = \\sum y_i \\times p_i.\\] E(\\(Y\\)) is a mean that uses expected (i.e. unobservable or theoretical or long run) relative frequencies (\\(p\\)’s). ( \\(\\bar{y}\\) uses observed relative frequencies. ) Can think of E(\\(Y\\)) as ‘center of mass’ of \\(p().\\) If \\(Y\\) takes on the CONTINUOUS values \\(y\\) - \\(\\delta y/2\\) to \\(y\\) - \\(\\delta y/2\\) with probability \\(p = f(y) \\delta y\\), then \\[ E[Y] = \\int f(y) dy.\\] RELEVANCE of Expectation of a Random Variable IT ACTS AS A MEAN FOR A VARIABLE THAT HAS A (CONCEPTUAL) REPETITION OR AN INFINITE N THE EXPECTED VALUE OF A RANDOM VARIABLE Y WILL USUALLY BE IN TERMS OF POPULATION PARAMETERS. . A STATISTIC WITH EXPECTED VALUE \\(\\theta\\) IS AN ‘UNBIASED ESTIMATOR’ OF \\(\\theta\\). example 1: Y = Proportion of ‘YES’ in sample on \\(n\\) E(\\(Y\\)) = \\(\\pi\\) = PROPORTION of YES’ in POPULATION Then \\(\\hat{\\pi}\\) = \\(Y,\\) and Y is an unbiased estimator of \\(\\pi.\\) example 2: If we use a divisor of \\(n - 1\\) to calculate the sample variance \\(s^2 = \\frac{\\sum(y-\\bar{y})^2}{n-1},\\) then E(\\(s^2) = \\sigma^2\\), so \\(s^2\\) is an unbiased estimator of \\(\\sigma^2\\). ( \\(\\widehat{\\sigma^2}\\) stands for ‘estimate/estimator of’ \\(\\sigma^2.\\)) If we use a divisor of \\(n\\), then \\[E\\bigg(\\frac{\\sum(y-\\bar{y})^2}{n}\\bigg) = \\frac{n-1}{n} \\times \\sigma^2.\\] This estimator produces estiamtes that, on average, are too small. EXAMPLES of EXPECTED VALUE of Random Variable Y = Winnings on a $1 wager on la Quotidienne 3 ..... y($) ... p(y) ...... y x p(y) ........ 0 .. 0.999 ....... $0.00 ...... 450 ...0.001 ....... $0.45 .......................... ===== ...... ..............SUM .. $0.45 Some may find it easier to think of averaging the winnings of 1 person who won 450 dollars and 999 persons who won 0 dollars. Keno: Y = Winnings on a $3 wager . E(Winnings) = $2.12 (&gt; 70%) see here. Banco: Winnings on a $1 wager . 42% ≤ E(Winnings) ≤ 55%, depending on how many numbers played see here. Longevity of a fictitious birth cohort if they were to experience age-specific death rates observed in Quebec in 1990 [see exercises in 3rd computing session] Y = Length of life = age at death. Y = Age at death (or Longevity or length of Life, or lifetime, if you prefer to be positive) . . E(Y) = E(Lifetime) = ‘LIFE EXPECTANCY AT BIRTH’ . Assume for sake of illustration that deaths in a decade are all at midpoint of interval (calculations done using one-year-wide age-bins rather than one-decade-wide would be more exact) age = mid-point of decade (for simplicity) p = proportion dying in decade ....................MALES .............. FEMALES decade .... age ..... p .... age x p ...... p ...... age x p . 00-10 ...... 05 .... 0.010 ... 0.050 ..... 0.080 ..... 0.040 10-20 ...... 15 .... 0.006 ... 0.089 ..... 0.002 ..... 0.030 20-30 ...... 25 .... 0.012 ... 0.295 ..... 0.004 ......y.yyy 30-40 ...... 35 .... 0.016 ... 0.544 ..... 0.007 ......y.yyy 40-50 ...... 45 .... 0.030 ... 1.335 ..... 0.017 ......y.yyy 50-60 ...... 55 .... 0.074 ... 4.079 ..... 0.040 ......y.yyy 60-70 ...... 65 .... 0.180 .. 11.697 ..... 0.096 ......y.yyy 70-80 ...... 75 .... 0.301 .. 22.610 ..... 0.214 ......y.yyy 80-90 .. ... 85 .... 0.279 .. 23.680 ..... 0.358 ......y.yyy 90-100 ..... 95 .... 0.093 ... 8.822 ..... 0.254 .....24.136 .................... =.=== .. ==.=== ..... =.=== .... ==.=== .ALL ............... 1.000 .. 73.2 ....... 1.000 .... xx.x Mean no. of children 12 years and under in households with at least 1 such child: ## n.children proportion.of.households product ## [1,] 1 0.335 0.335 ## [2,] 2 0.245 0.490 ## [3,] 3 0.180 0.540 ## [4,] 4 0.126 0.504 ## [5,] 5 0.072 0.360 ## [6,] 6 0.031 0.186 ## [7,] 7 0.009 0.063 ## [8,] 8 0.002 0.016 ## . Sum(products) = Expected Value = Mean = 2.5 If you want to start your own Insurance Company Y = Payout (from -99,750 to +$1250) E(Payout on single policy) &gt; 0 BUT… Variance(Payout) is VERY LARGE see MM3 p 341 Distance Where to wait if 3 unequally spaced elevators ? prob(it’s #1) = p(it’s #2)=p(it’s #3) = 1/3 12.4 Expected value of a FUNCTION of a random variable If \\(Y \\ \\sim \\ p(y)\\), i.e., \\(Y\\) has probability distribution p(\\(y\\)), and if \\(g(Y)\\) is some real-valued function of \\(Y\\), then \\[E[ g(Y) ] = \\sum g(y) \\times p(y),\\] i.e. it is a weighted mean of the \\(g(y)\\)’s, with \\(p(y)\\)’s as weights. NOTE In some instances, the expectation of \\(g(Y)\\) is \\(g(E[Y])\\), while in others it is more complex. Can you figure out when it is/is not in each of the following instances? Examples \\(Y\\) = Noon Temperature (C) in Montreal on a randomly selected day of the year; \\(g(Y)\\) = Temperature (F) = 32 + (9/5) \\(Y\\) \\(Y\\) = Weight in Kg (or Height in cm) of a randomly selected person; g(Y) = Weight in Kg (or Height in inches) \\(Y_1\\) and \\(Y_2\\) are two random variables that might or might not be related; . if \\(g(Y_1, Y_2) = Y_1 + Y_2,\\) then \\(E[g(Y_1, Y_2)] = E[Y_1] + E[Y_2].\\) . if \\(g(Y_1, Y_2) = \\frac{Y_1 + Y_2}{2},\\) then \\(E[g(Y_1, Y_2)] = \\frac{E[Y_1] + E[Y_2]}{2},\\) . and, by analogy, for a sum or mean of \\(n\\) related or unrelated random variables. \\(Y\\) = diameter of a randomly chosen sphere; \\(g(Y)\\) = Volume of sphere = \\(\\frac{\\pi}{6} Y^3.\\) \\(Y\\) = fuel consumption, in liters/100km, of a randomly selected make of car; \\(g(Y)\\) = miles per gallon or Km per liter (reciprocal) \\(Y\\) = which of 3 unequally spaced elevators shows up next. Prob(it is #1) = prop(it is #2) = prob(it’s #3) = 1/3. . \\(g(Y)\\) = Distance to elevator. How to mimimize E(distance)? . \\(g(Y)\\) = Squared Distance to elevator. How to mimimize E(\\(g(Y)\\))? Random Variable Y with Expectation or Mean \\(\\mu\\) . \\(g(Y) = (Y - \\mu)^2\\), the squared deviation from the mean The following is in bold to emphasize one of the most fundamental concepts in statistics, namely VARIANCE. 12.5 Variance (and thus, SD) of a random variable 12.5.1 Definitions E[\\((Y - \\mu)^2\\)] is called the VARIANCE of the random variable \\(Y\\). It is usually shortened to Var(\\(Y\\)) or even to V(\\(Y\\)). It, and its positive square root, called the standard deviation of \\(Y\\), or SD(\\(Y\\)), are two of the most commonly used measures of variability or spread or uncertainty. Computationally, Variance(\\(Y\\)) = E[ \\((Y - \\mu)^2\\) ] = \\(\\sum(y - \\mu)^2 \\times f(y),\\) or Mean Squared Deviation, and Standard Deviation, SD(\\(Y) = \\sqrt{(Var(Y)}= \\sqrt{E[ (Y — \\mu)^2]},\\) or Root Mean Squared Deviation In French, the Standard Deviation is called écart type. This French-&gt;English dictionary translates (the noun) écart as space, gap, distance between objects, interval, gap between dates, difference between numbers, opinions; à l’~ isolated, remote, out-of-the-way; à l’~ de well away from; ~ de conduite misdemeanour; ~ d’inflation inflation differential; ~ de langage strong language, bad language; ~ type standard deviation and (the adjective) type translates as typical, standard; lettre ~ standard letter; (Maths) écart ~ standard deviation. JH thinks this adjective better describes the meaning that ‘standard’ does. See here for the history of the term standard deviation. Here, graphically and numerically illustrated, are three (of the many) ways to measure the variability of a random variable. Figure 12.1: 6 symmetrically distributed random variables, and 3 ways of measuring their spreads about a common mean. [After Mosteller, Rourke and Thomas. Probability with statistical applications 2nd Ed, p205] In practice, the mean absolute deviation is often quite close to the SD, and certainly easier to explain to explain to non-statisticians. However, when computig was by ahnd and laborious, it took two passes through the data to compute it, whereas, the SD could be computed in one. Variance &amp; SD of number of children &lt;= 12 years in households with at least 1 such child: ## n.children devn. devn.squared proportion.of.households product ## [1,] 1 -1.494 2.232036 0.335 0.74773206 ## [2,] 2 -0.494 0.244036 0.245 0.05978882 ## [3,] 3 0.506 0.256036 0.180 0.04608648 ## [4,] 4 1.506 2.268036 0.126 0.28577254 ## [5,] 5 2.506 6.280036 0.072 0.45216259 ## [6,] 6 3.506 12.292036 0.031 0.38105312 ## [7,] 7 4.506 20.304036 0.009 0.18273632 ## [8,] 8 5.506 30.316036 0.002 0.06063207 ## ....... Sum(products) = Mean of Squared Deviations = Variance = 2.22 ## . Sq. Root of Mean of Squared Deviations = Standard Deviation = 1.5 Which is primary, Standard Deviation or Variance? Although we first define variance and then take the square root to reach the SD, we should think of the SD as primary, at least for descriptive purposes (Mosteller et al. use the natural order “…standard deviation and variance (either of these measures determines the other because the variance is the square of the SD”… However, there are good mathematical reasons to work with variance. 12.5.2 Some (good) reasons for using variance, which averages the squares of the deviations from the mean. ADDITIVITY The variance of the sum of two independent random variables is the sum of their variances, and even when the two variables are dependent the variability of their sum has a simple formula. SD;s dont add; their squares do. Or to quote the physicists, errors ‘Errors add in quadrature, like the lengths of the sides of Pythogoras’ traingle. It took mathematicains a long time to discover, this, and some of the blunders along the way are told in a fascinating chapter in this very readable book The Seven Pillars of Statistical Wisdom. THE CENTRAL LIMIT THEOREM The limiting behavior of a random variable that is the sum of a large number of independent random variables depends on the variances of these random variables. USEFUL RELATIONSHIP/SHORTCUT (especially for hand computation, ‘back when’, and in mathematical statistics, still today): \\[Variance(Y) = E(Y^2) - \\mu^2.\\] \\[\\textrm{Variance = average square minus squared average}.\\] 12.5.3 But, for end-users today …. Jerry Hill, who wrote this PhD thesis ‘at the end of a career in medicine and epidemiology’ commuted from Ottawa and taught the 607 course a few times in the 1980s. In the course, he used to joke about the (many) mathematical statisticians who refer to a random variable having a Normal (Gaussian) distribution with standard deviation \\(\\sigma\\) as \\(Y \\sim N(\\mu, \\sigma^2)\\), i.e., who defined the spread of the distribution (any distribution, not just Gaussian ones) using its variance. [Note that the dnorm, pnorm qnorm and rnorm functions in R all use the argument sd, as do all applied textbooks, writing \\(Y \\sim N(\\mu, \\sigma)\\)]. Gerry would then go on to give a numerical example, using the commonly used metric the Total Fertility Rate (TFR) – or often simply ‘fertility rate’ – which (as per Goggle) `measures the average number of children per woman.‘The global average fertility rate is just below 2.5 children per woman today’. He would them say, ‘Suppose the variation from country to country had a standard deviation of 1.2’. Then, reminding his class to use appropriate units when reporting statistical measures, he would ask that they give the appropriate units for the 2.5, the 1.2, and the \\(1.2^2\\) = 1.44, forcing them to say the variance was 1.44 square children per square woman. 12.5.4 Example of Variance-calculation using one-pass formula Number of children born alive Proportions of (currently) married women aged 45 and older who, in a 1911 census, reported this “number of children born alive to the present marriage” ## n.children squared.children proportion.of.women product ## [1,] 0 0 0.243878 0.000000 ## [2,] 1 1 0.047704 0.047704 ## [3,] 2 4 0.058265 0.233060 ## n.children squared.children proportion.of.women product ## [1,] 18 324 0.000561 0.181764 ## [2,] 19 361 0.000306 0.110466 ## [3,] 20 400 0.000051 0.020400 ## Sum(product) = Mean Squared Value = 39.47 ; Mean Value = 4.83 ## Variance = 39.47 minus the square of 4.83 = 16.11 ## Thus, Standard Deviation = sqrt( 16.11 ) = 4.01 12.6 Variance and SD of a FUNCTION of a random variable If we go back to some of the examples listed above we can reason out what the law must be \\(Y\\) = Noon Temperature (C) in Mtl on a randomly selected day of the year; … \\(g(Y)\\) = Temperature (F) = 32 + (9/5) \\(Y\\) If the SD was say 10 C, then surely the SD is 18 C. After all, Temperature is Temperature, so you are not – as some textbooks are inclined to say – ‘changing’ the fundamental variable, but rather changing the scale of the temperature variable. So, the SD scales up by 9/5, and so, being an average square, the variance scales up by \\((9/5)^2\\). If you are going the other way, from the larger F scale to the smaller C scale, the scalings are (5/9) for the SD, and \\((5/9)^2\\) for the variance. More generally, \\[SD[constant \\times RV] = constant \\times SD[RV]\\] \\[Var[constant \\times RV] = constant^2 \\times Var[RV]\\] . This example also shows another law related to spread: shifting left or by a constant amount (eg. suppose the conversion was \\(F = 10 + (9/5)C\\) instead of was \\(F = 32 + (9/5)C,\\) it would not alter the spread. Thus, . \\[SD[ RV + constant ] = SD[RV]\\] \\[Var[ RV + constant ] = Var[RV]\\] \\(Y\\) = Weight in Kg (or Height in cm) of a randomly selected person; … g(Y) = Weight in Kg (or Height in inches) . This involves just a scaling, with no shift. So if the SD were 10 Kg, it would be 22 lbs, and the variances in the 2 scales would be \\(100 \\ Kg^2\\) and \\(484 \\ lb^2\\). \\(Y\\) = Years of publication of all the books in the McGill Library, with Years measured from AD (Anno Dominini, ‘in the year of the Lord’). . The SD would be the same if we measured the Year from 1439 AD (\\(Y&#39; = Y\\) - 1439) when Gutenberg was the first European to use movable type, or from 1492 AD when Christopher Columbus reached North America. . What if, instead, we calculated the age of each book in the year 2020, i.e. as \\(Y&#39; = 2020 - Y\\)? . The scale would now be reversed. Instead of being at the extreme left, the older books would not be at the right hand of the scale, and vice versa. But the spread would still be the same, even though the shape of the new distribution would be the miorrow inage of the old one. . What if we measured age in decades, i.e., \\(Y&#39; = \\frac{2020 - Y}{10}\\) or centuries i.e., \\(Y&#39; = \\frac{2020 - Y}{100}\\)? . The SDs would be scaled down by 10 and by 100, and the variances by \\(10^2\\) and \\(100^2.\\) \\(Y\\) = Ocean Depth ot a randomly chosen location, measured in metres. If the ‘origin’ is the ocean floor, all depths will be positive; if it is the surface of the ocean, they will all be negative. The spread, the SD and the Variance stay the same, but the shapes of the distributions are mirror inages of each other. \\(Y\\) = diameter of a randomly chosen sphere; … \\(g(Y)\\) = Volume of sphere = \\(\\frac{\\pi}{6} Y^3.\\) . This one is more complicated – as you might have guessed from just trying to compute the expectation. The fact that the scaling is different at different values of \\(Y\\) complicated matters. There is an appriximate formula, that depends on the scaling at some ‘representative’ value of \\(Y\\), typically the mean or mode. For more on this, see the examples in this expository piece. \\(Y\\) = fuel consumption, in liters/100km, of a randomly selected make of car; \\(g(Y)\\) = miles per gallon or Km per liter (reciprocal) . There is no exact closed form, but the approximation works well because the values are well away from zero, and not too spread out – unless you allow Hummers! 12.7 Sums/means/differences of RVs 12.7.1 A sum (of 2 or \\(n\\)) To keep it simple, and to allow us to see what is going on, Let’s consider two very simple random variables (RV’s), each taking just 2 values, and with equal probabilities. [You can check for yourself later that the same law applies to random variables that take on more than 2 values, and with uneven probability distributions.] The key is that the 2 variables are independent of each other. The next panels show the two RV’s. \\(RV_1\\)  (6  or  12)  and  \\(RV_2\\)  (8  or  16) \\[Sum = RV_1 \\ (6 \\ or \\ 12) \\ + \\ RV_2 \\ (8 \\ or \\ 16) \\ = \\ 14 \\ or \\ 20 \\ or \\ 22 \\ or \\ 28.\\] We will come back much later to the choices of the specific values for each of the random variables; for our purposes here, the main point is that both equally-likely values are an even number apart, so the SDs are integers, and all calculations involve integers. Note that an RV with equally likely values 1 and 7 (or 3 and 9, or -1 and + 5) has the same SD as the RV with equally likely values 6 and 12: it is the distance between them that determines the SD. Note also, that with half of the values at one extreme and hald at teh other, all values are exactly one SD from the mean. [If you want to give them some meaning so that you can relate to them, thing of them as the durations of 2 stages in an industrial process, or a service (such as processing online orders, and arrranging shipping and delivery, or waiting for a mammogram, and then a biopsy).] Figure 12.2: Variance of RV1 Figure 12.3: Variance of RV2 We now imagine taking a random value of \\(RV_1\\) and a random value of \\(RV_2\\) and summing them. There are 4 possible sums, and in this case they are all distinct (this isn’y always be the case, but the values of the two RV’s in this example were deliberately chosen to make them all distict, and to avoid grouping RV combinations with the same sum.) A probability tree (next panel) helps to see the 4 possibilities, but here we add an extra feature: we let the lengths of the branches denote the values of the RVs. (This ‘stacking’ of RV’s can have practical advantages too: when Francis Galton wanted to quickly and precisely compute the mean diameter of a large sample of peas, he lined them up in a long groove, and just measured the total width of the entire sample, then divided it by the \\(n\\).) In our example, the 4 equally likely sums are 14, 20, 22 and 28, and their mean is \\(\\frac{14+20+22+28}{4} = 21.\\) That the mean (expected value) of the sum equals the sum of the 2 individual means or expected values, is hardly surprising [ – and it is even true if the 2 RV’s were not independent.] The 4 equally likely deviations from this 21 are -7, -1, +1, and +7, and so, from 1st principles, the Variance of the sum of the 2 RVs is \\[\\frac{(-7)^2 + (-1)2 + (+1)^2 + (+7)^2}{4} = \\frac{100}{4} = 25.\\] Thus, the SD of the sum of the 2 RVs is \\(\\sqrt{25}\\) = 5. Fortunately, we don’t have to go back to 1st principles to calculate the SD of the sum of 2 RVs – but we do NOT do so by adding the 2 SDs. IT IS THE VARIANCES THAT ADD! Figure 12.4: Variance (and thus SD) of the SUM of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the sum from its expectation is decomposed into its 2 components. This, (by the expansion rule for (a+b) squared that you learned in high school), each squared deviation becomes a sum of 2 squares, and a ‘cross-product’. But the 4 cross-products cancel each other, and you are left with the sums of two squares, the original 3-squared and the original 4-squared, i.e. the variances of RV1 and RV2. Whereas one numerical example doesn’t prove the ‘the variances of a sum of independent RVs is the sum of their variances’ rule, you can check out other more complicated examples with more values, and uneven distributions – or use simulation – and satisfy yourself that it is a general rule. Indeed, the formal mathematical statistics ‘proof’ uses the exact same method as that used in the panel, except that it replaces each number by a symbol! (By the way, when students from other disciplines who claim to have taken statistics courses ask JH for permission to enrol in the math-stat based course bios601, one of his standard tests is to give then this problem: ‘Let RV1 and RV2 be two random variables. From 1st principles, derive the formula for the variance of RV1+RV2’.) So we can sum up the forgoing numerical example by saying: \\[ SD_1 = 3; SD_2 =4;\\ but \\ SD[Sum] \\ \\ne \\ 3 + 4. \\ \\ \\ SD[Sum] \\ = \\ \\sqrt{3^2 + 4^2} = 5.\\] BOTTOM LINE : 3 + 4 is not 7! it’s 5! Or, as physicists say, Uncertainties ADD IN QUADRATURE’ (that’s the math word for the square root of the sum of squares). Now you can see why mathematical statisticians like to work with squared SD’s. And you can see why we chose the ‘nice’ variance values 3 and 4. Just like for the right-angled triangles in Pythagoras’ theorem, where the length of the hypotenuese is the square root of the squares of the elngths of the sides, so it is also with othogonal or independent random variables: the SD of their sum is the square root of the sum of the squared SD’s of the individual RVs. And of course, since the theorem works for the sum of 2 independent RV’s, it also works for the sum of 3, and for the sum of \\(n\\) such RVs. 12.7.2 Measurement Errors These are an important issue in the non-experimental – and even the experimental – sciences. The simplest case (called the ‘classical’ error model) is where the errors are independent of the true value, so that the variance in the observed (error-containing) values is the sum of the variance of the ‘true’ (errorless) values, and the variance of the errors. Even though many people think that random measurement errors cancel out, especially in large datasets, they do not. Even when they affect the \\(Y\\) on the left side side of a regression model, they add ‘noise’ to the slope estimates. But when they affect an \\(X\\) on the right hand side of a regression model, or even a correlation, their effects are more insidious. See for example, pages 19-21 of these Notes and exercises 6, 8, 9, 18 and 21 that follow. The other measurement error model, called ‘Berkson’ error, described here:, is less common, and has less nasty effects. The following diagram shows the classical error model, and one of the important metrics to measure the extent of the error, namely the intra-class correlation coefficient. The ‘ICC’ is relevant no matter whether the variable is on the left or right had side of the regression model. Even though we named the random variable \\(Y\\), it does not mean that measurement issues apply only to \\(Y\\) variables. In fact, errors in \\(X\\) variables have nastier effects. We chose the name \\(Y\\) because we don’t treat an errorless \\(X\\) in a regression model as a random variable, and we are seldom interested in inferences regarding it! Figure 12.5: Random Measurement Error (E) added to a Random Variable. On the left is the distribution of a Random Variable Y, with each stickfigure representing a very large number of individuals. On the right is the distribution of the Random Variable Y’, where Y’ = Y + E, and E is independent of Y, and has a 2-point distribution, namely -0.5 and +0.5, with equal probabilities. (Here, the provenance/origin of each Y’ value is shown by its colour, but in practice we would not have that luxury of knowing what the ‘true’ [errorless] value was). This is the variation we get to observe/measure. Of the variance of 1.93, some 1.68 of it is ‘real’/‘genuine’, and the remainder, 0.25 is measurement error. The genuine variance of interest, 1.68, expressed as a proportion of the observable variance 1.93, namely 1.68/(1.68 + 0.25) = 0.87 or 87%. The proportion that is real is called the INTRA-CLASS CORRELATION (ICC) or intra-class correlation coefficient, and is an important indicator of the quality of the measurement of Y. The concept of an ICC depends on the law that ‘variances add.’ 12.7.3 Mean (of 2 or \\(n\\) RVs) AND, if we know how to compute the SD of a SUM of \\(n\\) independent RV’s, we then automatically know how to compute the SD of a MEAN of \\(n\\) independent RV’s. Remember back to an assigment on the SD of a set of temperatures measured in the larger degrees F scale: then the SD of the same set of temperatures measured in the smaller degrees C scale is just 5/9ths of the one in the F scale. Going from a sum of \\(n\\) RVs to a mean of \\(n\\) RVs involves going to a scale that is (1/n)-th the spread of the sum scale. So, in the above example, with \\(n\\) = 2, the SD of the mean of the 2 RVs is (1/2) the SD of the sum, i.e., \\[ SD\\bigg(\\frac{RV_1 +RV_2}{2}\\bigg) = \\frac{SD(RV_1+RV_2)}{2} = \\frac{5}{2}.\\] SPECIAL CASE (quite common) where SDs are identical: Up to now, to keep things general, we used \\(n\\) non-identical – but independent – random variables. If we consider the Variance and the sum of \\(n\\) IDENTICAL – and independent – random variables, so the the \\(n\\) Variances (each abbreviated to Var) are all equal, the laws simplify First, since the variances add, we have that \\[ Variance(RV_1 + RV_2 + \\dots + RV_n) = Var_1 + Var_2 + \\dots + Var_n = n \\times \\ each \\ Var.\\] and so, taking square roots, \\[ SD( \\ RV_1 + RV_2 + \\dots + RV_n \\ ) = \\sqrt{ \\ n \\times \\ each \\ Var} = \\sqrt{n} \\ \\times \\ each \\ SD\\] When we go from sum of \\(n\\) IDENTICAL independent RVs to a mean of \\(n\\) IDENTICAL RVs, we go to a scale that is (1/n)-th the spread of the sum scale. So, again, just as when we went from the larger degrees F scale to the smaller degrees C scale, we have \\[ SD\\bigg(\\frac{RV_1 + RV_2 + \\dots + RV_n}{n}\\bigg) = \\frac{\\sqrt{n} \\ \\times \\ each \\ SD}{n} = \\frac{common \\ SD}{\\sqrt{n}} .\\] Sometimes, we will need to work with variances. When we do, we use this law: *\\[ VAR\\bigg(\\frac{RV_1 + RV_2 + \\dots + RV_n}{n}\\bigg) = \\frac{common \\ VAR}{n} .\\] EXAMPLE Lengths of words in a book (“book A”) The number of dashes in of each row in the first panel is the number of letters in a randomly selected word from the book: dashes are for better visibility. The words (rows) are sorted by length, form shortest to longest. One cannot judge the full distribution just from this limited set, but (even though shape doesn’t matter much in the big scheme of things) you get a sense of its shape. In the entire book, the mean word length is about 4.5 letters, and the SD is 2.4 letters. (The fact that the distance between the minimum word length (1 letter) and the mean word length is less than 2 SDs hints that the full distribution has a longe right tail.) In each row in the coloured panels, some 4 (or 9) randomly sampled words are (like Galton’s peas) pushed right up against each other, without spaces, and shown in a mono-spaced font, so that where the ‘line’ ends indicates the total number of letters in the 4 (9) words. Since this small number of rows (possibilies) is too small to give a good sense of the sampling distribution, the smooth purple histograms were calculated exactly. Figure 12.6: Illustrations of SD’s of Sums and Means of n = 1, 4 and 9 independent and identically distributed random variables. Each RV is the length of a randomly selected word from a certain book. [Below, we will compare the mean word length in this book with the mean in abook by a competitor]. The distributions in purple were computed theoretically, using convolutions. Each row shows 1 ‘realization’ of each of the n random variables, with each word in a different color. The rows are sorted according to the values of the total [or mean] numbers of letters (chars) in the sample of n words. In the panels where n is 4 or 9, the leftmost n-1 characters of the n concatenated words are cropped, but the total/mean length is correct. The top panel lists the ‘per word’ variation of all of the words in the book, and its SD, sometimes called the ‘unit’ variability. You can also think of the length of each unit as the mean of a sample of size n = 1. The second panel shows that to reduce the sampling variation (the SD) of the mean by half, one needs to quadruple the n. The third panel shows that to reduce the sampling variation (the SD) to 1/3, one needs to multiply the n by 9. Note, in passing, that at \\(n\\) = 9, via the Central Limit Theorem, and the fact that the original distribution is ‘CLT friendly’ (the mode is not at either extreme, and the tails don’t extend indefinitely), the shape of the sampling distribution is already close to Gaussian. As you would have expected, the purple sampling distributions narrow with increasing sample size, but the narrowing is not by a factor of \\(n\\), but by a factor of \\(\\sqrt{n}\\). All the billions of possible means of samples of size \\(n\\) = 4 would have a SD of 2.4/\\(\\sqrt{4}\\) = 2.4/2 = 1.2. The possible means of samples of size \\(n\\) = 9 would have a SD of 2.4/\\(\\sqrt{9}\\) = 2.4/3 = 0.8. Note the careful choice of the words in italics: in reality, you will only observe one of the billions of possibilities, so the sampling distribution is imaginary and thus the SD is also imaginary and so the SD of the conceptual sampling distribution is (would be) an imaginary 1.2 or 0.8. The only reason we are able to show the purple distributions is because of the laws of statistics, applied to all the words in the book, so we know the mean and the unit SD. 12.7.4 Difference of 2 RVs To keep it simple, let’s consider the two very simple random variables (RV’s), each taking just 2 values, and with equal probabilities, and independent of each other. But suppose now that we are interested in their difference \\[Difference = RV_1 \\ (6 \\ or \\ 12) \\ - \\ RV_2 \\ (8 \\ or \\ 16) \\ = \\ -10 \\ or \\ -4 \\ or \\ -2 \\ or \\ +4.\\] Now, imagine taking a random value of \\(RV_1\\) and subtracting from it a random value of \\(RV_2\\). There are 4 possible differences, and in this deliberately constricted example, they are all distinct. A probability tree (next panel) helps to see the 4 possibilities, and the lengths of the branches denote the values of the RVs. The 4 equally likely differences are -10, -4, -2 and +4, and so their mean is \\(\\frac{-10 -4 -2 +4}{4} = -3.\\) That the mean (expected value) of the difference equals the difference of the 2 individual means or expected values, is hardly surprising [ – and it is even true if the 2 RV’s were not independent.] The 4 equally likely deviations from this -3 are -7, -1, +1, and +7, and so, from 1st principles, the Variance of the difference of the 2 RVs is \\[\\frac{(-7)^2 + (-1)2 + (+1)^2 + (+7)^2}{4} = \\frac{100}{4} = 25.\\] Thus, the SD of the difference of the 2 RVs is \\(\\sqrt{25}\\) = 5. Fortunately, we don’t have to go back to 1st principles to calculate the SD of the differences of 2 RVs – but we do NOT do so by adding the 2 SDs. IT IS THE VARIANCES THAT ADD! Figure 12.7: Variance (and thus SD) of the DIFFERENCE, RV1 - RV2, of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the difference from its expectation is decomposed into its 2 components. Each each squared deviation becomes a sum of 2 squares, and a ‘cross-product’. But the 4 cross-products cancel each other, and you are left with the SUMS of two squares, the original 3-squared and the original 4-squared, i.e. the variances of RV1 and RV2. Again one numerical example doesn’t prove the ‘the variances of a difference of two independent RVs is the sum of their variances’ rule, but you can check out other more complicated examples with more values, and uneven distributions – or use simulation – and satisfy yourself that it is a general rule. So we can sum up the forgoing numerical example by saying: \\[ SD_1 = 3; SD_2 =4;\\ but \\ SD[Difference] \\ \\ne \\ 3 + 4. \\ \\ \\ SD[Difference] \\ = \\ \\sqrt{3^2 + 4^2} = 5.\\] It turns out that, from what we already knew about the sum of 2 independent RVs, we have anticipated this law. We didn’t need to go through all the formulae from scratch again. The reason had to do with the ‘mirror image’ distributions, such as the depths of the ocean, we saw above. The spread (SD, or variance) is the same, whether one writes/reads/computes from left to right, or right to left! In other words, the variance of the random variable \\(-RV_2\\) is the same as that of the random variable \\(RV_2.\\) So, by writing \\(RV_1 - RV_2\\) as a sum, and using the law for the variance of a sum, we arrive at \\[Var[RV_1 - RV_2] = Var[RV_1 + (-RV_2)] = Var[RV_1] + Var[(-RV_2)] = Var[RV_1] + Var[RV_2].\\] EXAMPLE Difference in mean length of words in books A (in blue) and B (in red) Figure 12.8: Differences in mean lengths of n randomly selected words from each of 2 books. The (sampling) distributions were computed theoretically, using convolutions. 12.8 Linear combinations of RVs (regression slopes) In non-experimental research especially, the focus is typically on a fitted regression slope/coefficient, rather that on the simple difference \\(\\bar{y}_1\\) - \\(\\bar{y}_0\\) between the means of the \\(y\\)s observed at each of two investigator-chosen values (\\(X=1\\) and \\(X=0\\)) of the determinant (\\(X\\)) being studied. Even if the estimator does not have a closed form, the fitted slope(s)/coefficient(s) are linear combinations of the \\(y\\)’s and the \\(x\\)’s. Thus, since each of the \\(n\\) \\(y\\)’s contains a random element, the slope (\\(\\hat{\\beta}\\)) is an \\(x\\)-based linear combination of \\(n\\) random variables. Thus one can view all variances (and thus all standard errors) in a unified way, and not have to learn separate laws for separate chapters. To see how this unified view avoids the typical ‘silo’ approach to statistical tecnniques, see Sample Size, Precision and Power Calculations: A Unified Approach. [Software developers who thrive on separate ‘niche’ markets are threatened by this parsimonious approach, just as are those who write 800-page textbooks with separate chapters for t-tests,l proportions, regression, multiple regression, logiostic regression, Cox regression, survival analysis,etc.] In the past, when first introduced to simple linear regression, it was common to learn the estimator formula and the Variance formula by heart, and use them to compute the fitted slope and the the standard error for a fitted slope by hand, \\[\\hat{\\beta} = \\frac{\\sum (y-\\bar{y})(x-\\bar{x})}{ \\sum (x-\\bar{x})^2} \\ ; \\ \\ Var[ \\hat{\\beta} ] = \\frac{\\sigma_e^2}{ \\sum (x-\\bar{x})^2} \\ ; \\ SE[ \\hat{\\beta} ] \\ = \\sqrt{Var[ \\hat{\\beta} ]} \\ .\\] In the variance formula, \\(\\sigma_e^2\\) is the variance in the ‘errors’ in the \\(y\\)’s. In practice, we have to estimate this quantirty, but in our example, for disdactic purposes, we will pretend to ‘know’ its value. Sadly, these formidable formulaa hide what is going on. To truly understand what is going on, lets consider a very simple example where we can see what is happening. A student from a country that uses the Fahrenheit (F) system moves to Montreal, and wishes to know how to translate the outside temperature, expressed as the number of degrees Celsius (C) and shown in the Metro, and heard on radio stations, back into the F scale (s)he is familiar with. The student knows that the conversion is of the form F = \\(\\alpha\\) + \\(\\beta \\times C\\) but, rather than look them up on Google, decides to estimate \\(\\alpha\\) and \\(\\beta\\) from pairs of (C,F) readings, taking the C readings directly from the Metro screen, and the F ones from his/her own portable thermometer. Suppose that the C readings are displayed to 1 decimal place, but that the F thermometer only displays the F to the nearest integer. Now, knowing that one it just takes 2 data points to draw a line, the student takes F measurements on two occasions, once when the displayed temperature is 12.5 C and one when it is 17.5 C. (The student didn’t know that when the real temperature was very close to xx.5 F, it had a 50% chance of being rounded up to xx+1 F, thereby creating an error of +0.5 F, and a 50% chance of being rounded down to xx-1 F, and producing an error of -0.5 F. Thus, the variance of each error is \\((-0.5)^2 \\times (1/2)\\) + \\((+0.5)^2 \\times (1/2)\\) = \\(0.5^2,\\) and the SD is 0.5. (In the computer exercises, we will treat a broader type of random errors in the F readings). Given that these two C settings correspond to 54.5 F and 63.5 F, but that the true temperature may be slightly on one side or the other of thse two values, what are the possible \\(\\beta\\) estimates? And, how variable would they be? The 4 possibilities, shown as the slopes of the 4 fitted lines shown in black below, are (64-55)/5, (64-54)/5, (63-55)/5, and (63-54)/5, or, when sorted, 1.6, 1.8, 1.8 and 2.0 degrees F per degree C, each with probability 1/4, so that the variance of the equally likely slopes is \\[Var[ \\hat{\\beta} ] = \\frac{(1.6 - 1.8)^2 + (1.8 - 1.8)^2 + (1.8 - 1.8)^2 + (2.0 - 1.8)^2}{4} = \\frac{1}{50} \\ = \\ 0.02,\\] and the SE is \\(\\sqrt{0.02} = 0.14\\) degrees F per degree C. Also shown in the diagram is the ‘anatomy’ of the ‘random slope’. The possible slopes are displayed as a single expression in which the 2 random elements (i.e the 2 random variables, or the two ‘errors’ e\\(_1\\) and e\\(_2\\)) are isolated. The random slope is in the form of a constant (9/5) plus another constant (1/5) times the difference of two independent random errors. Applying all of the variance rules above, we have that \\[Var[random \\ slope] = (1/5)^2 \\times ( Var[e_1] + Var[e_2]) \\ = \\ \\frac{1}{50} \\ .\\] Figure 12.9: The 4 lines (in black) fitted to the 4 possible and equally likely pairs of data points (The ‘true’ values are shown in blue). By algebraicly isolating the contributions of the 2 random errors in F to the variation in the 4 slopes, the variance of the (random) slopes is easily computed using the laws for the variance of a combination of 2 independent random variables. The important point of this simple regression example is that even though in practice there would be many more data points, the principle/law used to calculate the sampling variation of a slope based on any number of datapoints remains the same: the fitted slope is still an \\(x\\)-based linear combination of \\(y\\)’s, (in this case, a closed form combination) and thus an \\(x\\)-based linear combination of random errors – or more broadly of random deviations from the \\(x\\)-conditional means of the ‘\\(Y\\)’ variables. We will return later to all of the factors that influence the narrowness/width of sampling distributions generally, but you can maybe already see from the ‘algebraicly isolated’ representation of the slope that the more datapoints – and the wider apart they are on the \\(x\\) axis and the smaller the magnitudes of the errors — the more reproducible the slope. The influence of this latter factors is less evident in the textbook formula for the Variance and the SE. This little exercise (next) should help you figure out how the factors come into it. This piece also focuses on these isssues in a transparent way. See the exercise on this. 12.9 Exercises Refer to the fictitious cohort (shown above), constructed from the 1990 Quebec mortality rates. Use 1st principles (together with R) to calculate the standard deviation of the longevity of the male cohort. Do so in two ways, using (a) the definition (b) the ‘shortcut’. For human ‘computers’ back in the days before there were ‘electronic’ computers, what is the advantage of the shortcut? Suppose you get into the life insurance business in a small way, just taking on one client. The client pays you a premium of $100 at the beginning of each year for 5 years. If the client dies within the next 5 years, you will pay client’s estate $20,000. Thus, at the end of 5 years, your possible earnings from this single client, along with the associated (actuarily-based) probablities are: possible.earnings = c( seq(-19900,-19500,100), 500 ) probability = c(183,186,189,191,193,99058)/100000 cbind(possible.earnings,probability) ## possible.earnings probability ## [1,] -19900 0.00183 ## [2,] -19800 0.00186 ## [3,] -19700 0.00189 ## [4,] -19600 0.00191 ## [5,] -19500 0.00193 ## [6,] 500 0.99058 continued Compute the expected earnings Compute the variance (and thus the SD) of the possible earnings (a) using the definition (b) using the computational shortcut Compute the ‘risk’, the SD as a percentage of the mean, as do investors ranking how risky various stocks are. In statistics, and especially in applied statistics, what is the name for the SD as a percentage of the mean? Errors caused by rounding. Suppose one has to analyze a large number of 3 digit numbers. To make the job easier, one rounds each number to the nearest 10, e.g., 460 &lt;-- 460 461 462 463 464 ; 465 466 467 468 469 --&gt; 470. If the ending numbers of the unrounded data were uniformly distributed (each ending digit has a probability of 1/10), calculate: the average error per (rounded) number the average absolute error per (rounded) number the square root of the average squared error per (rounded) number [‘root mean squared error’, or ‘RMSE’ for short] Correcting for guessing on multiple choice exams. Suppose one wishes to estimate via a multiple choice examination [with \\(k\\) answers to choose from for each question], what proportion \\(\\pi\\) of questions a student knows the answer to (excuse the dangling preposition!). Imagine that \\(\\pi\\) refers to the N (&gt;&gt; n) questions in the much larger bank of questions from which the \\(n\\) exam questions are randomly selecetd. Show that the simple proportion \\(p\\) of correctly answered questions gives a biased (over) estimate of \\(\\pi\\) if the student simply randomly guesses among the \\(k\\) answers on questions where (s)he doesn’t know the answer. Do this by calculating the expected value of p (i.e. the average mark per question) when each answer is marked 1 if correct and 0 if not. (Hint: a tree diagram may help). One can ‘de-bias’ the estimate by giving each correct answer a mark of 1 and each incorrect answer a negative mark. What negative mark (penalty) will provide an unbiased estimate of \\(\\pi\\)? Begin by finding the expected mark per question, then set it to \\(\\pi\\) and solve for the penalty. (Hint: If you prefer, use concrete values of \\(\\pi\\) and \\(k\\) to see what penalty is needed.) Half the purchases of eggs in a market are for 6 eggs and half are for 12. What percentage of purchases are for a quantity that is more than 1 SD from the mean? less than 1 SD? Half the people in a population have 2 organs of a certain type and half have none. What is the standard deviation of the number of organs a randomly selecetd person has? Verify the variances displayed in the above Figure showing the distribution of a random variable, before and after measurement errors are added to it. Then subtract the smaller variance from the larger one to estimate the error variance. Finally show, by a separate calculation, why your answer ‘fits’ with the details of how the error-containing variable was constructed. Consider children of parents who both carry a single copy of the CF gene. (In the absence of ..) How many of their offspring will have 0, 1 or 2 copies? Half of a large number of orders were placed on Tuesday and half on Thursday. The combined orders were all jumbled together and shipped in 3 equal sized shipments on Monday Wednesday and Friday of the following week, arriving the same day they shipped. Calculate the mean and standard deviation of the number of days between ordering and arrival. Use a probability tree to depict the randomness, and to show the calculations. Refer to the example where the student tries to estimate the scaling factor between degrees C and degrees F. What if the student took the F readings at two C values that are 10 C (rather than 5 C) apart?, i.e. at 12.5 C and 22.5 C? How would the Variance and the SE be altered? What if the student took the F readings at four equally spaced C values 5 C apart, i.e., at 7.5 C, 12.5 C, 17.5C and 22.5 C?. +If you were limited to \\(n\\) C values, how would you decide where to place them? What if, rather than \\(0.5^2\\), the ‘errors’ in F had a variance of \\(1^2\\) or \\(2^2\\)? UNDER CONSTRUCTION Elevators (needs normal) Trial of Pyx 12.10 Summary Slides The concepts of a random variable, and of its expectation and variance, underpin all of statistical inference. That is why this chapter is so central, even if we don’t apply the laws by hand. The laws governing the variance of the sum and the mean of \\(n\\) random variables are the basis for Standard Errors (SEs) of statistics (parameter estimates based on aggregated data). When assessing the sampling variability or a sum or mean of independent random variables, it is not their standard deviations that sum (add), but rather their variances. This is why we have the \\(\\sqrt{n}\\) law in Statistics. The SE of a statistic is directly proportional to \\(\\sqrt{n}\\) if we are dealing with a sum, and inversely proportional to \\(\\sqrt{n}\\) (or proportional to 1/\\(\\sqrt{n}\\) ) if we are dealing with a mean. Since proportions are means (albeit of RVs that just take on two possibvle values), the same laws apply to them as well. This law was not understood/appreciated until recent centuries. Statistical historial Stephen Stigler has a very nice example, in this article, and retold in his book The Seven Pillars of Statistical Wisdom, where failure to understand it wrong gave people quite a bit of leeway to cheat. It’s also why statisticians are forced to work with variances when developing the properties of estimators. But as an end user, you will typically work with the square roots of these, and speak about the number of children per parent rather than square children per square parent. The law governing the variance of a difference of two random variables is even more important, since we are more often interested in contrasts than the level in a single context. Whether we are add or subtract independent random variables, the result is more variable than its parts. A regression slope can be represented as a linear combination (with varying positive and negative combining weights) of random variables, and so the variance and SD of the sampling distribution of a slope are gioverned by these same fundamental laws. Although the main focus was on Variances and SDs, along the way in these above sub-sections, you saw the Central Limit Theorem (CLT) trying to exert itself. Although the narrowness/width of a sampling distribution is measured by a variance or SD, the CLT focuses more on its shape. It is not possible to give a general rule for the \\(n\\) at which the CTL will ensure a sufficiently Gaussian shape for a sampling distribution. How ‘close’ to Gaussian any particular sampling distribution is depends on the ‘parent’ RV and the \\(n\\), but also on what you consider is ‘close enough for Government work’. It is not critical that we ‘do’ several exercises on the theory (laws) in this chapter. After all, you will seldom have to manually do the SE calculations based on these laws – the statistical software will do it for you. But, you do need to understand the factors that make the SE’s big or small, and the concepts involved in the propagation – and reduction – of errors. There are several exercises in the computing session that will allow you to ‘see’ the laws in action, so that you can adopt them as guiding principles for study design, and for appreciating the ‘precision’ with which you can chase (take a shot at) statistical parameters. For now, because the chapter is already a long one, we didn’t address sums and differences of non-independent random variables. But some of the computer exercsies will. "],
["distributions.html", "Chapter 13 Distributions 13.1 Objectives 13.2 Named Distributions", " Chapter 13 Distributions 13.1 Objectives Distributions of individual values take their shapes and spreads from the features of the setting, and thus do not follow any general laws. The shapes and the spreads of distributions of statistical summaries and parameter estimates made from aggregates of individual observations tend to be more regular and more predictable, and thus more law-abiding. So, the specific objectives in this chapter are to truly understand the distinction between a natural and investigator-made distributions, and between observable and conceptual ones. why we should automatically associate certain distributions with certain types of random variables why we need to understand the pre-requisites for random variables following the distributions they do why we rely so much on the Normal distribution, and why it is so ‘Central’ to statistical inference concerning parameters. why the pre-occupation with checking ‘Normality’ (Gaussian-ness) is misguided why Normality is not even relevant when the ‘variable’ is not ‘random’, and appears on the right hand side of a regression model. the few contexts where shape does matter 13.2 Named Distributions In historical order Bernoulli, and scaled Bernoulli Binomial Normal beta Poisson t Wilcoxon ======= To get a full list of the named distributions available in R you can use the following command: help(Distributions) 13.2.1 Bernoulli The simplest random variable is one that take just 2 possible values, such as YES/NO, MALE/FEMALE, 0/1, ON/OFF, POSITIVE/NEGATIVE, PRESENT/ABSENT, EXISTS/DOES NOT, etc. This random variable \\(Y\\) is governed by just one parameter, namely the probability, \\(\\pi\\), that it takes on the YES (or ‘1’) value. Of course you can reverse the scale, and speak about the probability of a NO (or ‘0’) result. It is too bad that when Wikipedia, which has a unified way of showing the main features of statistical distributions, does not follow its own principles and show a graph of various versions of a Bernoulli distribution. So here is such a graph. Figure 13.1: Various Bernoulli random variables/distributions. We continue our convention of using the letter Y (instead of X) as the generic name for a random variable. Moreover, in keeping with this view, all of the selected Bernoulli distributions are plotted with their 2 possible values shown on the vertical axis. Please, when reading the Wikipedia entry, replace all instances of \\(X\\) and \\(x\\) by \\(Y\\) and \\(y\\). Note also that we will use \\(\\pi\\) where Wikipedia, and some textbooks, use \\(p\\). As much as we can, we use Greek letters for parameters and Roman letters for their empirical (sample) counterparts. Also, to be consistent, if the random variable itslef is called \\(Y\\), then it makes sense to use \\(y\\) as the possible relaizations of it, rather than the illogical \\(k\\) that Wikipedia uses.] In the sidebar, Wikipedia shows the probability mass function (pmf, the probabilities that go with the possible \\(Y\\) values) in two separate rows, but in the text the pmf is also shown more concisely, as (in our notation) \\[f(y) = \\pi^y (1-\\pi)^{1-y}, \\ \\ y = 0, 1.\\] If we wish to align with how the R software names features of distributions, we might want to switch from \\(f\\) to \\(d\\). R uses \\(d\\) because it harmonizes with the probability \\(d\\)ensity function (pdf) notation that its uses for random variables on an interval scale, even though some statistical ‘purists’ see that as mixing terminology: they use the term probablility mass function for discrete random variables, and probablity density function for ones on an interval scale. \\[d_{Bernoulli}(y) = \\pi^y (1-\\pi)^{1-y}.\\] Sadly, Bernoulli does not get its own entry in R’s list of named distributions, presumably because it is a special case of a binomial distribution, one where \\(n\\) = 1. So we have to call dbinom(x,size,prob) to get the density (probability mass) function of the binomial distribution with parameters size (\\(n\\)) and prob (\\(\\pi\\)), and set \\(n\\) to 1. The 3 arguments to dbinom(x,size,prob) are: x: a vector of quantiles (here just 0 or/and 1), size: the number of ‘trials’ (our ‘\\(n\\)’, so 1 for Bernoulli), and prob: the probability of ‘success’ on each ‘trial’. We think of it as the probability that a realization of $Y4, i.e, \\(y\\) will equal 1, or as \\(\\pi.\\)) Thus, dbinom(x=0,size=1,prob=0.3) yields 0.7, while dbinom(x=1,size=1,prob=0.3) yields 0.3 and dbinom(x=c(0,1),size=1,prob=0.3) yields the vector 0.7, 0.3. Incidentally, please do not adopt the convention that \\(x\\) (or our \\(y\\)) is the number of ‘successes’ in \\(n\\) trials. It is the number of ‘positives’ in a sample of \\(n\\) independent draws from a population in which a proportion \\(\\pi\\) are positive. Expectation (E) and Variance (V) of a Bernoulli random variable. Shortening \\(Prob(Y=y)\\) to \\(P_y\\), we have From first principles, \\[E[Y] = 0 \\times P_0 + 1 \\times P_1 = 0 \\times (1-\\pi) + 1 \\times \\pi = \\pi,\\] while \\[V[Y] = (0-\\pi)^2 \\times P_0 + (1-\\pi)^2 \\times P_1 = \\pi^2(1-\\pi) + (1-\\pi)^2\\pi = \\underline{\\pi(1-\\pi)}.\\] This functional form for the (‘unit’) variance is not entirely surprising: it is obvious from the selected distributions whon that the most concentrated Bernoulli distributions are the ones where the proportion \\((\\pi)\\) of Y = 1 values is either close to 1 or to zero, and that the most spread out Bernoulli distributions are the ones where \\(\\pi\\) is close to 1/2. And, and a function of \\(\\pi\\), the Variance must be symmetric about \\(\\pi\\) = 1/2. The fact that the greatest uncertainty (‘entropy’, lack of order or predictability) is when \\(\\pi\\) = 1/2 is one of the factors that makes sports contests more engaging when teams or players are well matched. Later, when we come to study what influences the imprecision of sample surveys, we will see that for a given sample size, the imprecision is largest when \\(\\pi\\) is closer to 1/2. Why focus on the variance of a Bernoulli random variable? because, later, when we use the more intesting binomial distribution, we can call on first prionciples to recall what its expection and variance are. A Binomial random variable is the sum of \\(n\\) independently distributed Bernoulli random variables, all with the same expectation \\(\\pi\\) and unit variance \\(\\pi(1-\\pi).\\) Thus its expectation (\\(E\\)) and variance (\\(V\\)) are the sums of these ‘unit’ versions, i.e., \\(E[binom.sum] = n \\times \\pi\\) and \\(V[binom.sum] = n \\times \\pi(1-\\pi).\\) Moreover, again from first principles, we can deduce that if instead of a sample sum, we are interested in a sample mean (here the mean of the 0’s and 1’s is the sample proportion), its expected value is \\[\\boxed{\\boxed{E[binom.prop&#39;n] = \\frac{n \\pi}{n} = \\pi; \\ V[.] = \\frac{n \\pi(1-\\pi))}{n^2} = \\frac{\\pi(1-\\pi)}{n}; \\ SD[.] = \\frac{\\sqrt{\\pi(1-\\pi)}}{ \\sqrt{n}} = \\frac{\\sigma_{0,1}}{\\sqrt{n}} } } \\] Note here the generic way we write the SD of the sampling distribution of a sample proportion, in the same way that we write the SD of the sampling distribution of a sample mean, as \\(\\sigma_u/\\sqrt{n},\\) where \\(\\sigma_u\\) is the ‘unit’ SD, the standard deviation of the values of individuals. The individual values in the case of a Bernoulli randomn variable are just 0s and 1s, and their SD is \\(\\sqrt{\\pi(1-\\pi)}.\\) We call this SD the SD of the 0’1 and 1’s, or \\(\\sigma_{0,1}\\) for short. Notice how, even though it might look nicer and simpler to compute, and involves just 1 square root calculation, we did not write the SD of a binomial proportion as \\[SD[binom.proportion] = \\sqrt{\\frac{\\pi(1-\\pi)}{ n} }.\\] We choose instead to use the \\(\\sigma/\\sqrt{n}\\) version, to show that it has the same form as the SD for the sampling distribution of a sample mean. Now that we no longer need to savw keystrokes on a hand caloculator, we should move away from computational forms and focus instead on the intuitive form. Sadly, many textbooks re-use the same concept in disjoint chapters without telling readers they are cases of the same SD formula. There is a lot to be gained by thinking of proportions as means, but where the \\(Y\\) values are just 0’s and 1’s. You can use the R code below to simulate a very large number of 0’s and 1’s, and calculate their variance. The sd function in R doesn’t know or care that the values you supply it are limited to just 0s and 1s, or spread along an interval. Better still don’t use the rbinom function; instead use the sample function, with replacement. n = 750 zeros.and.ones = sample(0:1, n , prob=c(0.2, 0.8),replace=TRUE ) m = matrix(zeros.and.ones,n/75,75) noquote(apply(m,1,paste,collapse=&quot;&quot;)) ## [1] 111111111110011111111111111111101111111110011101010111111011011101001000110 ## [2] 110011111111111111111111001011111011100111011101111101011101111111111111011 ## [3] 011011110111111111101111011111110111011111000111011111101110111101101011101 ## [4] 110100011111111111011111111011111111111111111101101101111110010011111111111 ## [5] 101111011111101011111111110111011110110111111110011011011010111111010111111 ## [6] 111110111111110010101111111111111101111110001111111111100110110111110110111 ## [7] 011111101111110111111111110111111110011101111111101111110111111111111110011 ## [8] 111110111101010111111111110101101110111010111100111101010110101001001010100 ## [9] 110111100111111100111001111111101111111110101010110101101111111111000111111 ## [10] 011111011111111011111101111111011010001011111010101101111000100111111011111 sum(zeros.and.ones)/n ## [1] 0.7786667 round( sd(zeros.and.ones),4) ## [1] 0.4154 Try the above code with a larger \\(n\\) and a different \\(\\pi\\) and convince yourself that the variance (and thus the SD) of the individual 0 and 1 values (a) have nothing to do with how many there are and everything to do with what proportion of them are of each type and (b) are larger when the proportions are close to each other, and smaller when they are not. Scaled-Bernoulli random variables Could we get by without studying the Binomial Distribution? The answer is ‘for most applications, yes.’ The reason is that in in most cases, we are able to use a Gaussian (Normal) approximation to the binomial distribution. Thus, all we need are its expectation and variance (standard deviation): we don’t need the dbinom() probability mass function, or the pbinom() that gives the cumulative distribution function and thus the tail areas, or the qbinom() function that gives the quantiles. But sometimes we deal with situations where the binomial distributions are not symmetric and close-enough-to-Gaussian. Below we recount how, in 1738, almost 4 decades before Gauss was born, when summing the probabilities of a binomial distribution with a large \\(n\\), deMoivre effectively used the as-yet unrecognized ‘Gaussian’ distribution as a very accurate approximation. Without calling it this, he relied on the standard deviation of the binomial distribution. === 13.2.2 Binomial The Binomial Distribution is a model for the (sampling) variability of a proportion or count in a randomly selected sample The Binomial Distribution: what it is The \\(n+1\\) probabilities \\(p_{0}, p_{1}, ..., p_{y}, ..., p_{n}\\) of observing \\(y\\) = \\(0, 1, 2, \\dots , n\\) ‘positives’ in \\(n\\) independent realizations of a Bernoulli random variable \\(Y\\)with probability, \\(\\pi,\\) that Y=1, and (1-\\(\\pi\\)) that it is 0. The number is the sum of \\(n\\) independen Bernoulli random variables with the same probability, such as in s.r.s of \\(n\\) individuals. Each of the \\(n\\) observed elements is binary (0 or 1) There are \\(2^{n}\\) possible sequences … but only \\(n+1\\) possible values, i.e. \\(0/n,\\;1/n,\\;\\dots ,\\;n/n\\) can think of \\(y\\) as sum of \\(n\\) Bernoulli r. v.’s. [Later on, in ptractive, we will work in the same scale as parameter. i.e., (0,1). not the (0,n) ‘count’ scale.] Apart from \\(n\\), the probabilities \\(p_{0}\\) to \\(p_{n}\\) depend on only 1 parameter: the probability that a selected individual will be ‘positive’ i.e., have the trait of interest the proportion of ‘positive’ individuals in the sampled population Usually we denote this (un-knowable) proportion by \\(\\pi\\) (or sometimes by the more generic \\(\\theta\\)) Textbooks are not consistent (see below); we try to use Greek letters for parameters, Note Miettinen’s use of UPPER-CASE letters, [e.g. \\(P\\), \\(M\\)] for PARAMETERS and lower-case letters [e.g., \\(p\\), \\(m\\)] for statistics (estimates} of parameters). Author(s) PARAMETER Statistic Clayton and Hills \\(\\pi\\) \\(p = D/n\\) Moore and McCabe, Baldi and Moore \\(p\\) \\(\\hat{p} = y/n\\) Miettinen \\(P\\) \\(p = y/n\\) This book \\(\\pi\\) \\(p = D/n\\) Shorthand: \\(Y \\sim Binomial(n, \\pi)\\) or \\(y \\sim Binomial(n, \\pi)\\) How it arises Sample Surveys Clinical Trials Pilot studies Genetics Epidemiology Uses to make inferences about \\(\\pi\\) from observed proportion \\(p = y/n\\). to make inferences in more complex situations, e.g. … Prevalence Difference: \\(\\pi_{index.category}\\) - \\(\\pi_{reference.category}\\) Risk Difference (RD): \\(\\pi_{index.category}\\) - \\(\\pi_{reference.category}\\) Risk Ratio, or its synonym Relative Risk (RR): \\(\\frac{\\pi_{index.category}}{\\pi_{reference.category}}\\) Odds Ratio (OR): \\(\\frac{\\pi_{index.category}/(1-\\pi_{index.category})}{ \\pi_{reference.category}/(1-\\pi_{reference.category}) }\\) Trend in several \\(\\pi\\)’s Requirements for \\(Y\\) to have a Binomial\\((n, \\pi)\\) distribution Each element in the ‘population’ is 0 or 1, but we are only interested in estimating the proportion (\\(\\pi\\)) of 1’s; we are not interested in individuals. Fixed sample size \\(n\\). Elements selected at random and independent of each other; each element in population has the same probability of being sampled: i.e., we have \\(n\\) independent Bernoulli random variables with the same expectation (statisticians say ‘i.i.d’ or ‘independent and identically distributed’). It helps to distinguish the population values, say \\(Y_1\\) to \\(Y_N\\), from the \\(n\\) sampled values \\(y_1\\) to \\(y_n\\). Denote by \\(y_i\\) the value of the \\(i\\)-th sampled element. Prob[\\(y_i\\) = 1] is constant (it is \\(\\pi\\)) across \\(i\\). In the What proportion of our time do we spend indoors? example, it is the random/blind sampling of the temporal and spatial patterns of 0s and 1s that makes \\(y_1\\) to \\(y_n\\) independent of each other. The \\(Y\\)’s, the elements in the population can be related to each other [e.g. there can be a peculiar spatial distribution of persons] but if elements are chosen at random, the chance that the value of the \\(i\\)-th element chosen is a 1 cannot depend on the value of \\(y_{i−1}\\) or any other \\(y\\): the sampling is ‘blind’ to the spatial location of the 1’s and 0s. Binomial probabilities, illustrated using a Binomial Tree Figure 13.2: From 5 (independent and identically distributed) Bernoulli observations to Binomial(n=5), with the Bernoulli probability left unspecified. There are 2 to the power n possible (distinct) sequences of 0’s and 1’s, each with its probability. We are not interested in these 2 to the power n probabilities, but in the probability that the sample contains y 1’s and (n-y) 0’s. There are only (n+1) possibilities for y, namely 0 to n. Fortunately, each of the n.choose.y sequences that lead to the same sum or count y, has the same probability. So we group the 2.to.power.n sequences into (n+1) sets, according to the sum or count. Each sequence in the set with y 1’s and (n-y) 0’s has the same probability, namely the prob.to.the.power.y times (1-prob).to.the.power.(n-y). Thus, in lieu of adding all such probabilities, we simply multiply this probability by the number, n.choose-y – shown in black – of unique sequences in the set. Check: the frequencies in black add to 2.to.power.n. Nowadays, the (n+1) probabilities are easily obtained by supplying a value for the ‘prob’ argument in the R function dbinom(), instead of computing the binomial coefficient n.choose-y by hand. If you rotate the binomial tree to the right by 90 degrees, and use your imagination, you can see how it resembles the quincunx constructed by Francis Galton. He used it to show how the Central Linit Theorem, applied to the sum of several ‘Bernoulli deflections to the right and left’, makes a Binomial distribution approach a Gaussian one. Several games and game shows are built on this pinball machine, for example, Plinko and, more recently, The Wall. Galton’s quincunx has its own cottage industry, and versions of it often displayed in Science Museums. The present authors inherited a low tech version of the Galton Board, where the ‘shot’ are turnip seeds, from former McGill Professor – and early teacher of course 607 – FDK Liddell. Does the Binomial Distribution apply if… ? Interested in: \\(\\pi\\) the proportion of 16 year old girls in Quebec protected against rubella Choose: \\(n\\) = 100 girls: 20 at random from each of 5 randomly selected schools [‘cluster’ sample] Count \\(y\\) how many of the \\(n\\) = 100 are protected \\(\\bullet\\) Is \\(y ~ \\sim Binomial(n=100, \\ \\pi)\\) ? …………. ……….. ……………………………………………………… ‘SMAC’: \\(\\pi\\) Chemistry Auto-analyzer with n = 18 channels. Critertion for ‘positivity’ set so that Prob[‘abnormal’ result in Healthy person] = 0.03 for each of 18 chemistries tested Count \\(y\\) (In 1 patient) how many of \\(n\\) = 18 give abnormal result. \\(\\bullet\\) Is \\(y ~ \\sim Binomial(n=18, \\ \\pi=0.03)\\) ? credit: Ingelfinger textbook …………. ……….. ……………………………………………………… …………. ……….. ……………………………………………………… Sex Ratio: \\(n=4\\) children in each family \\(y\\) number of girls in family \\(\\bullet\\) Is \\(y ~ \\sim Binomial(n=4, \\ \\pi=0.49)\\) ? …………. ……….. ……………………………………………………… Interested in: \\(\\pi_u\\) proportion in ‘usual’ exercise classes and in \\(\\pi_e\\) expt’l. exercise classes who ‘stay the course’ Randomly 4 classes of Allocate 25 students each to usual course \\(n_u = 4 \\times 25 = 100\\) \\(n_e\\) = 4 classes of 25 students each to experimental course \\(n_e = 4 \\times 25 =100\\) Count \\(y_u\\) how many of the \\(n_u\\) = 100 complete course \\(y_e\\) how many of the \\(n_e\\) = 100 complete course \\(\\bullet\\) Is \\(y_u ~ \\sim Binomial(n = 100, \\ \\pi_u)\\) ? \\(\\bullet\\) Is \\(y_e ~ \\sim Binomial(n = 100, \\ \\pi_e)\\) ? …………. ……….. ……………………………………………………… Pilot Study: To estimate proportion \\(\\pi\\) of population that is eligible and willing to participate in long-term research study, keep recruiting until obtain \\(y\\) = 5 who are. Have to approach \\(n\\) to get \\(y\\). \\(\\bullet\\) Is \\(y ~ \\sim Binomial(n, \\ \\pi)\\) ? …………. ……….. ……………………………………………………… Calculating Binomial probabilities Exactly probability mass function (p.m.f.) : formula: \\(Prob[y] = \\ ^n C _y \\ \\pi^y \\ (1 − \\pi)^{n−y}\\). recursively: \\(Prob[0] = (1−\\pi)^n\\);    \\(Prob[y] = \\frac{n−y+1}{y} \\times \\frac{\\pi}{1-\\pi} \\times Prob[y−1]\\). Statistical Packages: R functions dbinom(), pbinom(), qbinom() probability mass, distribution/cdf, and quantile functions. Stata function Binomial(n,k,p) SAS PROBBNML(p, n, y) function Spreadsheet — Excel function BINOMDIST(y, n, π, cumulative) Tables: CRC; Fisher and Yates; Biometrika Tables; Documenta Geigy Using an approximation Poisson Distribution (\\(n\\) large; small \\(\\pi\\)) Normal (Gaussian) Distribution (\\(n\\) large or midrange \\(\\pi\\), so that the expected value, \\(n \\times \\pi\\), is sufficiently far ‘in from’ the ‘edges’ of the scale, i.e., sufficiently far in from 0 and from \\(n\\), so that a Gaussian distribution doesn’t flow past one of the edges. The Normal approximation is good for when you don’t have access to software or Tables, e.g, on a plane, or when the internet is down, or the battery on your phone or laptop had run out, or it takes too long to boot up Windows!). To use the Normal approximatiom, be aware of the scale you are working in, .e.g., if say \\(n = 10\\), whether the summary is a count or a proportion or a percentage. r.v. e.g. E S.D. count: \\(y\\) 2 \\(n \\times \\pi\\) \\(\\{n \\times \\pi \\times (1-\\pi) \\}^{1/2}\\) i.e., \\(n^{1/2} \\times \\sigma_{Bernoulli}\\) proportion: \\(p=y/n\\) 0.2 \\(\\pi\\) \\(\\{\\pi \\times (1-\\pi) / n \\}^{1/2}\\) i.e., \\(\\sigma_{Bernoulli} / n^{1/2}\\) percentage: \\(100p\\%\\) 20% \\(100 \\times \\pi\\) \\(100 \\times SD[p]\\) The first person to suggest an approximation, using what we now call the ‘Normal’ or ‘Gaussian’ of ‘Laplace-Gaussian’ distribution, was deMoivre, in 1738. There is a debate among historians as to whether this marks the first description of the Normal distribution: the piece does not explicitly point to the probability density function \\(\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\times exp[-z^2/2\\sigma^2],\\) but it does highlight the role of the quantity \\((1/2) \\times \\sqrt{n}\\), the standard deviation of the sum of \\(n\\) independent Bernoulli random variables, each with expectation 1/2 and thus a ‘unit’ standard deviation of 1/2, and also the SD quantity \\(\\sqrt{\\pi(1-\\pi)}\\) \\(\\times\\) \\(\\sqrt{n}\\) in the more general case. DeMoivre arrived at the familiar ‘68-95-99.7 rule’ : the percentages of a normal distribution that lie within 1, 2 and 3 SD’s of its mean. Factors that modulate the shapes of Binomial distributions size of \\(n\\): the larger the n, the more symmetric value of \\(\\pi\\): the closer to 1/2, the more symmetric In these small-\\(n\\) contexts, only those distribtions where \\(\\pi\\) is close to 0.5 are reasonably symmetric. In larger-\\(n\\) contexts (see below), as long as there is ‘room’ for them to be, binomial distribtions where the expected value \\(E = n \\times \\pi\\) is at least 5-10 ‘in from the edges’ (i.e. to the right of 0, or the left of \\(n\\), are reasonably symmetric. Figure 13.3: Binomial random variables/distributions, where n = 5, and the Bernoulli expectation (probability) is smaller (left panels) or larger (right panels). Figure 13.4: Various Binomial random variables/distributions, where n = 20. The dotted horizontal lines in light blue are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units from the (0,n) boundaries are shown in blue. Figure 13.5: Various Binomial random variables/distributions, where n = 50. The blue dotted lines are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units in from the (0,n) boundaries are shown in blue Back when binomial computations were difficult, and the normal approximation was not accurate, there was another approximation that saved labour, and in particular, avoided having to deal with the very large numbers involved in the \\(^nC_y\\) binomial coefficients (even if built into a hand calculator, these can be problematic when \\(n\\) is large). Poisson, in 1837, having shown how to use the Normal distribution for binomial (and the closely-related negative binomial) calculations, devoted 1 small section (81) of less than 2 pages, to the case where (in our notation) one of the two chances \\(\\pi\\) or \\((1-\\pi)\\) ‘est très petite’ [Poisson’s \\(q\\) is our \\(\\pi\\), his \\(\\mu\\) is our \\(n\\), his \\(\\omega\\) is our \\(\\mu\\), and his \\(n\\) is our \\(y\\)]. In our notation, he arrived at \\[Prob[y \\ or \\ fewer \\ events] = \\bigg(1 + \\frac{\\mu}{1!} + \\frac{\\mu^2}{2!} + \\dots + \\frac{\\mu^y}{y!} \\bigg) \\ e^{-\\mu}.\\] In the last of his just three paragraphs on this digression, he notes the probability \\(e^{-\\mu}\\) of no event, and \\(1 - e^{-\\mu}\\) of at least 1. He also calculates that, when \\(\\mu\\) = 1, the chance is very close to 1 in 100 million that more than \\(y\\) = 10 events would occur in a very large series of trials (of length \\(n\\)), where the probability is 1/\\(n\\) in each trial. Although he give little emphasis to his formula, and no real application, it is Poisson’s name that is now undividely associated with this formula. Below are some examples of how well it approximates a Binomial in the ‘corner’ where \\(\\pi\\) is very small and \\(n\\) is very large. If, of course, the product, \\(\\mu = n \\times \\pi\\) , reaches double digits, the Normal approximation distribution provides and accurate approximation for both the Binomial and the Poisson distributions, and – if one has ready acccess to the tail areas of a Normal distribution – with less effort. Today, of course, unless you are limited to a hand calculator when the internet and R and paper tables are nor available, you would not need to use any approximation, Normal or Poisson, to a binomial. Figure 13.6: Various Binomial random variables/distributions, with large n’s and small Bernoulli probabilities, together with the Poisson distributions with the corresponding means. The Poisson distrubution provides a good approximation in the ’lowee corner of the Bimonial distribution with large n’s and small probabilities. And, when the product, mu = n * probability, is in the double digits, the Normal approximation distribution provides and accurate approximation for both the Binomial and the Poisson distributions. Direct applications of the Poisson distribution today have little direct connection with a large number of Bernoulli trials with a small expecation per trial, or with providing computational shortcuts. Although Poisson did not appreciate it at the time, today his distribution has to do with the distribution of counts of events in a given amount of space or time. However, most mathematical statistics textbooks still derive the Poisson distribution as the limiting case of a Binomial. Below, we will take a broader view, emphasize the ‘time’ dimension, and give a littel more history. We will also focus on when the Poisson distribution is too narrow, and an inappropriate model for counts. We will repeat the earlier message that just becasue a random variable involbs counts doesn’t make its distribution Poisson. The best example of this is the falancy of treating the distributions of white bllood cell, or CD4 cell concentrations as if there were observed counts. 13.2.3 Poisson model for (Sampling) Variability of a Count in a given amount of ‘experience’ The Poisson Distribution: what it is, and some of its features The (infinite number of) probabilities \\(P_{0}, P_{1}, ..., P_{y}, ...,\\) of observing \\(Y = 0, 1, 2, \\dots , y, \\dots\\) ‘events’/‘instances’ in a given amount of ‘experience.’ These probabilities, \\(Prob[Y = y]\\), or \\(P_{Y}[y]\\)’s, or \\(P_{y}\\)’s for short, are governed by a single parameter, the mean \\(E[Y] = \\mu.\\) \\(P[y]= \\exp[-\\mu]\\: \\frac{\\mu^{y}}{y!};\\)  in R: dpois( ) – too bad \\(\\mu\\) is referred to as lambda. Shorthand: \\(Y\\sim\\; \\textrm{Poisson}(\\mu)\\). \\(\\sigma^2_Y = \\textrm{Variance}[Y] = \\mu \\ ; \\ \\ \\sigma_Y = \\sqrt{\\mu_{Y}}.\\) It can be approximated by \\(N(\\mu, \\: \\sigma_{Y} = \\mu^{1/2})\\) when \\(\\mu &gt;&gt; 10\\). It is open-ended (unlike Binomial), but in practice, has finite range. Poisson data are sometimes called ‘numerator only’: (unlike in the Binomial) you may not ‘see’ or count ‘non-events’: but there is (an invisible) denominator ‘behind’ the number of ‘wrong number’ phone calls you receive. How it arises / derivations Count of events (items) that occur randomly, with low homogeneous intensity, in time, space, or ‘item’-time (e.g. person–time). As a limiting case of a Binomial(\\(n,\\pi\\)) when \\(n \\rightarrow \\infty\\textrm{ and } \\pi \\rightarrow 0,\\) but \\(n \\times \\pi = \\mu\\) is finite. [Poisson, in 1837, derived it as an approximation to the closely-related negative binomial distribution.] \\(Y\\sim Poisson(\\mu_{Y}) \\Leftrightarrow T \\textrm{ time between events}\\sim \\: \\textrm{Exponential}(\\mu_{T} = 1/\\mu_{Y}).\\) Here is one attempt to explain this and here is another together with this translation and additional annotation As the sum of \\(\\ge 2\\) independent Poisson rv’s, with the same or different \\(\\mu\\)’s: \\(Y_{1} \\sim \\textrm{Poisson}(\\mu_{1}) \\: \\: Y_{2} \\sim \\textrm{Poisson}(\\mu_{2}) \\Rightarrow Y = Y_{1} + Y_{2} \\sim \\textrm{Poisson}(\\mu_{1}+\\mu_{2}).\\) Examples: numbers of asbestos fibres, deaths from horse kicks, needle-stick or other percutaneous injuries, bus-driver accidents, twin-pairs, radioactive disintegrations, flying-bomb hits*, white blood cells, typographical errors, ‘wrong numbers’, cancers; chocolate chips, radioactive emissions in nuclear medicine, cell occupants – in a given volume, area, line-length, population-time, time, etc. * included in Some History (see Haight ). 1718 deMoivre : the series \\(e^{-\\mu} \\bigg(1 + \\frac{\\mu}{1!} + \\frac{\\mu^2}{2!} + \\dots \\bigg)\\) appears. 1837 Poisson (see above) 1860 Simon Newcomb, a Canadian-born ‘astronomer, applied mathematician and autodidactic polymath who who was Professor of Mathematics in the United States Navy and at Johns Hopkins University, derives the ’Poisson’ Law ’from scratch’ as a limiting case of the Binomial, and applies it to the determination of the ’probability that, if the stars were scattered at random over the heavens, any small space selected at random would contain \\(s\\) stars.’ 1878 Abbe determined the number of cells one needed to count in order to achieve a sufficiently small ‘probable error’ for an estimated blood cell concentration. 1905 Schweidler provided error calculations for counting radioactive transformations. 1989 Bortkewitsch book, with many examples, and tables of the Poisson Distribution. 1907 ‘Student’ derived the Poisson distribution from scratch (as a liniting case of the binomial) and provided error calculations for measuring yeast cells in beer. 1909 Erlang, a Danish engineer concerned with the theory of telephone traffic, developed the Poisson Law by an elegant argument based on the steady state behaviour of the beginning and terminations of calls. Here are the original Danish version and a translation. This ‘simple explanation’ in the English translation of a 1920 article has a critical cut and paste typographical error. The original Danish version has it correct. 1910 Rutherford (having departed McGill in 1907) and Geiger (of the ‘counter’), physicists, and Bateman, a mathematician, developed the Poisson Law by an time-based argument expressed as a differential equation, a Law that agreed with the empirical distibution in their large particle-counting experiment. 1946 Flying-bomb attacks on London during WW II, along with 2019 Update: Significance Magazine Early-career Writing Award announcement and intro to and full article. Commentators Newbold gives a lot of credit to deMoivre. Haight is the mots comprehensive. Stigler is a bit more nuanced. Distribution of particles in volumes The following two distributions are taken from ’Student’s 1907 article on counting yeast cells under a haemocytometer, an instrument for visual counting of the number of cells in a blood sample or other fluid under a microscope. In the article, he derived what today we would call the Poisson distribution, and offered guidance on how many fields/cells to count in order the ensure a small enough margin of error when estimating a concentration. Student’s main messages The same accuracy is obtained by counting the same number of particles whatever the dilution. Hence the most accurate way is to dilute the solution to the point at which the particles may be counted most rapidly, and to count as many as time permits. The standard deviation of the mean number of particles per unit volume is \\(\\sqrt{m/M},\\) where \\(m\\) is the mean number and M the number of unit volumes counted. This theoretical work was tested on four distributions which had been counted over the whole 400 squares of the haemocytometer. The particles counted were yeast cells which were killed by adding a little mercuric chloride to the water in which they had been shaken up. A small quantity of this was mixed with a 10% solution of gelatine, and after being well stirred up drops were put on the haemacytometer. This was then put on a plate of glass kept at a temperature just above the setting point of gelatine and allowed to cool slowly till the gelatine had set. Four different concentrations were used. The full description can be found here. Figure 13.7: Examples of distributions of Yeast Cells over 1 sq. mm. divided into 400 squares. The data in the right panel are based on the 400 counts in Table 1 of Student’s article (concentration IV), while those in the left panel are simulated to match the reported mean (1.32 per square) and variance (1.28) for the 400 counts at concentration II. It appears that Student calculated the variance using a divisor of n rather than n-1. The data in the panel on the right panel can be viewed at the bottom of this page. Student took care to avoid ‘clumping’ of cells, and other artifacts, and measured the (as they turned out to be, small) correlations between the nunber of cells on a square and the number in each of the four cells nearest it. He noticed that in both concentrations I and III the variance was greater than the mean, due to an [to us, slight] excess (of the “observed”) over the calculated (“fitted”) among the high numbers in the tail of the distribution. [Mean = 0.6825, variance 0.8117 in I; Mean = 1.80; variance = 1.96 in III]. He has pointed out earlier in the article that the budding of the yeast cell increases these high numbers, and there is also probably a tendency to stick together in groups which was not altogether abolished even by vigorous shaking. He also tested the observed frequency distribution of the 400 counts (they varied from 0 to 6 in the left panel, and 1 to 12 in the right panel) against the expected numbers of 0’s, 1’s, 2’s, etc. under the (Poisson) law using a \\(\\chi^2\\) goodness of fit statistic, and found them to be reasonable. ‘The p-values, 0.04, 0.68, 0.25 and 0.64, though not particularly high, are not at all unlikely in four trials, supposing our theoretical law to hold, and we are not likely to be very far wrong in assuming it to do so.’ Clearly, the narrow pure-Poisson variation we see in Student’s counts is a tribute to his ‘vigorous shaking’ that produced the randomness. [Think about how you would ensure this with raisins in dough, or chocolate chips in cookies.] Distribution of events in time intervals This next example – on counting alpha-radiation disintegrations – has the same element of randomness, but in the time dimension. Here is Rutherford’s description: The source of radiation was a small disk coated with polonium, which was placed inside an exhausted tube, closed at one end by a zinc sulphide screen. The scintillations were counted in the usual way by means of a microscope on an area of about one sq. mm. of screen. During the timo of counting (5 days), in order to correct for the decay, the polonium was moved daily closer to the screen in order that the average number of \\(\\alpha\\) particles impinging on the screen should be nearly constant. The scintillations were recorded on a chronograph tape by closing an electric circuit by hand at the instant of each scintillation. Time-marks at intervals of one half-minute were also automatically recorded on iho same tape. After the eye was rested, scintillations were counted from 3 to 5 minutes. The motor running the tape was then stopped and the eye rested for several minutes ; then another interval of counting, and so on. It was found possible to count 2000 scintillations a day, and in all 10,000 were recorded. The records on the tape were then systematically examined. The length of tape corresponding to half-minute marks was subdivided into four equal parts by means of a celluloid film marked with five parallel lines at equal distances. By slanting the film at different angles, the outside lines were made to pass through the time-marks, and thee number of scintillations between the lines corresponding to 1/8 minute intervals were counted through the film. By this method correction was made for slow variations in the speed of the motor during the long interval required by the observations. The following is meant to look like the marks on a 5-minute strip on the paper tape described by Rutherford. ## [1] 3.85 Figure 13.8: Simulated distributions of events (scintillations produced by a radioactive source) over a 5-minute time-span. Each 1/8th of a minute is shown is a different colour. The counts in each 7.5 second bin are shown at the bottom. "],
["d.html", "Chapter 14 1 d 14.1 Exercises", " Chapter 14 1 d strips = 10; w=20 ; d.w = 0.001; n.trials = w/d.w; lambda = 0.7; p = lambda*d.w ; x=c(); y=c(); for (strip in 1: strips) { for (i in 1:n.trials) { if (runif(1) &lt; p) { x=c(x, i*d.w ) ; y=c(y, strip/strips) } } } par(yaxt = “n”) r,eval=T, echo=F, fig.align=“center”, fig.height=7, fig.width=9, warning=FALSE, message=F,fig.cap=“Various Binomial random variables/distributions, with large n’s and small Bernoulli probabilities, together with the Poisson distributions with the corresponding means. The Poisson distrubution provides a good approximation in the ’lowee corner of the Bimonial distribution with large n’s and small probabilities. And, when the product, mu = n * probability, is in the double digits, the Normal approximation distribution provides and accurate approximation for both the Binomial and the Poisson distributions.”} plot(0:w, (0:w)/w, “s”,col = “white”, xlab=“time”, ylab=&quot; “, frame.plot=F ) for (strip in 1: strips) abline(h= strip/strips, col =“lightgray”) for (i in 0:w) abline(v= i, col =“lightgray”) points(x,y,cex=0.5) Events in Population-Time randomly generated from intensities that are constant within (2 squares high by 2 squares wide) `panels’, but vary between such panels. In Epidemiology, each square might represent a number of units of population-time, and each dot an event.} Events in Time: 10 examples [1/row], randomly generated from constant over time intensities. Simulated with 1000 Bernoulli\\((\\tiny{\\textrm{small }\\pi})\\)’s per time unit.} Does the Poisson Distribution apply to\\(\\dots\\)? Yearly variations in numbers of killed in plane crashes? Daily variations in numbers of births? Daily variations in numbers of deaths [extra variation over the seasons] Daily variations in numbers of traffic accidents [variation over the seasons, and days of week, and with weather etc.] Daily variations in numbers of deaths in France in summer 2002 &amp; 2003 Variations across cookies/pizzas in numbers of chocolate chips/olives. Variations across days of year in numbers of deaths from sudden infant death syndrome. See Calculating Poisson probabilities: Exactly R: dpois, ppois, qpois rpois mass/cum./quant./rand `SAS: POISSON; `Stata: Spreadsheet — Excel function POISSON} (\\(y, \\mu\\), cumulative) Using Gaussian Approximations to distribution of \\(y\\) or transforms of it: described below, under Inference. In ‘the old days’ i.e. in the years BC (Before Computers’), Poisson tail probabilities were often calculated using links to the tail areas of other better-tabulated continuous distributions, such as the Chi-Square distribution. ======== https://en.wikipedia.org/wiki/Normal_distribution#CITEREFStigler1982 dark https://physics.stackexchange.com/questions/28563/hours-of-light-per-day-based-on-latitude-longitude-formula https://environhealthprevmed.biomedcentral.com/articles/10.1186/s12199-017-0637-4 car sppeds https://purr.purdue.edu/publications/2671/1 Hypergeometric Non-Central Hypergeometric Chi-square === EXERCISES ADD YOUR OWN SEARCH RESULTS to table above Hydroxychloroquine uniformity table. few for differences woolf normal transformations 14.1 Exercises 14.1.1 Clusters of Miscarriages [based on article by L Abenhaim] Assume that: 15% of all pregnancies end in a recognized spontaneous abortion (miscarriage) – this is probably a conservative estimate. Across North America, there are 1,000 large companies. In each of them, 10 females who work all day with computer terminals become pregnant within the course of a year [the number who get pregnant would vary, but assume for the sake of this exercise that it is exactly 10 in each company]. *There is no relationship between working with computers and the risk of miscarriage. a cluster'' of miscarriages is defined asat least 5 of 10 females in the same company suffering a miscarriage within a year’’ Exercise: Calculate the number of ‘clusters’ of miscarriages one would expect in the 1,000 companies. Hint: begin with the probability of a cluster. 14.1.2 ‘Prone-ness’ to Miscarriages ? Some studies suggest that the chance of a pregnancy ending in a spontaneous abortion is approximately 30%. On this basis, if a woman becomes pregnant 4 times, what does the binomial distribution give as her chance of having 0,1,2,3 or 4 spontaneous abortions? On this basis, if 70 women each become pregnant 4 times, what number of them would you expect to suffer 0,1,2,3 or 4 spontaneous abortions? (Think of the answers in (i) as proportions of women). Compare these theoretically expected numbers out of 70 with the following observed data on 70 women, each of whom had 4 pregnancies: No. of spontaneous abortions: 0 1 2 3 4 No. of women with this many abortions: 23 28 7 6 6 Why don’t the expected numbers agree very well with the observed numbers? i.e. which assumption(s) of the Binomial Distribution are possibly being violated? (Note that the overall rate of spontaneous abortions in the observed data is in fact 84 out of 280 pregnancies or 30%) 14.1.3 Automated Chemistries (from Ingelfinger et al) At the Beth Israel Hospital in Boston, an automated clinical chemistry analyzer is used to give 18 routinely ordered chemical determinations on one order (glucose, BUN, creatinine, …, iron). The ``normal’’ values for these 18 tests were established by the concentrations of these chemicals in the sera of a large sample of healthy volunteers. The normal range was defined so that an average of 3% of the values found in these healthy subjects fell outside. Using the binomial formula [even if it is na&quot;{}ve to do so here], compute the probability that a healthy subject will have normal values on all 18 tests. Also calculate the probability of 2 or more abnormal values. Which of the requirements for the binomial distribution are definitely satisfied, and which ones may not be? Among 82 normal employees at the hospital, 52/82 (64%) had all normal tests, 19/82 (23%) had 1 abnormal test and 11/82 (13%) had 2 or more abnormal tests. Compare these observed percentages with the theoretical distribution obtained from calculations using the binomial distribution. Comment on the closeness of the fit. 14.1.4 Binomial or Opportunistic? (Capitalization on chance… multiple looks at data) [Question from Ingelfinger et al. textbook] Mrs A has mild diabetes controlled by diet. Blood values vary rapidly, so think of each day as a new (independent) situation. Her morning urine sugar test is negative 80% of the time and positive (+) 20% of the time. [It is never graded higher than +]. At her regular visits to her physician, the physician always asks about last 5 days. At this particular visit, she tells the physician that the test has been + on each of the last 5 days. What is the probability that this would occur if her condition has remained unchanged? Does this observation give reason to think that her condition has changed? Is the situation different if she observes, between visits, that the test is positive on 5 successive days and phones to express her concern? 14.1.5 Can one influence the sex of a baby? This question was prompted by this article in an 1979 issue of the New England Journal of Medicine. Consider a binomial variable with \\(n = 145\\) and \\(\\pi = 0.528\\). Calculate the SD of, and therefore a measure of the variation in, the proportions that one would observe in different samples of 145 if \\(\\pi\\) = 0.528. [In other words, the SD of the sampling distribution of the sample proportion.] The following is abstracted from that NEJM article: The baby’s sex was studied in births to Jewish women who observed the orthodox ritual of sexual separation each month and who resumed intercourse within two days of ovulation. The proportion of male babies was 95/145 or 65.5% (!!) in the offspring of those women who resumed intercourse two days after ovulation (the overall percentage of male babies born to the 3658 women who had resumed intercourse within two days of ovulation [i.e. days -2, -1, 0, 1 and 2] was 52.8%). How does the SD you calculated above help you judge the findings? And why did you not have to substitute the \\(p=0.655\\) into the SD formula, and call it an SE of \\(p\\)? 14.1.6 It’s the 3rd week of the course: it must be Binomial In which of the following would \\(Y\\) not have a Binomial distribution? Why? The pool of potential jurors for a murder case contains 100 persons chosen at random from the adult residents of a large city. Each person in the pool is asked whether he or she opposes the death penalty; \\(Y\\) is the number who say ‘Yes.’ \\(Y\\) = number of women listed in different random samples of size 20 from the 1990 directory of statisticians. \\(Y\\) = number of occasions, out of a randomly selected sample of 100 occasions during the year, in which you were indoors. (One might use this design to estimate what proportion of time you spend indoors) \\(Y\\) = number of months of the year in which it snows in Montreal. \\(Y\\) = Number, out of 60 occupants of 30 randomly chosen cars, wearing seatbelts. \\(Y\\) = Number, out of 60 occupants of 60 randomly chosen cars, wearing seatbelts. \\(Y\\) = Number, out of a department’s 10 microcomputers and 4 printers, that are going to fail in their first year. \\(Y\\) = Number, out of simple random sample of 100 individuals, that are left-handed. \\(Y\\) = Number, out of 5000 randomly selected from mothers giving birth each month in Quebec, who will test HIV positive. You observe the sex of the next 50 children born at a local hospital; \\(Y\\) is the number of girls among them. A couple decides to continue to have children until their first girl is born; \\(Y\\) is the total number of children the couple has. You want to know what percent of married people believe that mothers of young children should not be employed outside the home. You plan to interview 50 people, and for the sake of convenience you decide to interview both the husband and the wife in 25 married couples. The random variable \\(Y\\) is the number among the 50 persons interviewed who think mothers should not be employed. 14.1.7 Tests of intuition A coin will be tossed either 2 times or 20 times. You will win $2.00 if the number of heads is equal to the number of tails, no more and no less. Which is correct? (i) 2 tosses is better. (ii) 100 tosses is better. (iii) Both offer the same chance of winning. Hospital A has 100 births a year, hospital B has 2500. In which hospital is it more that at least 55% of births in one year will be boys. 14.1.8 CI for proportion when observe 0/n or n/n Oncologists often first try out new experimental agents on patients with metastatic disease who have failed all standard therapies. One (old) rule for deciding to abandon an experimental agent was: if it was tried on 14 successive patients, but did not show anti-tumour in any of them, i.e. if it `struck out’ in all 14, yielding a response rate of 0/14. Their reasoning is that this poor result ``rules out’’ (with 95% confidence) the possibility that it would be active in more than 20% of future patients. In other words, the data are incompatible with any \\(\\pi &gt; 20\\%.\\) Check out their rule, by computing/obtaining a 95% 1-sided CI for \\(\\pi.\\) [Use a 1-sided CI if one is interested in putting just an bound on the probability of benefit or risk: e.g., what is upper bound on $= $ probability of getting HIV from HIV-infected dentist? See JAMA article ``If Nothing Goes Wrong, Is Everything All Right? Interpreting Zero Numerators’’ by Hanley and Lippman-Hand http://www.epi.mcgill.ca/hanley/Reprints/If\\_Nothing\\_Goes\\_1983.pdf and the rule of $3/n$' for a 1-sided 95\\% CI. See also the second example (with sample size 4 !) in the sectionin defense of (some) small studies’ in this article http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/Place\\_of\\_statistical\\_1989.pdf "],
["math.html", "Chapter 15 Mathematics 15.1 Notation 15.2 Powers, Logarithms and Anti–logarithms", " Chapter 15 Mathematics 15.1 Notation Variables and Subscripts Variable \\(Y\\) with \\(n\\) sample values denoted \\(y_1, y_2, ..., y_n\\) in order of entry; The “1”, “2”, … are called subscripts or indices. We use the letter i (or j) and the range “1 to n” to denote the n different \\(y\\) values and refer to the value of the ith \\(y\\) as “\\(y_i\\)”. Summation The term \\(\\Sigma y\\) (spoken: “sigma y” or “sum of y’s”) is used as a shorthand for the sum \\(y_1 + y_2 + \\dots + y_n\\). 15.2 Powers, Logarithms and Anti–logarithms The term \\(y^{1/2}\\) is shorthand for the square root of \\(y\\) or \\(\\sqrt{y}\\). Likewise, \\(y^{1/n}\\) denotes the n-th root of \\(y\\). \\(\\ln (y)\\) denotes the “natural log of \\(y\\)” or “log of \\(y\\) to the base e” i.e. log\\(_e(x)\\), where e is 2.718. Note: y must be positive; ln (\\(y\\)) ranges from -\\(\\inf\\) to +\\(\\inf\\).   ln (0.1) = -2.30; ln (1) = 0; ln (2) = 0.69; ln (10) =2.30 \\(\\ln(A \\times B) = \\ln(A) + \\ln(B); \\ \\ \\ \\ln(\\frac{A}{B}) = \\ln(A) - \\ln(B)\\) exp(\\(y\\)) is shorthand for \\(e^y\\) or “exponential of \\(y\\)” or the natural anti-log of \\(y\\). \\(y\\) ranges from -\\(\\infty\\) to +\\(\\infty\\). and exp(\\(y\\)) yields a positive value. eg. exp(-1) = 0.36; exp(0) =1; exp(.5) =1.64; exp(1) = 2.71… "],
["computing01.html", "Chapter 16 Computing Session 1 16.1 Objectives 16.2 Background to two datasets 16.3 Statistical/Computing Tasks 16.4 The p and q functions: an orientation 16.5 Shapes of Distributions 16.6 Exercises 16.7 SUMMARY", " Chapter 16 Computing Session 1 16.1 Objectives The ‘computing’ objectives are to learn how to use R to put series of observations into vectors, and how to plot one series against another. The ‘statistical’ objective of this exercise is to understand the concept of a distribution of a numerical characteristic (here an amount of elapsed time), and the various numbers describing its ‘central’ location and spread, and other ‘landmarks’. You will also be introduced (in the next section) to 2 functions that give a more complete description of a distribution. 16.2 Background to two datasets 16.2.1 Climate Later on, when we get to regression models, we will examine climate trends using unusual datasets, which suggest that over the last few centuries, winter tends to end earlier, and plants tend to flower earlier. One such dataset arose as part of a long-running contest, the Nenana Ice Classic More here. Here is the 2020 brochure. When this exercise was constructed, in March 2020, is was time to compete this year, but it looked like you needed to be in Alaska to find and fill out a ticket, and the organizers asked for your mailing address, not your email one! 16.2.2 Ages of Cars One measure of the state of, and confidence in, a country’s economy is the monthly, or quarterly or yearly, numbers of new car registrations, such as these data from government e.g., and commercial sources, e.g., for Canada, the USA, and the UK. Another, which we will explore here, is the age distribution of all registered cars. (Because it is the only one we could find with detailed age data) we will examine a dataset (‘VEH0126’) that contains the numbers, as of the end of 2019, of Licensed vehicles by make and model and year of manufacture, produced by the UK Department for Transport 16.3 Statistical/Computing Tasks 16.3.1 Guesses re Date of Ice Breakup You are asked to approximate and carefully examine the distribution of guesses in 2018, contained in the Book of Guesses for that year. The full book is available as the 7th tab in the 2020 website. If the full book takes too long to load, here are some excerpts from the book. For now, we will measure the guesses (and eventually the actual time) as the numbers of days since the beginning of 2018. Thus a guess of Tuesday April 17 5:20 p.m. would be measured as 31 + 28 + 31 + 16 + (17 + 20/60)/24 = 106.7208 days since the beginning of 2018. It would be tedious to try to apply optical character recognition (OCR) to each of the 1210 pages in order to be able to computerize all of the almost 242,000 guesses. Instead, you are asked to reconstruct the distribution of the guesses in two more economical ways: By determining, for each of the 36 x 2 = 72 half-days days from April 10 to May 15 inclusive, the proportion, p, of guesses that are earlier than mid-day and midnight on that date. [ In R, if p = 39.6% of the guesses were below xy.z days, we would write this as pGuessDistribution(xy.z) = 0.396. Thus, if we were dealing with the location of a value in a Gaussian (‘normal’) distribution, we would write pnorm(q=110, mean = , sd = ) ] Once you have determined these 72 proportions (p’s), plot them on the vertical axis against the numbers of elapsed days since the beginning of the year on the horizontal axis. Thus the horizontal axis runs from 92 + 10 = 102 days to 92 + 30 + 15 = 137 days. By determining the 1st, 2nd, … , 98th, 99th percentiles. These are specific examples of ‘quantiles’, or q’s. The q-th quantile is the value (here the elapsed number of days since the beginning of 2018) such that a proportion q of all guesses are below this value, and 1-q are above it. [ In R, if 40% of the guesses were below 110.2 days, we would write this as qGuessDistribution(p=0.4) = 110.2 days. Thus, if we were dealing with the 40th percentile of a Gaussian distribution with mean 130 and standard deviation 15, we would write qnorm(p=0.4, mean = 130, sd = 15). ] Once you have determined them, plot the 99 p’s (on the vertical axis) against the 99 (elapsed) times on the horizontal axis. [see the spreadsheet]. 16.3.2 How old are UK cars? Below, you are asked to summarize the age distribution of all of the cars registered in 2019. Since the datafile (which you can open in OpenOffice or in Excel) has separate sheets for different types of vehicles (cars, motorcycles, buses, etc.) you will need to save the ‘cars’ sheet as a ‘.csv’ (comma separated values) file. Besides some header material (which you can skip using the skip argument in the read.csv' command inR`) it has as may rows as there are models (approx 40,000), and a ‘count’ column for every year of manufacture, back to 1900 or so, as well as a total across all of these years.k But first, before we get to the actual exercises, an orientation … 16.4 The p and q functions: an orientation The ‘p’ function tells us, for a given value of the characteristic, what proportion of the distribution lies to the left of this specified value. The ‘q’ (or quantile) function tells us, for a given proportion p, what is the value of the characteristic such that that specified proportion p of the distribution lies to the left of this ‘q’ value. In the plot below, the values of the p function are shown on the vertical axis, in red, against the (in this case, equally-spaced) values of the characteristic, shown on the horizontal axis. You enter on the horizontal axis, and exit with an answer on the vertical axis. The q function (in blue) goes into the opposite direction. You enter at some proportion on the vertical axis, and exit with a value of the characteristic (a quantile) on the horizontal axis. In our plot, the proportions on the vertical axis are equally-spaced. Percentiles and quartiles are a very specific sets of quantiles: they are obtained by finding the values that divide the distribution into 100 or into 4. 16.5 Shapes of Distributions There are a lot of misconceptions here. Part of the confusion stems from not distinguishing distributions of individual values (the theme in this chapter) from the (conceptual) distributions of statistics (summaries such as means, proportions, regression slopes) computed from samples of individual values. Distributions of the second type are called sampling distributions, and their shapes cannot be observed … unless we simulate them, or work them out using the mathematical statistics. Otherwise, they are unseen, virtual, conceptual. The sample size (the \\(n\\)) plays a central (pun intended) role in the shape, even more so than the shape of the distribution being sampled from. Sampling distributions are the topic of the next computing chapter. For now, in this chapter, we are only concentrate on distributions of individual values. The variation of these individual values can be due just to measurement error, or to genuine biological variation, or – in the case of the pedictions about the date of the ice break-up – differences in individual perrceptions and knowledge. The main points concerning the shape of a distribution of individual values are that: The size of the population has nothing to do with the shape. If we could measure the distribution of the diameters of one’s red blood cells, the size distribution should look the same whether you are smaller person or a larger person, or male or female. Moreover, the distribution of individuals’ red bllod cell concentrations would be the same in provinces/states/countries with larger/smaller populations of persons. There is no default shape. The shape is determined by the context This is especially the case if the values are determined/generated by humans and human behaviour/choice and by circumstances. A good example is the distribution of finishing times of marathon runs. In one setting you might find peculiarities such as seen in this dataset. In another, it might have very different shapes in those who are and are not elite runners and whether there is a ‘qualifying’ time requirement – see ‘Elite and the rest of us’ in the link. Another example are the distributions of hospital size (no. of beds) in different countries or states. [Likewise with the sizes of schools and universities] These might be determined by government policy. In former communist countries, there are often cookie-cutter hospitals, all the same size within a region, and given the names hospital # 1, hospital # 2, etc. In other countries, they might reflect variations in population density, or historical reasons. The year-of-manufacture distribution of cars registered in the the UK has some ‘person-made’ (and mainly ‘man’-made) non-smoothness. Debate continues as to whether the further ‘irregularities’ that will be evident by the end of 2020 were man-made or nature-made. Yet another examples are age-distributions: many factors determine the age-structure of a country or region of a country. It used to be like a pyramid, and still is in less developed countries. See here for some historical and modern Canadian and world examples. The distribution of age-at-death is determined by the age-distribution of the living – and by the many factors that drive age-specific death rates. The (sex-specific) distribution of adult heights of a homogeneous population might be close to symmetric (and even close to Gaussian), since the heights are determined by a large number of both ‘nature’ and ‘nurture’ factors. But, the (sex-specific) distribution of adult weights would not. Weight is determined in part by height, but also by a large number of ‘elective’ factors that are determined by one’s own lifestyle, and by the environment one lives in. In over-nourished North America, the distribution has a long right tail, whereas in some under-nourished parts of the world, it would have a long left tail. Why does the distribution of blood levels of lead in women of child bearing age have a long right tail, and why is it better fiot by a log-‘normal’ than a ‘normal’ curve? Note that the shape would not change much if a bigger sample had been taken. The histogram would just be smoother. Might it have to do with subpopulations of persons with different other sources of exposure besides Glasgow’s drinking water? For older distributions of blood lead, see here. For modern ones, use Google. The distribution of the lengths of flight delays might well be determined by competition, regulation, location, etc. ‘Then God said, ’Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.’ Thus God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament; and it was so. And God called the firmament Heaven. So the evening and the morning were the second day’ (Genesis 1:6-8). Did ‘God’ try to make it so that the distribution of the depths of the ocean would have a Gaussian curve? The distribution is a property of the population or the owner. The testing for normality that is so common needs to stop. There are a few individual-based contexts (such as with growth curves, where the percentile where an individual is located on a growth curve critical, or in tests to detect doping in sports, where an athlete’s test value is located in a reference distribution) where the shape is critical. However, when we are interested in estimating the mean or another measure of (covariate-patten-specific) distributions, the shape of the distribution of individaul values is not that relevant. See here for shapes – some odd – of distributions of individual values – and material on graphics. 16.6 Exercises 16.6.1 Guesses in Nenana Ice Classic Refer again to the guesses about the time of the ice-breakup. Once you have determined the 72 (cumulative) proportions (p’s) associated with the 72 half-days, plot them on the vertical axis against the numbers of elapsed days since the beginning of the year on the horizontal axis. Thus the horizontal axis runs from 92 + 10 = 102 days to 92 + 30 + 15 = 137 days. The 1st, 2nd, … , 98th, 99th percentiles are not so easy to determine since you have to locate the 2419th, 4839th, 7258t, … entries in the 1201-page Book of Guesses and plot the 99 p’s (on the vertical axis) against the 99 (elapsed) times (q’s) on the horizontal axis. Instead, use the first entry on each of pages 11, 21, … in this excerpt. [see the shared spreadsheet]. Using a different colour, plot these slightly-more-dense quantiles on the horizontal axix against the following percentages: entries = 200*seq(10,1200,10) + 1 percent = 100 * entries/241929 noquote( paste(head(round(percent,1),10),collapse=&quot;%, &quot;) ) ## [1] 0.8%, 1.7%, 2.5%, 3.3%, 4.1%, 5%, 5.8%, 6.6%, 7.4%, 8.3 tail(round(percent,1),10) ## [1] 91.8 92.6 93.4 94.2 95.1 95.9 96.7 97.5 98.4 99.2 Compare the Q\\(_{25}\\), Q\\(_{50}\\), and Q\\(_{75}\\) obtained directly with the ones obtained by interpolation of the curve showing the results of the other method. Compare the directly-obtained proportions of guesses that are before (the end of) April 20, April 30, and May 10 with the ones obtained by interpolation of the curve showing the results of the other method. By successive subtractions, calculate the numbers of guesses in each 1/2 day bin, and make a histogram of them. From them, calculate the mean, the mode, and the standard deviation. How far off was the median guess in 2018 from the actual time? Answer in days, and (with reservations stated) as a percentage? {see the 2020 brochure} Why did the ‘experts’ at the 1906 English country fair do so much better that their Alaskan counterparts? Why was the shape of the distribution of guesses by Dutch casino goers so different from the English and Alaskan ones? Instead of measuring the guessed times from the beginning of the year, suppose that, as Fonseca et al did, we measure the guessed times from the spring equinox in Alaska, i.e. from 8:15 a.m. on Tuesday, March 20, 2018, Alaska time. In this scale, compute the mean guess, and the SD of the guesses. Suppose, again, we measure the guessed times from the spring equinox, but in weeks. In this scale, compute the mean guess, and the SD of the guesses. How much warmer/colder in Nov-April is Montreal than Nenana? (For a future assignment, but you can start thinking about how) From a random sample of 100 guesses from the book, estimate how many guesses in the entire book are PM. my.id = 800606 set.seed(my.id) n = 50 sample.entry.numbers = sample(x = 1:241929, size=n) sorted.sample.entry.numbers = sort(sample.entry.numbers) head(sorted.sample.entry.numbers,10) ## [1] 10542 17437 18351 21113 24086 28782 30055 32220 33162 36443 page.number = ceiling(sorted.sample.entry.numbers/200) within.page = sorted.sample.entry.numbers-200*(page.number-1) column.number = ceiling(within.page/100) row.number = within.page - 100*(column.number-1) dataset = data.frame(page.number,column.number,row.number) head(dataset) ## page.number column.number row.number ## 1 53 2 42 ## 2 88 1 37 ## 3 92 2 51 ## 4 106 2 13 ## 5 121 1 86 ## 6 144 2 82 tail(dataset) ## page.number column.number row.number ## 45 1087 1 80 ## 46 1097 2 3 ## 47 1121 1 16 ## 48 1131 1 55 ## 49 1175 2 52 ## 50 1181 2 30 Some more links on the ‘Wisdom of Crowds’ https://www.technologyreview.com/s/528941/forget-the-wisdom-of-crowds-neurobiologists-reveal-the-wisdom-of-the-confident/ https://www.all-about-psychology.com/the-wisdom-of-crowds.html http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf 16.6.2 Exercise: Ages of UK Cars Once you have read in the data from the .csv file Sum the row-specific (model-specific) totals, so as to arrive at the total of all registrations. Then look up the population of the UK, and calculate the number of registered cars per capita. Compare this with the corresponding figures for Canada and for the USA. Hint: If you have a vector v, or a row or a column in a data.frame, then this statement sum(v,na.rm=TRUE) will return the overall sum of all of the values in v. The na.rm = TRUE option excludes (removes) NA values: otherwise, any NA values make the sum NA as well. Sum the counts in each column, so as to obtain a vector of year-of-manufacture-specific frequencies. Plot these against the years of manufacture, and comment on the shape of the frequency distribution. Hint: If you have a data.frame (or matrix) called df, then this statement apply(df[,4:13], MARGIN=2, FUN=sum, na.rm=TRUE) will return, as a vector of length 10, the sums of the entries in columns 4 to 13 respectively. [If you specify MARGIN=1, it applies the sunsum` function to each row, and returns row totals] Apply the cumsum function in R to these frequencies, and plot the cumulative numbers against the year of manufacture. Comment on any any ‘remarkable’ (^) features. What do you think the graph will look like at thee nd of 2020? (^) In handrwitten medical charting, remarks are often numbered R\\(_1\\), R\\(_2\\), etc. see here. JH’s mother was always annoyed when the community nurse would say something was ‘unremarkable’, but it made sense: in their training, nurses are taught to document (remark on) any deviation from the normal. Just like so many adjectives today, tremarkable has lost some of its specificity. From this graph, visually estimate the \\(Q_{25}\\), \\(Q_{50}\\) and \\(Q_{75}\\) values. Hint: you might use the abline function to add 3 horizontal lines to help you. Also, to see what cumsum returns, try cumsum( c(1,2,3,4,5) ) or (for short) cumsum( 1:5 ). Use R to do formally what you did visually: find the first (earliest) year of manufacture such that the cumulative total exceeeds 25% (50%, 75%) of the total. The which function in R can help: for example, to find the earliest partial/cumulative sum of the vector v =c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) to exceed 55/2, you can use min(which( cumsum(v) &gt; 55/2 )) to find that it is the 7-th one. To see how itwhichworks, usewhich( cumsum(v) &gt; 55/2 )` first to see what it returns. It would make the statistics more useful (less particularistic) if you converted the scale from year of manufacture to age. How would this change of scale affect (a) the shape of the distribution (b) the \\(Q\\)’s? 16.7 SUMMARY 16.7.1 Computing Assigning values to objects via &lt;- or = Putting numbers into vectors via concatenation c( , , ) Putting a sequence of values into vectors via the seq() function Looking at the first n and the last n elements of an object via head(object,n) and tail(object) – if you omit the n, it defaults to 6 Making a new numerical value or vector of numerical values from existing ones via, e.g. via + , * ( multiplication), ^ power etc. Using str(object) to see the **str*ucture of an object Using plot(x,y) to plot an ‘x’ vector versus a ‘y’ vector. lines(),points()andtext()` can be added to an existing plot. The approx() function for fitting 16.7.2 Statistical Concepts and Principles Definition of the p (Cumulative Distribution) function of a random variable \\(Y:\\) \\[CDF_Y(y) = F_Y(y) = Prob( Y \\le y).\\] Definition of the q (quantile, or InverseCDF) function of a random variable \\(Y:\\) \\[ q_Y(p) : the \\ y \\ such \\ that \\ Prob( Y \\le q) = p.\\] The is no default shape for the distributions of random variables, especially those representing individual values generated by, or concerning, humans. The shape is determined by the context. "],
["computing02.html", "Chapter 17 Computing: Session No. 2 17.1 Objectives 17.2 Scientific background 17.3 Random Variation 17.4 When these Laws don’t apply 17.5 SUMMARY", " Chapter 17 Computing: Session No. 2 17.1 Objectives The ‘computing’ objectives are to learn how to use R to simulate random variation, and random variables visualize the consequences of aggregating independent random variables discover the statistical laws that govern the variability of combinations of independent observations The ‘statistical’ objectives of this exercise are to be introduced to parameters that measure the spread of the distribution of a random variable, whether it be an error distribution, or a biological one. learn (empirically, and heuristically) the statistical laws that govern how the spread of a linear combination (e.g, the sum, or the mean) of several (generically, \\(n\\)) independent random variates is related to the spread of the individual random variables. The ultimate objective is to be able to use these laws to help investigators, such as Henry Cavendish [see below], to (probabilistically) quantify ‘how far off’ their parameter estimates might be. Gelman and Hills distinguish the more common sampling model, where ‘we are interested in learning some characteristics of a population’ from a ‘measurement error’ model. We start with the simpler context. 17.2 Scientific background In Isaac Newton’s Principia, Book III, The System of the World, Proposition 10, Theorem 10 we read: ‘If the earth were not denser than the seas, it would emerge from those seas and, according to the degree of its lightness, a part of the earth would stand out from the water, while all those seas flowed to the opposite side. By the same argument the spots on the sun are lighter than the solar shining matter on top of which they float. And in whatever way the planets were formed, at the time when the mass was fluid, all heavier matter made for the centre, away from the water. Accordingly, since the ordinary matter of our earth at its surface is about twice as heavy as water, and a little lower down, in mines, is found to be about three or four or even five times heavier than water, it is likely that the total amount of matter in the earth is about five to six times greater than it would be if the whole earth consisted of water, especially since it has already been shown above that the earth is about four times denser than Jupiter.’     The fact that the average of Newton’s ‘five or six’ is very close to today’s value of the mean relative density of the Earth shows just how prescient he was. The mean density of the Earth was an extremely important quantity in early Renaissance science as it provided a strong clue as to planetary composition. Some of words above are taken from this article by astronomer David Hughes. Table 2 in the article shows ‘Values suggested for the mean density of the Earth, as a function of the date of publication’ starts with Newton’s 1687 guesstimate, and ends, (23 estimates later) with Heyl and Chrzanowski’s 1942 value. Five of the 24 estimates are accompanied by a \\(\\pm\\) value, but what these \\(\\pm\\) values signify is left unexplained. One of the 24 is Cavendish’s 1798 estimate, which he obtained by taking the mean of \\(n =\\) 29 density measurements derived using a torsion balance on 17 days in the months of August and September, and the following April and May. Cavendish’s ‘point estimate’ was 5.48. Since his’extreme results do not differ from the mean by more than 0.38,or \\(\\frac{1}{14}\\) of the whole’ […] Therefore, it seems very unlikely that the density of the earth should differ from the 5.48 by so much as \\(\\frac{1}{14}\\) of the whole.’ Here we have an effort, by no less than Isaac Newton, to give a numerical interval within which the true parameter value is likely to lie. To Cavendish, it seems very unlikely that the density of the earth should differ from the 5.48 by so much as \\(\\frac{1}{14}\\) of the whole.’ Our objective is to learn the statistical laws and tools, unknown in their time, to statistically quantify how far off the mark our modern statistical estimators are (un)likely to be – and to tighten Cavendish’s lower and upper bounds! 17.3 Random Variation 17.3.1 Measurement errors The ‘standard’ has since been replaced by fancier methods, but for now let’s imagine that every family in the world make their own independent physical copy (using say a length of string or a paper or cardboard strip) of the official 1 meter platimun bar that used to be stored in Paris. Suppose these copies had a measurement error of either +1 centimeter or -1 centimeter (with plus errors and minus errors being equally likely). Thus, these errors, or ‘deviations’ from the 100cm, average out to 0. Each squared deviation is 1, so the mean squared deviation (called the error variance) is also 1. Its square root, also 1 in this instance, is called the standard deviation of the errors, or \\(\\sigma_{e}\\) for short. Now imagine taking a random sample of \\(n\\) of these copies, and computing the sum and the mean of the \\(n\\) lengths. Although it is possible to mathematically enumerate/calculate the exact probalities that the sum or mean of the lengths of the \\(n\\) copies takes on the various possible values, we will instead use R to approximate the probabilities by simulation. The probabilities in the following Figure are based on large enough numbers of simulations that – while they don’t show the perfect symmetry they would exhibit if we worked them out mathematically – they give quite close approximations. The panels in the following figure shows the probability of obtaining various possible sums and various possible sample means. Clearly, the patterns of the probabilities are strong functions of \\(n.\\) Instead of a 2-point error distribution, the next Figure shows how variable the sample totals and sample means would be if the errors were distributed a bit differently, as in the first row below. There, a quarter of the measurements have errors of +\\(\\sqrt{2} \\approx\\) +1.4cm, half have no error, and a quarter have errors of -\\(\\sqrt{2} \\approx\\) -1.4cm. Thus the error variance, the average of the squared deviations, is \\[\\sigma_e^2 = \\frac{1}{4} \\times (-1.4)^2 + \\frac{1}{2} \\times (0)^2 + \\frac{1}{4} \\times (1.4)^2 = 1,\\] so that the standard deviation is again \\(\\sigma_e = 1.\\) The row- (i.e., n-) specific distributions in the two Figures are not exactly the same. For example, the possible means in samples of size \\(n\\) = 2 have a 3-point distribution in the first one, but a 5-point distribution in the second one. But, as you will be asked to verify in the exercises below, the row-specific variances are the same in the two figures. More important than this are the statistical laws governing how widely the sums deviate from \\(\\mu_{sum}\\) = \\(n \\times\\) 100cm, and the means deviate from \\(\\mu\\) = 100cm. Clearly, the possible sums of 5 copies have a wider spread than the possible sums of 2, and the sums of 10 a wider spread than the sums of 5. Conversely, the means of 5 copies have a narrower spread than the means of 2, and the means of 10 a narrower spread than the means of 5. Instead of just telling you what the laws are, we ask you to use R to discover them yourself. 17.3.1.1 Discovering the Laws [via this computing exercise] Put the two (equally likely) errors (or deviations), i.e., -1cm and +1cm, into an R vector of length 2. [By the way, c(a,b) in R means concatenate a and b into a vector.] Then, make a new vector containing the squares of these deviations. [you can use vector * vector or vector^2]. Then use the built-in R function mean to compute the mean squared deviation. Although it is not needed in this case, use the round function to display the average squared deviation to a suitable number of decimals. Since what you have computed is a variance', usethisvariance (or better still, error.variance as the name for the mean squared deviation. Finally, check by hand that the calculation is correct. Use the sqrt() function (or the ^0.5 power) to compute the standard deviation of the errors. Change the errors from -1cm and +1cm to say -5cm and +5cm, and repeat steps 1 and 2. From this, what did you learn about the laws goverming variances and standard deviations? Add the errors onto 100cm to make 2 measurements (imperfect copies) of the meter bar, and calculate the mean, variance and standard deviation of the 2 measurements. From this, and by varying the sizes of the 2 errors, what did you learn about the mean the variance and the standard deviation of a shifted (re-located) random variable? {Hint: be careful to use your own variance and standard deviation functions, not the inbuilt var and sd functions. The reason is that whereas we say there are just 2 errors, in reality there are as many as there are copiers – effectively an infinite number, about half of whom will make an error of +1cm, and about half of whome will an error of -1cm. Or you can say that the probabilities of errors of -1cm and +1cm are 0.5, and 0.5. Change the -1cm and +1cm errors to -10mm and +10mm, and tepeat steps 1 and 2. What did you learn? (In passing: At St Hubert airport on Montreal’s South Shore from 1928 to 2004, over the 68 years where records were kept, the mean temperature for the month of January was recorded. The mean of these 68 values was -6.6 degrees Celsius (C) and the standard deviation was 2.9 degrees Celsius. Convert these to degrees F [Hint: \\(F = 32 + (9/5) \\times C\\)]) In order to see the laws in action, and figure them out (with the aid of the R code below) Begin with many simulated random pairs of measurements of (possibly imperfect copies) of the meter bar. For example, you might specify no.of.pairs = 10000. Then, using the 2-point (-1cm, +1cm) error distribution shown in the first Figure, simulate this large number of pairs of measurements. There are almost as many ways to do this as there are R programmers. One way (looking ahead to when we want to generalize and simulate larger samples) would be to use a matrix, i.e. a 2-way array, with as many rows as there are pairs (sets), and as many columns as the number of measurements per sample (here 2, but adjustable as you go along). See below. For each of these simulated samples of size \\(n\\) = 2, calculate the sample sum and sample mean. The apply function is very helpful here: you tell it to apply the desired function (FUN) separately for each row of the matrix by specifying MARGIN = 1. (Specifying MARGIN = 2 would give you a separate result for each column.) Now (finally) calculate the spread of the sample sums and sample means. Do so using both the standard deviation, and its square (the variance). The latter is not a natural quantity for non-statistians, but, as you will deduce, it is variances that scale with \\(n\\). Repeat these steps for samples of size \\(n\\) = 1, 3, 4, 5, 6, 7 , 8, 9, and 10. and plot the variances and sd’s against \\(n.\\) What laws do these plots suggest? According to Stephen Stigler, a historian of statistics, understanding of this law is one of the things that separates statisticians from mathematicains and computer scientists. Indeed, it is the second of what he calls the Seven Pillars od Statistics: The first recognition that the variation of sums did not increase proportionately with the number of independent terms added (and that the standard deviation of means did not decrease inversely with the number of terms) dates to the 1700s. This novel insight, that information on accuracy did not accumulate linearly with added data, came in the 1720s to Abraham de Moivre as he began to seek a way to accurately compute binomial probabilities with a large number of trials. In 1733 this would lead to his famous result, which we now call the normal approximation to the binomial, but by 1730 he already noticed that a crucial aspect of the distribution was tied to … [Stigler The Seven Pillars of Statistical Wisdom. Chapter 2. Its Measurement and Rate of Change.] His story of The Trial of the Pyx dramatically illustrates the early and continued blindness to the correct form of the laws. But he doesn’t think that Newtom, who was Master of the Mint for many years, took advantage of this blindness to become rich. possible R code: ERRORS = c(-1,1) no.of.pairs = 10000 means.samples.of.size.2 = rep(NA,no.of.pairs) measurements = matrix(100+sample(ERRORS,size = 2*no.of.pairs, replace = TRUE), nrow = no.of.pairs, ncol=2) str(measurements) ## num [1:10000, 1:2] 99 101 99 99 99 99 101 99 101 99 ... head(measurements,4) ## [,1] [,2] ## [1,] 99 99 ## [2,] 101 99 ## [3,] 99 101 ## [4,] 99 99 tail(measurements,4) ## [,1] [,2] ## [9997,] 101 99 ## [9998,] 101 101 ## [9999,] 99 99 ## [10000,] 101 99 sums.samples.of.size.2 = apply(measurements,MARGIN=1,FUN=sum) str(sums.samples.of.size.2) ## num [1:10000] 198 200 200 198 198 200 200 198 202 198 ... means.samples.of.size.2 = apply(measurements,MARGIN=1,FUN=mean) tail(sums.samples.of.size.2,4) ## [1] 200 202 198 200 tail(means.samples.of.size.2,4) ## [1] 100 101 99 100 table(sums.samples.of.size.2) ## sums.samples.of.size.2 ## 198 200 202 ## 2529 4941 2530 round( mean(sums.samples.of.size.2), 2) ## [1] 200 round( var(sums.samples.of.size.2), 2) ## [1] 2.02 round( sd(sums.samples.of.size.2) , 2) ## [1] 1.42 table(means.samples.of.size.2) ## means.samples.of.size.2 ## 99 100 101 ## 2529 4941 2530 round( mean(means.samples.of.size.2), 2) ## [1] 100 round( var(means.samples.of.size.2), 2) ## [1] 0.51 round( sd(means.samples.of.size.2) , 2) ## [1] 0.71 References David Hughes. The mean density of the Earth. Journal of the British Astronomical Association, Vol. 116, No. 1, p.21. 2006 17.3.2 Biological variation 17.3.2.1 Example We move now to contexts where the variation is primarily (or in a few deluxe cases where there are no measurement issues, entirely) due to genuine – e.g., biological or geographical – variation. Below, you will address one such example, the weights of a specific population where we have good national-level data to help us to set up simulations that demonstate and let us discover the statistical laws governing the (sampling) variability of various statistics derived from samples. For for now, we will consider a demographic characteristic, namely age, in a setting where it was documented for an entire population. The ages were gathered on the night of Sunday April 2nd in the 1911 Census of Ireland, the last one to be carried out under British rule. Here are the returns of one famous statistician, whose important work, published under the pen-name ‘Student,’ we will meet shortly. We will limit our attention to the county of Dublin, which at the time has a population of about half a million people, mostly urban. The male age-distribution displayed here in the usual vertical orientation has the (1/2) pyramid shape that characterizes developing countries. The somewhat different female age-distribution is in part because Ireland’s capital city, Dublin City, attracted many female workers in their late teens and their 20s (cf. the last 3 entries in Gosset’s return). The age-distributions today show large city vs. rural differences. Shapes of distributions Below, the age-distribution is rotated by 90 degrees, so that age is on the horizontal axis, and numbers of persons in the different age-bins (we will combine male and female) are on the vertical axis. In this more familiar orientation, it is easier to see that the age-distribution does not have the symmetry observed in the earlier section, where the variation was entirely due to measurement variations around some constant. (Incidentally, in the ‘age-distributions of today’s Irish population, there are none of the ’spikes’ at ages 40, 50, 60, .. that were seem in the 1911 data. The origin of these spikes is left for you to puzzle about. There are many modern examples of this phenomenon, as in this example of the recording of emergency department arrival and departure times . Another difficulty faced by earlier-century census takers is described in this account The Puzzled Census-Taker ) This – first of many – distribution of a human characteristic will serve as a strong messsage that we should not expect such distributions to be symmetric, let alone bell-shaped (‘Gaussian’ / ‘Normal’). The default stance should be that they are not. The same stance should apply to distributions of characteristics of ‘man-made’ or ‘human-made’ institutions – such as the distribution of the sizes of the hopitals or schools in a province or country. Nor should we expect symmetric distributions in the physical world, such as the distribution of the depths of the ocean, heights of land, lengths of rivers, daily temperatures in a region over a year, magnitudes of earthquakes, or intervals between eruptions of the Old Faithful geyser. Foe example, the distributions of the amounts of income tax paid by individuals in Quebec, the salaries of professional ice-hockey players, and the lengths of stay {LOS] of hospital patients do not have symmetric shapes (some distributions may even be multi-modal). Despite this, in some instances it may be critical to know the mean of the distibution, so that one can calculate the total revenue, or total payroll, or how much the savings would be if the mean LOS were reduced by 2%. We have to think about a summary appropriate to the situation. If you are representing the players, would you cite the median or the mean when telling the team-owners how poorly paid the ‘average’ player is? What if represented the owners, and wanted to show how much the team costs in payroll? Measures of ‘centre’ (Online, there are now many jokes about silly statistics and silly statisticians, such as on this site. A radio prgram used to end with &gt;Well, that’s the news from Lake Wobegon, where all the women are strong, all the men are good-looking, and all the children are above average. There are stories of a statistician drowning in a river that was 3 feet deep on average or being comfortable on average. One year in the 607 summer course for doctors, JH used some of these stories to warn against silly averages, and even quoted Francis Galton. Why do statisticians commonly limit their inquiries to Averages? It is difficult to understand why statisticians commonly limit their inquiries to Averages, and do not revel in more comprehensive views. Their souls seem as dull to the charm of variety as that of the native of one of our flat English counties, whose retrospect of Switzerland was that, if its mountains could be thrown into its lakes, two nuisances would be got rid of at once. [Natural Inheritance, 1889.] A urologist in the class gave a better example. The average person has 1 ovary and 1 testicle. Despite these jibes, there are some situations where it does make sense to talk about the mean value, even if nobody has a value close to the mean. When it comes to the age distribution of a population, the reports of national statistical agencies use a number of summary parameters, sometimes the median, sometimes the mean, and sometimes a ‘dependency ratio’ such as non-working-age to working-age numbers. For now, we will pursue the ‘mean’ parameter \\(\\mu\\). Later, we also pursue other numerical parameters for the centre. Part of the reason for our first choice is that the behavioural properties of the sample mean (an estimator of the {population mean_) are much easier to describe with standard statistical laws. Fortunately, nowadays, we have intensive computer techniques that use data-driven techniques rather than formulae-based ones, that can handle estimators of other parameters. Mean-Pursuit In the following R code, we simulate trying to estimate the mean age (\\(\\mu\\)), using just a random sample of \\(n\\) persons. Clearly, the \\(n\\)’s we will use are too small to give estimates that are ‘close enough for government work’ but the purpose is to understand what size \\(n\\) would ensure sufficiently precise estimates. In the following code, AGES refers to the ages (0 to 109) and Proportions the proportions of the population in each 1-year age bin. no.of.sims = 10000 ; # no. of samples of each size # enough to generate relatively smooth histograms sample.sizes = c(1,2,4,10,25) ; par(mfrow=c(length(sample.sizes),1),mar = c(0.5,1,0.5,1) ) for (n in sample.sizes ){ # loop over the various sample sizes ages = matrix(sample(AGES, # 1 row per simulation size = n*no.of.sims, # to save time, do all at once replace = TRUE, # only because data compressed prob = Proportions), # = FALSE if has indiv. data nrow = no.of.sims, ncol=n) # put into rows / columns if(n &gt; 1 &amp; n &lt;= 10){ print( noquote( paste(&quot;Ages of sampled persons in first 2 samples of size&quot;, toString(n)) ) ) print(head(ages,2)) } if( n == max(sample.sizes) ){ cat(&quot;The first panel shows the age-distribution of the entire population.\\n&quot;) cat(&quot;The remaining ones show the distributions of the sample sums and means.\\n&quot;) message(&quot;test&quot;) } # compute the row-specific (simulation-specific) sums and means # apply sum/mean to MARGIN=1, i.e., to each simulation (each row) sums.samples.of.size.n = apply(ages,MARGIN=1,FUN=sum) means.samples.of.size.n = apply(ages,MARGIN=1,FUN=mean) fr = table(sums.samples.of.size.n) # fr = frequency Y = max(Proportions*no.of.sims)/sqrt(0.8*n) # scale the y axis plot(fr,lw=0.4,xlim=c(0,n*(max(AGES)+12) ), ylim=c(-0.25,1)*Y, xaxt=&quot;n&quot;) text(n*105,0.55*Y,paste(&quot;n =&quot;,toString(n)),cex=2,font=3,adj=c(0,1)) for(a in seq(0,100,5)) { text(n*a, -0.01*Y, toString(n*a),adj=c(0.5,1),cex=1.5) txts = paste(&quot;Sum of&quot;,toString(n),&quot;Ages&quot;) if(n==1) txts = &quot;Individual Ages&quot; if(a==100 ) text(n*105, -0.01*Y, txts,adj=c(0,1),cex=1.5) if(n &gt; 1) text(n*a, -0.15*Y, toString(a),adj=c(0.5,1),font=4,cex=1.5) if(a==100 &amp; n &gt; 1) text(n*105, -0.15*Y, paste(&quot;Mean of&quot;,toString(n),&quot; Ages&quot;),adj=c(0,1),font=4,cex=1.5) } # how big is the spread (sd) of the simulated sums and means ? sd.sums = round( sd(sums.samples.of.size.n), 1 ) sd.means = round( sd(means.samples.of.size.n),1 ) txt.s = paste( &quot;SD of Sum:&quot;, toString(sd.sums) ) if(n==1) txt.s = paste(&quot;SD of Individual Ages:&quot;,toString(sd.sums) ) txt.m = paste(&quot;\\n\\n SD of Mean:&quot;,toString(sd.means)) if(n==1) txt.m = &quot;\\n\\n &quot; text(n*mu + sd.sums,Y*0.7, paste(txt.s,txt.m), cex=1.5,adj=c(0,0.5) ) points(n*mu,0,pch=19,col=&quot;red&quot;,cex=1.5) if(n==1){ text(A.50-0.1,0.95*Y,&quot;50% &lt;- | -&gt; 50%&quot;,adj=c(0.5,1),cex=1.5,col=&quot;blue&quot;) segments(A.50-0.1,0.95*Y, A.50-0.1,0,col=&quot;blue&quot;) text(115,Y, &quot;Reported Ages of Population of Dublin\\nIrish Census of 1911&quot;, adj=c(1,1),cex=1.25) } } ## [1] Ages of sampled persons in first 2 samples of size 2 ## [,1] [,2] ## [1,] 18 28 ## [2,] 0 45 ## [1] Ages of sampled persons in first 2 samples of size 4 ## [,1] [,2] [,3] [,4] ## [1,] 7 30 21 25 ## [2,] 15 50 38 58 ## [1] Ages of sampled persons in first 2 samples of size 10 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 81 7 25 58 14 21 37 74 23 17 ## [2,] 44 3 8 69 14 34 11 33 61 22 ## The first panel shows the age-distribution of the entire population. ## The remaining ones show the distributions of the sample sums and means. 17.3.2.2 Exercises Based on the numbers in the 5 panels, derive the statistical law that connects the spreads of the sampling distributions of the sample sum to the spread of the individual ages. (Since the calculated sd’s are based on a finite set of simulations, the numbers may not fit the law exactly ; also, the sd’s shown are rounded) Likewise, state the statistical law that connects the spreads of the sampling distributions of the sample mean to the spread of the individual ages. Use this law to predict the spread of the sampling distribution if we were to use a sample size of \\(n\\) = 100. What \\(n\\) would you need to have so that the (approx. 95%) Margin of Error, i.e., 2 times the SD of the mean (or 2 times the ‘Standard Error of the Mean’ or 2 times the ‘SEM’) is less than (a) 1 year (b) 0.5 years? Are these laws the same as the ones that apply when the only source of variation is measurement error? In the ‘measuring the meter bar’ example, the error distribution was symmetric; in this example, the ages do not have a symmetric distribution: it has a long right tail. Describe how the shape of the (sampling) distribution changes with the sample size involved. Look online for the name of this law or theorem. Let’s go back and help Cavendish with his Margin of Error. Calculate the standard deviation of his 29 individual measurements. [Hint: Don’t call it \\(\\sigma\\), since \\(\\sigma\\) refers to an infinite set of possible errors, and is therefore unknown. Call it \\(s\\), the conventional name for standard deviation of the individual values in a sample, sometimes abbreviated to sample standard deviation. It is an estimate of \\(\\sigma\\), so you are allowed to call it \\(\\hat{\\sigma}\\) or ‘sigma-hat’.] Now ‘plug-in’ the \\(s\\) value into the \\(\\frac{\\sigma}{\\sqrt{n}}\\) formula and calculate \\(\\frac{s}{\\sqrt{29}}\\). You should refer to this as the Standard Error of the Mean, or SEM for short. For a sample size this large, approx. 2 times the SEM can be used as the 95% Margin of Error or ‘ME’. [‘Student’, whom we met above, and will meet again soon, worked out what (bigger) multiple Cavendish would need to use if he had say just 4 measurements, or maybe 9, or 19 measurements rather than 29. Why, do you think, did he suggested bigger multiples when \\(s\\) – and thus the SEM and the ME – are based on just a few measurements?] 17.3.3 Example 2 The frequency distribution of the self-reported weights of a population of adults is available here. ds = read.table(&quot;http://www.biostat.mcgill.ca/hanley/statbook/weightsEtc.txt&quot;) MEAN=round(weighted.mean(ds$Weight.lbs,w=ds$Freq)) VAR = sum( ds$Freq * (ds$Weight.lbs-MEAN)^2 ) / sum(ds$Freq) SD = sqrt(VAR) WEIGHTS = sort(unique(ds$Weight.lbs)) Freq = aggregate(ds$Freq,by=list(ds$Weight.lbs),sum)$x Proportions = Freq/sum(Freq) W.50 = WEIGHTS[min(which(cumsum(Proportions) &gt; 0.50))] mu = sum(WEIGHTS*Proportions) no.of.sims = 10000 ; # no. of samples of each size # enough to generate relatively smooth histograms sample.sizes = c(1,2,4,8,16) ; par(mfrow=c(length(sample.sizes),1),mar = c(0.5,1,0.5,1) ) for (n in sample.sizes ){ # loop over the various sample sizes weights = matrix(sample(WEIGHTS, # 1 row per simulation size = n*no.of.sims, # to save time, do all at once replace = TRUE, # only because data compressed prob = Proportions), # = FALSE if has indiv. data nrow = no.of.sims, ncol=n) # put into rows / columns if(n &gt; 1 &amp; n &lt;= 10){ print( noquote( paste(&quot;Weights (lbs) of sampled persons in first 2 samples of size&quot;, toString(n)) ) ) print(head(weights,2)) } if( n == max(sample.sizes) ){ cat(&quot;The first panel shows the weight-distribution of the entire population.\\n&quot;) cat(&quot;The remaining ones show the distributions of the sample sums and means.\\n&quot;) message(&quot;test&quot;) } # compute the row-specific (simulation-specific) sums and means # apply sum/mean to MARGIN=1, i.e., to each simulation (each row) sums.samples.of.size.n = apply(weights,MARGIN=1,FUN=sum) means.samples.of.size.n = apply(weights,MARGIN=1,FUN=mean) fr = table(sums.samples.of.size.n) # fr = frequency Y = max(Proportions*no.of.sims)/sqrt(0.75*n) # scale the y axis plot(fr,lw=0.4,xlim=c(n*100,n*(max(WEIGHTS)+50) ), ylim=c(-0.25,1)*Y, xaxt=&quot;n&quot;) text(n*320,0.55*Y,paste(&quot;n =&quot;,toString(n)),cex=2,font=3,adj=c(0,1)) for(w in seq(100,300,20)) { text(n*w, -0.01*Y, toString(n*w),adj=c(0.5,1),cex=1.5) txts = paste(&quot;Sum of&quot;,toString(n),&quot;Weights&quot;) if(n==1) txts = &quot;Individual Weights&quot; if(w==300 ) text(n*310, -0.01*Y, txts,adj=c(0,1),cex=1.5) if(n &gt; 1) text(n*w, -0.15*Y, toString(w),adj=c(0.5,1),font=4,cex=1.5) if(w==300 &amp; n &gt; 1) text(n*310, -0.15*Y, paste(&quot;Mean of&quot;,toString(n),&quot; Weights&quot;),adj=c(0,1),font=4,cex=1.5) } # how big is the spread (sd) of the simulated sums and means ? sd.sums = round( sd(sums.samples.of.size.n), 1 ) sd.means = round( sd(means.samples.of.size.n),1 ) txt.s = paste( &quot;SD of Sum:&quot;, toString(sd.sums) ) if(n==1) txt.s = paste(&quot;SD of Individual Weights:&quot;,toString(sd.sums),&quot;(lbs)&quot; ) txt.m = paste(&quot;\\n\\n SD of Mean:&quot;,toString(sd.means)) if(n==1) txt.m = &quot;\\n\\n &quot; text(n*mu + sd.sums,Y*0.7, paste(txt.s,txt.m), cex=1.5,adj=c(0,0.5) ) points(n*mu,0,pch=19,col=&quot;red&quot;,cex=1.5) if(n==1){ text(W.50-1,0.95*Y,&quot;50% &lt;- | -&gt; 50%&quot;,adj=c(0.5,1),cex=1.5,col=&quot;blue&quot;) segments(W.50-0.1,0.95*Y, W.50-0.1,0,col=&quot;blue&quot;) text(352,-0.175*Y, &quot;Reported Weights, US Adults, 2014-2018&quot;, adj=c(1,1),cex=1.25) } } ## [1] Weights (lbs) of sampled persons in first 2 samples of size 2 ## [,1] [,2] ## [1,] 120 110 ## [2,] 130 140 ## [1] Weights (lbs) of sampled persons in first 2 samples of size 4 ## [,1] [,2] [,3] [,4] ## [1,] 130 170 150 190 ## [2,] 200 200 150 150 ## [1] Weights (lbs) of sampled persons in first 2 samples of size 8 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 150 130 160 130 180 130 180 140 ## [2,] 130 130 170 150 130 200 220 150 ## The first panel shows the weight-distribution of the entire population. ## The remaining ones show the distributions of the sample sums and means. 17.3.3.1 Exercises If an elevator is designed to lift a maximum of 5,000 pounds, what is the probability that it will be overloaded by a random group of 25 persons? What maximum number of persons should you specify so that the probability is below 1 in a million? The weights of individuals do not have a Gaussian (“Normal”) distribution. Are you still comfortable using the Normal distribution for your calculations. Explain carefully. Explain also why the ‘random’ is key to being able to answer, and what the impact would be if it is not the case. 17.4 When these Laws don’t apply In these panels, it’s the same elevator, and the same population. But what’s going on with these elevator-loads? Is the safe to have a load of 25? ``` 17.5 SUMMARY 17.5.1 Computing Assigning values to objects via &lt;- or = Putting numbers (or character strings) into vectors via concatenation c( , , ) Putting repeated values into vectors via the rep() function Looking at the first n and the last n elements of an object via head(object,n) and tail(object) – if you omit the n, it defaults to 6 Making a new numerical value or vector of numerical values from existing ones via, e.g. via + , * ( multiplication), ^ power etc. Using built-in functions, such as mean(), sum(), sd() and var(), that operate on vectors, or on single numbers (or ‘element-wise’ on vectors), such as round() and sqrt() Making numerical arrays using the matrix function Taking values at random from a vector via sample() Using str(object) to see the **str*ucture of an object Using apply(object, MARGIN = , FUN = ) to apply a function to the spefified margins (1=rows,2=cols) of an matrix or of a (possibly higher-dimensional) array. Using table to create a vector, matrix or array of the frequencies (cell counts) corresponding to a single variable, or combination of variables Using plot(x,y) to plot an ‘x’ vector versus a ‘y’ vector. lines(),points()andtext()` can be added to an existing plot. A graphic can be split into a 2-way grid of panels by using a vector of the form c(nr, nc) as the input to mfrow or mfcol in the par statement. The bottom, left, upper, and right margins can be set using the mar parameter Consider a Gaussian distribution with a specified mean.value and standard deviation sd.value. Then the proportion of the distrution to that lies ot the left of a specified value q is given by the inbuilt R function pnorm(q, mean = mean.value, sd = sd.value). For example, if in a certain population IQ has a Gaussian distribution with a mean of 100, and a sd of 15, then using this R expression round( 100*pnorm(110, mean = 100, sd = 15) , 1 ) we can determine that 74.8 percent of the distribution would be below 110, and thus that 25.2 percent would be below 90. The middle 50% could be calculated using this expression round( 100*qnorm(c(1/4,3/4), mean = 100, sd = 15) , 1 ) to give 89.9, 110.1 17.5.2 Statistical Concepts and Principles Definition of the Standard Deviation \\(\\sigma,\\) (and its square, the ‘Variance’, \\(\\sigma^2\\), of a random variable \\(Y\\) with mean \\(\\mu\\). \\[ Var[Y] = \\sigma^2 = \\textrm{mean of } (Y - \\mu)^2 \\ ; \\ \\ \\ SD[Y] = \\sigma.\\] \\[ Var[Y \\pm a \\ constant] = Var[Y] ; \\ \\ \\ SD[Y \\pm a \\ constant] = \\sigma.\\] \\[ Var[Y \\times a \\ constant] = constant^2 \\ \\times \\ Var[Y] ; \\ \\ \\ SD[Y \\times a \\ constant] = |constant| \\times \\sigma.\\] Rules for Variances and SDs of sums of \\(n\\) independent random variables, say \\[ Var[ Y_1 + Y_2 + \\dots + Y_n] = \\sigma^2 + \\sigma^2 + \\dots + \\sigma^2 = n \\times \\sigma^2.\\] \\[ SD[ Y_1 + Y_2 + \\dots + Y_n] = \\sqrt{n} \\times \\sigma.\\] Rules for Variances and SDs of means of \\(n\\) independent random variables. \\[ Var\\bigg[\\frac{Y_1 + Y_2 + \\dots + Y_n}{n}\\bigg] = \\frac{1}{n} \\times \\sigma^2.\\] \\[ SD\\bigg[\\frac{Y_1 + Y_2 + \\dots + Y_n}{n}\\bigg] = \\sqrt{\\frac{1}{n}} \\times \\sigma.\\] Sums of \\(n\\) independent random variables are \\(\\sqrt{n}\\) times more variable than their individual components. [But on a relative scale, since the sum is proportional to \\(n\\) while its spread is proportional to \\(\\sqrt{n}\\), the percentage spread is a decreasing function of \\(n.\\) Means of \\(n\\) independent random variables are \\(\\sqrt{n}\\) times less variable than their individual components. The SHAPES of the sampling distributions of the sums and means of \\(n\\) independent random variables become more ‘Gaussian-looking’ with larger \\(n.\\) "],
["computing03.html", "Chapter 18 Computing: Session No. 3 18.1 Objectives 18.2 Exercises 18.3 Other Exercises (under construction)", " Chapter 18 Computing: Session No. 3 18.1 Objectives The ‘computing’ objective is to learn how to use R to evaluate probabilities by (Monte Carlo) simulation. Taks, and the R Functions/structures used include taking a sample of a specified size from the elements of a vector, either with or without replacement: sample, creating a matrix from the given set of values: matrix, forming a vector or array or list of values by applying a function to margins of an array or matrix: apply concatenating vectors after converting to character: paste creating a vector whose elements are the cumulative sums, products, minima or maxima of the elements of the argument: cumsum, cumprod , cummax and cummin determining which elements of a vector or data frame are duplicates of elements with smaller subscripts, by returning a logical vector indicating which elements (rows) are duplicates: duplicated removing duplicate elements/rows from a vector, data frame or array: unique, stacking columns beside each other: cbind (rbind stacks rows one under the other) The ‘statistical’ objective of this exercise is to learn some of the probability laws that which random processes by seeing how frequently certain events occur in computer simulations, and to deduce the laws from the (long-run) frequencies. 18.2 Exercises 18.2.1 Pooled Blood [variation on an exercise from Chapter 3 of Colton’s 1974 book Statistics in Medicine] ‘Each time an individual receives pooled blood products, there is a 3% chance of his developing serum hepatitis. An individual receives pooled blood products on 45 occasions. What is his chance of developing serum hepatitis?’ (JH to 1980s classes: Note that the chance is not 45 \\(\\times\\) 0.03 = 1.35! To keep it simple, assume that there is a 3% chance that a unit is contaminated and calculate the chance that at least one of the 45 units is contaminated.) \\(\\bullet\\) For this computing class, simulate several sets of 45 occasions and estimate the chance that at least one of the 45 units is contaminated. \\(\\bullet\\) By experimentation, see if you can arrive at the formula the epi607 students in the 1980s would have used. \\(\\bullet\\) By repeating the process with the same number of simulations (the same N.SIMS value), and then experimenting with this same number, come up with a rule of thumb for the number of simulations (the N.SIMS value) needed for you to ‘trust’ the first decimal place in your (Monte Carlo) estimate of the true probability. n.times = 45 ; prob.per.time = 0.03 ; N.SIMS = 100 sims = matrix(sample(c(&quot;+&quot;,&quot;-&quot;), size=N.SIMS*n.times, prob=c(prob.per.time,1-prob.per.time), replace=TRUE), N.SIMS, n.times) # N.SIMS rows, n.times columns noquote(apply(sims[1:10,],1,paste,collapse=&quot;&quot;)) ## [1] --------------------------------------------- ## [2] -------+-------------------------+-+--------- ## [3] ---------------------------+---------------+- ## [4] +---------------------------------------+---- ## [5] -------------------------+------------------- ## [6] --------------------------------------------- ## [7] -+--+---------------------------------------- ## [8] ----+---------------------------------------- ## [9] ---------------------+----------------------- ## [10] ------------------+-------------------------- str(sims) ## chr [1:100, 1:45] &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;+&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; ... positive = apply(sims==&quot;+&quot;,1,any) # applies the &#39;any&#39; function to each row str(positive) ## logi [1:100] FALSE TRUE TRUE TRUE TRUE FALSE ... proportion.of.sims.with.a.hit = mean(positive) num = sum(positive) cat(paste(&quot;Proportion.of.sims.with.a.hit: &quot;,toString(num),&quot;/&quot;,toString(N.SIMS),&quot; = &quot;,toString(proportion.of.sims.with.a.hit),&quot;;\\nfor a more stable estimate, use a larger N.SIMS&quot;,sep=&quot;&quot;)) ## Proportion.of.sims.with.a.hit: 69/100 = 0.69; ## for a more stable estimate, use a larger N.SIMS 18.2.2 Life Tables [1990] The following [conditional] probabilities are taken from the abridged life tables for males (M) and females (F) computed from mortality data for Québec for the year 1990, and published by the Bureau de la statistique du Québec: [the probabilities for 90-year olds have been modified!]. In a (current) life table, one takes current (in this case 1990) i.e. cross-sectional death rates and applies them to a fictitious cohort to calculate what % of the cohort would survive past various birthdays – if die these rates persisted – and to calculate the average age at death (also known as life expectancy at birth). ........ Probability that a person who lives to his/her ........ x-th birthday will die during next 10 years .......x .... .M ....... F ...... 0 ... 0.010 ... 0.008 ..... 10 ... 0.006 ... 0.002 ..... 20 ... 0.012 ... 0.004 ..... 30 ... 0.016 ... 0.007 ..... 40 ... 0.031 ... 0.017 ..... 50 ... 0.080 ... 0.042 ..... 60 ... 0.211 ... 0.104 ..... 70 ... 0.448 ... 0.259 ..... 80 ... 0.750 ... 0.585 ..... 90 ... 1.000 ... 1.000 Fill in the blanks in the following diagram, which shows the proportions of males and females who survive past their xth birthday (x = 0, 10, 20, 30, … 100) (these plots are called survival curves). Calculate, by successive subtractions^ or otherwise, the [unconditional] proportions [i.e. proportions of the entire cohort] who will die between their xth and x+10th birthdays (x = 0, 10, 20, 30, … 90). Plot them as histograms. In a later exercise, we will use these proportions to calculate life expectancy. e.g. Pr[Die after 70th birthday] = 0.wwww Pr{Die after 80th birthday] = 0.zzzz Pr[die between 70 and 80] = difference Figure 18.1: Life Tables based on abbreviated data from 1990 given in above table. The tabular orientation is ‘bottom-up’ rather than the conventional ‘top-down’ format, and side by side, in the spirit of population ‘pyramids’. Also unusually, the ‘survival surve’ for males has the surviving proportion on the horizonatl axis, and the ages on the vertiacl axis. But if you were to rotate it by 90 degrees to the right, it would have conventional survival curve orentation. For population pyramids, see https://www.populationpyramid.net/world/2019/ For dynamic ones, see https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/pyramid/pyramid.cfm?geo1=01&amp;type=1 18.2.3 Life Tables [2018] For males and females separately, extract the 22 [conditional] probabilities of dying within the (1, 4 or 5 year) age intervals shown in Table 3.9 of this Institut de la statistique du Quebec report. They can be found in the \\(q_x\\) column [ x: age; q\\(_x\\) : given survival to birthday x, probability of dying between birthday x and birthday x+a, where a, a=1, 4 et 5.] Use these to compute the surviving fractions at each of the birthdays shown in Table 3.9. As a double check on your answers, you have the \\(l_x\\) columns in Table 3.9 [l\\(_x\\): number surviving at x-th birthday.] Hint: instead of using a loop, you can use the ‘cumulative product’ function cumprod. For example, cumprod(c(0.9, 0.8, 0.7)) yields the vector of length 3, namely 0.9, 0.9 \\(\\times\\) 0.8 = 0.72, and 0.9 \\(\\times\\) 0.8 \\(\\times\\) 0.7 = 0.504. Plot the fractions against the ages, overlaying one curve on the other. Make sure to label the axes appropriately, and using large enough lettering that people at the right hand of the age scale, where most of the action is, can read them. Visually measure the median survival for each gender. Do you think the mean survival (the life-expectancy at birth) is smaller or larger than the median? Explain your reasoning. The best way to be sure is to have and plot the frequency distributions of the ages at death. Using the same logic as in question 2, derive these frequencies, and plot them. Note that the first 2 age-bins are narrower than the others, so make wahtever adjustments are needed. Why don’t these frequency distributions match those shown in Figure 3.5 of the report? Examine Figure 3.15 of the report, and explain why life expectancies calculated from single-calendar-year data can ‘wobble’ quite a bit from year to year. 18.2.4 A simplified epidemic [from Mosteller and Rourke Book] An infectious disease has a one-day infectious period, and after that day the person who contracts it is immune. Six hermits (numbered 1,2, 3, 4, 5, 6, or named A-F) live on an island, and if one has the disease he randomly visits another hermit for help during his infectious period. If the visited hermit has not had the disease, he catches it and is infectious the next day. Assume hermit 1 has the disease today, and the rest have not had it. In the pre-R era students were asked to throw a die to choose which hermit he visits (ignore face 1) Today you are asked to use the sample function in R to choose which hermit he visits. That hermit is infectious tomorrow. Then ‘throw’ (or use R) again to see which hermit he visits, and so on. Continue the process until a sick hermit visits an immune one and the disease dies out. Repeat the ‘experiment’ several times and find the average number who get the disease the probability that (the proportion of experiments in which) everyone gets the disease. Try to figure out a closed-form (algebraic) expression for the answer to the second part. one.random.sequence =function(n.hermits=6){ # max 26, for 1-letter names names = LETTERS[1:n.hermits] who.is.infectious.today = 1 trail = names[who.is.infectious.today] for(visit in (1:(n.hermits-1)) ) { who.infectious.hermit.visits = sample( (1:n.hermits)[-who.is.infectious.today], 1) trail = c(trail, names[who.infectious.hermit.visits]) } return(trail) } (s = one.random.sequence(6)) ; duplicated(s) ## [1] &quot;A&quot; &quot;F&quot; &quot;E&quot; &quot;E&quot; &quot;E&quot; &quot;B&quot; ## [1] FALSE FALSE FALSE TRUE TRUE FALSE N.SIMS = 100; n.hermits = 6 sequences = matrix(NA,N.SIMS,n.hermits) for(i.row in 1:N.SIMS ){ sequences[i.row,1:6] = one.random.sequence(n.hermits) } first.duplicate = function(x){ d = duplicated(x) result = length(x)+1 if(any(d)) result = min( (1:n.hermits)[duplicated(x)] ) return(result) } when.first.dup.happens = apply(sequences,1,first.duplicate) table(when.first.dup.happens) ## when.first.dup.happens ## 3 4 5 6 7 ## 21 39 23 16 1 18.2.5 Screening for HIV See Figure 1 ‘Meaning of Positive HIV Tests for HIV’) in the article Can we afford the false positive rate? Use R to redraw their Figure Add on top of it a curve for a population with a 1% prevalence. Add a curve for a prevalence of 0.16%, but assuming sensitivity is only 90%. 18.2.6 Duplicate Birthdays [The exercises are found at the end of this sub-section] For 13 years in the 607 course from 1981 to 1993, JH handed out a page with a 12 x 31 grid and asked the students to indicate their birthday (month and day). The numbers of students in the class (‘n’) varied from 19 to 52 (mean 32, median 33) The results, in ‘raw’ form here, may surprize you. In 10 of the 13 classes, there was at least one duplicate birthday. http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/LotteriesProbabilitiesHANLEY1984TeachingStatistics.pdf#page=3 By the way, the ‘Birthday Problem’ is a classic in probability classes. year=c(0.8+(1981:1984),0.8+(1986:1988), 1989.4,1989.8,0.4+(1990:1992),1993.8) n = c(27,37,33,20,21,19,39,31,31,52,43,49,32) at.least.one.duplicate.birthday = c(TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,TRUE,TRUE, FALSE,TRUE,TRUE,TRUE,TRUE) cbind(year,n,at.least.one.duplicate.birthday) ## year n at.least.one.duplicate.birthday ## [1,] 1981.8 27 1 ## [2,] 1982.8 37 1 ## [3,] 1983.8 33 1 ## [4,] 1984.8 20 0 ## [5,] 1986.8 21 1 ## [6,] 1987.8 19 0 ## [7,] 1988.8 39 1 ## [8,] 1989.4 31 1 ## [9,] 1989.8 31 0 ## [10,] 1990.4 52 1 ## [11,] 1991.4 43 1 ## [12,] 1992.4 49 1 ## [13,] 1993.8 32 1 sum(at.least.one.duplicate.birthday) ## [1] 10 cat(&quot;n&#39;s in the classes without a duplicate:&quot;) ## n&#39;s in the classes without a duplicate: n[!at.least.one.duplicate.birthday] ## [1] 20 19 31 Leaving out people born on Feb. 29, and assuming [FOR NOW] all of the 365 birthdays are equally likely, we have the following formula for the probability of NO duplicate birthday in a sample of \\(n\\). By subtracting this probability from 1 (a common ‘trick’ when working with the probability of ‘at least’ … ), we obtain the probability of AT LEAST ONE duplicate birthday: \\[Prob[all \\ n \\ birthdays \\ are \\ different] = \\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\dots \\times \\frac{365-(n-1)}{365}.\\] Let’s work this out (or tell R to work it out) for n’s all the way from 1 to 365, and show the probabilities for the 13 classes, overlaid on all of the probabilities for \\(n\\) from 1 to 60. prob.all.n.different = cumprod( (365:1) / rep(365,365) ) prob.all.different = round(prob.all.n.different[n],2) prob.at.least.1.duplicate = 1 - prob.all.different par(mfrow=c(1,1),mar = c(4.5,5,0,0)) plot(1:60,prob.all.n.different[1:60],type=&quot;l&quot;, cex.axis=1.25,cex.lab=1.25, xlab=&quot;n&quot;,col=&quot;blue&quot;,ylab=&quot;Probability&quot;) abline(h=seq(0,1,0.1),col=&quot;lightblue&quot;) abline(h=0.5,col=&quot;lightblue&quot;,lwd=2) abline(v=seq(0,60,5),col=&quot;lightblue&quot;) lines(1:60,1-prob.all.n.different[1:60],col=&quot;red&quot;) points(n,prob.all.different,pch=19,cex=0.5) points(n,1-prob.all.different,pch=19,cex=0.5) points(n,at.least.one.duplicate.birthday, pch=&quot;X&quot;,cex=1.0, col=c(&quot;blue&quot;,&quot;red&quot;)[1+at.least.one.duplicate.birthday]) text(35,1-prob.all.n.different[35], &quot;Prob. at least 1 duplicate&quot;, cex=1.5,col=&quot;red&quot;,adj=c(0,1.25)) text(35,prob.all.n.different[35], &quot;Prob. all n are different&quot;, cex=1.5,col=&quot;blue&quot;,adj=c(0,-0.25)) Why is the probability so much higher than you would naively think? If in a class of 30, we ask you what the probability is, you are likley to reason like this … ‘30 out of 365, so about 1 chance in 12 or so, or about 8%’. BUT, by reasoning this way, you are just thinking about the probability that YOUR birthday will match the birthday of someone else. But then EVERYONE ELSE has to go through the same calculation, so there are a lot of possibilities for a match, between someone else and someone else. An easy way to appreciate the large number opportunities there are for a match is to look at a random sample of 30 birthdays, and see if they contain a match. If you can’t sort them first, you will have to compare the 2nd with the 1st (thats 1), the 3rd with the 1st and the 2nd (2 more), the 4rd with the 1st and the 2nd and 3rd (3 more), and so on, so 1 + 2+ … + 29 = 435 in all! But if you are the first to call out your birth, and then look/listen for a match with YOUR birthday, thats just 29 others. So instead of 1/12, the probability of SOME match (anyone with anyone else) os a LOT higher than 1/12. (From the blue graph, it’s about 70%!). all.birthdays = read.table( &quot;http://www.biostat.mcgill.ca/hanley/statbook/BirthdayProbabilities.txt&quot;, as.is=TRUE) # (as.is=TRUE doesn&#39;t convert alphabetic variables into factors) plot(1:366,all.birthdays$prob, ylim=c(0,max(all.birthdays$prob)), cex = 0.5,pch=19, xlab=&quot;Days of Year (1=Jan 1, 366=Dec 31)&quot;) abline(h= c(0, 1/(365:366))) abline(v= seq(0,366,30),col=&quot;lightblue&quot;) sample.of.30 = sample(all.birthdays$Birthday,30,replace=TRUE) noquote(sample.of.30) ## [1] Jun 20 Mar 16 Mar 09 Oct 16 Jun 04 Mar 13 Oct 14 Nov 11 Feb 26 May 16 ## [11] Feb 14 Jul 03 Jun 26 Apr 24 Jun 23 Sep 06 May 21 Nov 10 Jul 20 Apr 23 ## [21] Oct 01 Mar 04 Jan 29 Dec 27 Aug 24 Jan 12 Aug 28 Aug 22 Oct 05 May 23 If you have the software, you would sort this list first, noquote( sort(sample.of.30) ) ## [1] Apr 23 Apr 24 Aug 22 Aug 24 Aug 28 Dec 27 Feb 14 Feb 26 Jan 12 Jan 29 ## [11] Jul 03 Jul 20 Jun 04 Jun 20 Jun 23 Jun 26 Mar 04 Mar 09 Mar 13 Mar 16 ## [21] May 16 May 21 May 23 Nov 10 Nov 11 Oct 01 Oct 05 Oct 14 Oct 16 Sep 06 so its easier to spot any duplicates. And, if this is too laborious to do manually, you can use the unique and duplicated functions in R unique.birthdays = unique(sample.of.30) ( n.unique = length(unique.birthdays) ) ## [1] 30 duplicated(sample.of.30) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE ( sample.of.30[ duplicated(sample.of.30) ] ) ## character(0) But, as computer science people will tell you, sorting large lists takes a lot of computing. Just think about how you would manually sort a list of 1 million or 1 billion! The book Algorithms to Live By explains this well. In the early 1980s, Loto Quebec did not realize that there is a high chance of duplicates when the sample size is sizeable, and, as we tell in our case III, they were embarrassed. We tell more of the story here. They now SORT the list first, to find duplicates. Exercises By splitting 4 successive nodes into FirstDuplicate = YES or NO, draw a probability tree for the first 4 birthdays announced (Such a tree, with each node splitting into an Infected hermit visit a Susceptible hermit or visit an immune/recovered hermit, can help you think through the simulated mini-epidemic. BTW: you hear a lot abot SIR models thse days, and you will have seen the ‘R’ category displated in many of the covid-19 epidemic graphs) Using the prob.all.n.different vector derived above, calculate, by successive subtractions^ or otherwise, the probability that the first duplicate will show up on the [\\(Y\\) =] 2nd, 3rd, … birthday announced. Plot the frequency distribution of \\(Y\\). By simulation, estimate the probability that ‘in 20 draws one will not obtain a duplicate’ ^^. In every class where this Birthday Problem is addressed, students object that leaving out Feb 29 and assuming all of the 365 birthdays are equally likely are not realistic. Births vary somewhat by season and more so by day of the week So, in reality, are the probabilities of at least one duplicate birthday higher or lower than you have calculated above? Or is it impossible to say in which direction they are moved? To investigate this use the all.birthdays dataset created in the R code above. you can visualize the variation in the 366 probabilites by using this statement plot(1:366,all.birthdays$prob, ylim=c(0,max(all.birthdays$prob)), cex = 0.5,pch=19, xlab=&quot;Days of Year (1=Jan 1, 366=Dec 31)&quot;) and drawing grids with these ones abline(h= c(0, 1/(365:366))) and abline(v= seq(0,366,30),col=&quot;lightblue&quot;). This distribution is based on all births in a certain country from 1990-1999 [the years when many of you were born]. From it, draw several samples of 20 birthdays, and calculate the proportion of these sets in which the 20 birthdays are unique, and conversely, contain at least 1 duplicate. Some R code is given below. Try to figure out from the features of the plot what country the birthday data are from? Explain your reasoning. Just from how much the 366 probabilities ‘wobble’ around 1/365 or 1/366, try to figure out (approximately, to within an order of magnitude) how many births were used to derive the 366 proportions. For example, is it a country with a population size such as China, India, USA, Germany, New Zealand, Iceland, Prince Edward Island, the Yukon Territory, etc? https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2013.00705.x ^ If \\(Y\\) = which draw produces 1st duplicate (so \\(Y\\) = 2,… 365). e.g. Pr[$Y$ &gt; 6] = Pr[Y = 7 or 8 or 9 or .. or 365 ] Pr[$Y$ &gt; 7] = Pr[Y = .... 8 or 9 or .. or 365 ] Difference: = Pr[Y = 7] N.SIMS=100 n = 23 #PROBS = rep(1/366,366) # ON for comparison PROBS = all.birthdays$prob # OFF for comparison B = matrix( sample(all.birthdays$Birthday, size = N.SIMS*n, prob = PROBS, replace=TRUE), nrow = N.SIMS, ncol = n) head(B,2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] &quot;Feb 09&quot; &quot;Jul 09&quot; &quot;Apr 02&quot; &quot;Sep 17&quot; &quot;Nov 27&quot; &quot;Oct 28&quot; &quot;Oct 16&quot; &quot;May 25&quot; ## [2,] &quot;Jul 17&quot; &quot;Mar 21&quot; &quot;Nov 29&quot; &quot;Jun 20&quot; &quot;Apr 03&quot; &quot;Dec 13&quot; &quot;Jan 18&quot; &quot;Jun 04&quot; ## [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] ## [1,] &quot;Feb 28&quot; &quot;Apr 24&quot; &quot;Nov 21&quot; &quot;Jun 05&quot; &quot;Jun 20&quot; &quot;Feb 08&quot; &quot;Jun 03&quot; &quot;Nov 30&quot; ## [2,] &quot;Jan 13&quot; &quot;Aug 21&quot; &quot;Feb 08&quot; &quot;Dec 06&quot; &quot;Aug 12&quot; &quot;Dec 06&quot; &quot;Nov 11&quot; &quot;May 10&quot; ## [,17] [,18] [,19] [,20] [,21] [,22] [,23] ## [1,] &quot;Jul 31&quot; &quot;Sep 18&quot; &quot;Oct 20&quot; &quot;May 16&quot; &quot;May 22&quot; &quot;Jan 02&quot; &quot;Nov 26&quot; ## [2,] &quot;Aug 07&quot; &quot;Nov 11&quot; &quot;Dec 14&quot; &quot;Dec 23&quot; &quot;Jul 28&quot; &quot;Oct 24&quot; &quot;Feb 13&quot; duplicate = apply(B,1, function(x) sum(duplicated(x)) &gt; 0 ) noquote(paste(sum(duplicate), &quot;duplicates /&quot;, N.SIMS, &quot; ; larger N.SIMS: more precise probability estimate.&quot;,collapse=&quot; &quot;)) ## [1] 52 duplicates / 100 ; larger N.SIMS: more precise probability estimate. 18.2.7 Chevalier de Méré [see here}(https://www.cut-the-knot.org/Probability/ChevalierDeMere.shtml) or Gamblers in 17th century France used to bet on the event of getting at least one 1 ‘six’ in four rolls of a die or in 1 roll of 4 dice [dice is plural for die]. As a more trying variation, two dice were rolled 24 times with a bet on having at least one double six. Chevalier de Méré who expected a couple of sixes to turn up in 24 double rolls with the frequency of a six in 4 single rolls. However, he lost consistently. Simulate a large number of each of these games, and estimate the probability of getting (a) at least one six in four rolls of a dice (b) at least one “double six” when a pair of dice is thrown 24 times Why does it take a large number of simulations to determine which game offers the better chance of winning? Can you work out the probabilities directly? https://www.casino-games-online.biz/dice/chevalier-de-mere.html 18.3 Other Exercises (under construction) 18.3.1 HIV transmission [ probability per act ] Napoleon Minard http://euclid.psych.yorku.ca/www/psy6135/tutorials/Minard.html Efron Monty Hall The Economist Wald : CF screening OJ p-value Adult soccer ball.. wrong probability Vietnam deaths NHL birthdays Buffon’s needle Estimating Pi Similar Adult size soccer ball Medical School admission rates, by gender. Vietnam war deaths, by month of birth NHL success, by month of birth John Snow, cholera deaths South London, by source of drinking water Galton Greenwood Mosteller Ages of books ngrams Prob of duplicate birthdays in a sample of \\(n\\) persons surviving from 80th to 85th birthday if the (conditional) 1-year at a time proportions dying (risks) are … Chevalier de Méré no 6, at least 1 six. Variation with and without constraints -3- “Duplicate Numbers” [mini-version of birthday problem] To appreciate the high probability of duplicate birthdays, take a simpler case of drawing single digit numbers at random from a Random Number Table or spreadsheet until one gets a duplicate. (also, try actually doing it to see how many draws it takes) a Calculate the probability that in 5 draws one will not obtain a duplicate, i.e., the probability of a sequence 1st# ; 2nd# [≠ 1st#] 3rd# [≠ 2nd# ≠ 1st#] 4th# [≠ 3rd# ≠ 2nd# ≠ 1st#] 5th# [≠ 4th# ≠3rd# ≠ 2nd# ≠ 1st#] b Calculate, by successive subtractions* or otherwise, the probability that the first duplicate will show up on the [Y =] 2nd, 3rd, …11th draw. Plot the frequency distribution of the # draws, Y, until a duplicate. Ice Breakup Dates Here are some details on the Nenana Ice Classic More here 18.3.2 The 2018 Book of Guesses We are keen to establish the distribution of guesses, with the guessed times measured from midnight on December 31, 2017. Thus a guess of April 06, at 5:15 p.m. would be measured as 31 + 28 + 31 + 5 + 12/24 + (5+ 15/60)/24 = 95.71875 days into the year 2018. It would be tedious to apply optical character recognition (OCR) to each of the 1210 pages in order to be able to computerize all 240,000 guesses. Instead, you are asked to reconstruct the distribution of the guesses in two more economical ways: By determining, for (the beginning of) each day, from April 01 to June 01 inclusive, the proportion, p, of guesses that predede that date. [ In R, if p = 39.6% of the guesses were below 110 days, we would write this as pGuessDistribution(110) = 0.396. Thus, if we were dealing with the location of a value in a Gaussian (‘normal’) distribution, we would write pnorm(q=110, mean = , sd = ) ] Once you have determined them, plot these 62 p’s (on the vertical axis) against the numbers of elapsed days (90-152) on the horizontal axis. By determining the 1st, 2nd, … , 98th, 99th percentiles. These are specific examples of ‘quantiles’, or q’s. The q-th quantile is the value (here the elapsed number of days since the beginning of 2018) such that a proportion q of all values are below this value, and 1-q are above it. [ In R, if 40% of the guesses were below 110.2 days, we would write this as qGuessDistribution(p=0.4) = 110.2 days. Thus, if we were dealing with the 40th percentile of a Gaussian distribution with mean 130 and standard deviation 15, we would write qnorm(p=0.4, mean = 130, sd = 15). ] Once you have determined them, plot the 99 p’s (on the vertical axis) against the 99 (elapsed) times on the horizontal axis. Compare the Q\\(_{25}\\), Q\\(_{50}\\), and Q\\(_{75}\\) obtained directly with the ones obtained by interpolation of the curve showing the results of the other method. Compare the directly-obtained proportions of guesses that are before April 15, April 30, and May 15 with the ones obtained by interpolation of the curve showing the results of the other method. By successive subtractions, calculate the numbers of guesses in each 1-day bin, and make a histogram of them. From them, calculate the mean, the mode, and the standard deviation. To measure the spread of guesses, Galton, in his vox populi (wisdom of crowds) article, began with the interquartile range (IQR), i.e. the distance between Q75 and Q25, the 3rd and 1st quartiles. In any distribution, 1/2 the values are within what was called the ’probable error (PE) of the mean; i.e., it is equally probable that a randomly selected value would be inside or outside this middle-50 interval. Today, we use standard deviation (SD) instead of probable error. In a Gaussian distribution, some 68% of values are within 1 SD of the mean, whereas 50% of values are within 1 PE of the mean. We can use R to figure out how big a PE is in a Gaussian distribution compared with a SD. By setting the SD to 1, and the eman to 0, we have Q75 = qnorm(p = 0.75, mean=0, sd=1) round(Q75,2) ## [1] 0.67 i.e, a PE is roughtly 2/3rds of a SD. Galton – convert to SD. Geometric mean Amsterdam study How far off was the median guess in 2018 from the actual time? Answer in days, and (with reservations stated) as a percentage? Why did the experts at the country fair do so much better? Where were the punters in 2019 wrt the actual ? https://www.technologyreview.com/s/528941/forget-the-wisdom-of-crowds-neurobiologists-reveal-the-wisdom-of-the-confident/ https://www.all-about-psychology.com/the-wisdom-of-crowds.html http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf [Nenana Ice Classic] Tanana River 18.3.3 Trends over the last 100 years fill in the data since 200x. cbind with … https://www.adn.com/alaska-news/2019/04/14/nenana-ice-classic-tripod-goes-down-setting-record-for-earliest-river-break-up/ time trends morning vs afternoon ? 2019 extreme… how many SD’s from the line? Where were the punters in 2019 wrt the actual ? Sources: 1917-2003 data in textfile on course website; http://www.nenanaakiceclassic.com/ for data past 2003 ascii (txt) and excel files with data to 2003 Working in teams of two … *Create a dataframe containing the breakup data for the years 1917-2007. Possible ways to do so include: directly from the ascii (txt) file; from the Excel file *From the decimal portion of the Julian time, use R to create a frequency table of the hour of the day at which the breakup occurred. *From the month and day, use R to calculate your own version of the Julian day (and the decimal portion if you want to go further and use the hour and minute) *Is there visual evidence that over the last 91 years, the breakup is occurring at an earlier date? *Extract the date and ice thickness measurements for the years 1989-2007 from the website and use (your software of choice) R to create a single dataset with the 3 variable, year, day and thickness. From this, fit a separate trendline for each year, and calculate the variability of these within-year slopes. Galton’s data on family heights These data were gathered to examine the relation between heights of parents and heights of their (adult) children. They have been recently ‘uncovered’ from the Galton archives. As a first issue, for this exercise, you are also asked to see whether the parent data suggest that stature plays “a sensible part in marriage selection”. For the purposes of this exercise, the parent data [see http://www.epi.mcgill.ca/hanley/galton ] are in a file called parents.txt , with families numbered 1-135, 136A, 136-204 ( {the heights of the adult offspring will be used in a future exercise) Do the following tasks using R Categorize each father’s height into one of 3 bins (shortest 1/4, middle 1/2, tallest 1/4). Do likewise for mothers. Then, as Galton did [ Table III ], obtain the 2-way frequency distribution and assess whether “we may regard the married folk as picked out of the general population at haphazard”. Calculate the variance Var[F] and Var[M] of the fathers’ [F] and mothers’ [M] heights respectively. Then create a new variable consisting of the sum of F and M, and calculate Var[F+M]. Comment. Galton called this a “shrewder” test than the “ruder” one he used in 1. When Galton first anayzed these data in 1885-1886, Galton and Pearson hadn’t yet invented the correlation coefficient. Calculate this coefficient and see how it compares with your impressions in 1 and 2. Temperature perceptions Create 5 datasets from the questionnaire data on temperature perceptions etc. by importing directly from the Excel file applied to .csv version of Excel file); by first removing the first row (of variable names) and exporting the Excel file into a ’comma-separated-values&quot; (.csv) text file, then … reading the data in this .csv file via the INFILE and INPUT statements in a SAS DATA step, [SAS] INFILE ‘path’ DELIMITER =“,”; INPUT ID MALE $ MD $ EXAM TEMPOUTC TEMPINC TEMPOUTF TEMPINF TEMPFEEL TIME PLACE $ ; by reading the data in the text file temps_1.txt into the SAS dataset via the INFILE and INPUT statements. Notice that the ‘missing’ values use the SAS representation (.) for missing values. or the Stata dataset using the ‘infile’ command by reading the data in the text file temps_2.txt via [in SAS] the INFILE and INPUT statements in a DATA step or [in Stata] the ‘infix’ command. Here you will need to be careful, since ‘free-format’ will not work correctly (it is worth trying free format with this file, just to see what goes wrong!). When using the INFILE method, you can control some of the damage by using the ‘MISSOVER’ option in the INFILE statement: this keeps the INPUT statement from continuing on into the next data line in order to find the (in our example) 11 values implied by the variable list. JH uses this ‘defensive’ option in ALL of his INFILE statements. by cutting and pasting the contents of the text file temps_2.txt directly into the SAS or Stata program - in SASthe lines of data go immediately after the DATALINES statement, and there needs to be a line containing a semicolon to indicate the end of the data stream. In Stata, the lines of data go immediately after the infile or infix statement, and there needs to be a line containing the word ‘end’ to indicate the end of the data stream This Cut and Paste Method is NOT RECOMMENDED when the number of observations is large, as it is too all too easy to inadvertently alter the data, and the SAS/Stata porogram becomes quite long and unwieldy. It is Good Data Management Practice to separate the program statements from the data. [Run [in SAS] PROC MEANS [in Stata] the ‘describe’ command, on the numerical variables, and [in SAS] PROC FREQ or [in Stata] the ‘tabulate’ command, on the non-numerical variables, to check that the 5 datasets you created contain the same information. Also, get in the habit of viewing or printing several observations and checking the entries against the ‘source’. When using (i), have SAS show you the SAS statements generated by the wizard. Store these, and the DATA steps for (ii) to (v) in a single SAS program file (with suffix .sas). Annotate liberally using comments: in SAS, either begin with * ; or enclose with /* … */ in Stata ..begin the line with * or place the comment between /* and */ delimiters or begin the comment with // or begin the comment with /// Q2 Use one of these 5 datasets, and the appropriate [in SAS, PROCs (see Exploring Data under UCLA SAS Class Notes 2.0)], or [in Stata, the list comamnd, and the analyses from the Statistics menu] to list the names and characteristics of the variables list the first 5 observations in the dataset list the id # and the responses just to q3, w5 and q6, for all respondents, with respondents in the order: female MDs, male MDs, female non-MDs, male non-MDs. Indicate the [sub-]statement that is required to reverse this order. create a 2-way frequency table, showing the frequencies of respondents in each of the 2 (MD nonMD) x 2 (male female) = 4 ‘cells’ (one defintion of an epidemiologist is ‘an MD broken down by age and sex’). Turn off all the extra printed output, so that the table just has the cell frequencies and the row and column totals. compare the mean and median attitude to exams in MDs vs. non-MDs (hint: in SAS, the CLASS statement may help). Get SAS/Stata to limit the output to just the ‘n’, the min, the max, the mean and the median for each subgroup. And try to also get it to limit the number of decimal places of output (in SAS the MAXDEC option is implememnted in some procedures, but as far as JH can determine not in all) compare the mean temperature perceptions (q6) of male and female respondents [in SAS] create a low-res (‘typewriter’ resolution) scatterplot of the responses to q5 (vertical axis) vs. q4 (horizonatal axis), using a plotting symbol that shows whether the responsdent is a male or a female. If we have not covered how to show this ‘3rd dimension’, look at the ONLINE Documentation file {the guide for most of the procedures covered in this set of exercises is in the Base SAS Procedures Guide; other procedures are in sthe more advanced ‘STAT’ module}. You can specify the variable whose values are to mark each point on the plot. See PLOT statement in PROC PLOT, and the example with variables height weight and gender. [in Stata] use the (automatically hi-res) graphics capabilities available from the ‘Graphics’ menu [if SAS] Put all of the programs for Q1, and all of these program steps and output for Q2 in a single .txt file (JH will use a mono-spaced font such as Courier to view it – that way the alignment should be OK), with PROC statements interleaved with output, and a helpful 2-line title (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line (that way, lines won’t wrap-around when JH views the material). [if Stata] paste the results and graphics into Word. NOTE: To be fair to SAS, it CAN produce decent (and even some publication-quality) graphics. See http://www.ats.ucla.edu/stat/sas/topics/graphics.htm Then submit the text file electronically (i.e., by email) to JH by 9 am on Monday October 2. Natural history of prostate cancer Q1 The following data items are from an investigation into the natural history of (untreated) prostate cancer [ report (.pdf) by Albertsen Hanley Gleason and Barry in JAMA in September 1998 ]. id, dates of birth and diagnosis, Gleason score, date of last contact, status (1=dead, 0=alive), and – if dead – cause of death (see 2b below). data file (.txt) for a random 1/2 of the 767 patients Compute the distribution of age at diagnosis (5-year intervals) and year of diagnosis (5 year intervals). Also compute the mean and median ages at diagnosis. For each of the 20 cells in Table 2 (5 Gleason score categories x 4 age-at-dx categories), compute the number of man-years (M-Y) of observation number of deaths from prostate cancer(1), other causes(2), unknown causes(3) prostate cancer(1) death rate [ deaths per 100 M-Y ] proportion who survived at least 15 years. For a and b you can use the ‘sum’ option in PROC means; ie PROC MEANS data = … SUM; VAR vars you want to sum; BY the 2 variables that form the cross-classification. Also think of a count as a sum of 0s and 1s. For c (to avoid having to compute 20 rates by hand), you can ‘pipe’ i.e. re-direct the sums to a new sas datafile, where you can then divide one by other to get (20) rates. Use OUTPUT OUT = …. SUM= …names for two sums; On a single graph, plot the 5 Kaplan-Meier survival curves, one for each of the 5 Gleason score categories (PROC LIFETEST .. Online help is under the SAS STAT module, or see http://www.ats.ucla.edu/stat/sas/seminars/sas_survival/default.htm. For Stata, see http://www.ats.ucla.edu/stat/stata/seminars/stata_survival/default.htm. [OPTIONAL] In order to compare the death rates with those of U.S. men of the same age, for each combination of calendar year period (1970-1974, 1975-1979, …, 1994-1999) and 5 year age-interval (55-59, 60-64, … obtain the number of man-years of follow-up and the number of deaths. Do so by creating, from the record for each man, as many separate observations as the number of 5yr x 5yr “squares” that the man traverses diagonally through the Lexis diagram [ use the OUTPUT statement within the DATA step]. Then use PROC MEANS to aggregate the M-Y and deaths in each square. If you get stuck, here is some SAS code that does this, or see the algorithm given in Breslow and Day, Volume II, page ___ Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday Nov 7. Serial PSA values Q1 These two files contain PSA values [pre-] and [post-] treatment of prostate cancer *. Create a ‘wide’ PSA file of 25 log-base-2 PSA values per man (some will be missing, if PSA not measured 25 times). Print some excerpts. (b)From the dataset created in (a), create a long file, with just the observations containing the non-missing log-base-2 PSA values [OUTPUT statement in DATA step]. Print and plot some excerpts. From the dataset created in (b), create a wide file [ RETAIN, first. and last. helpful here; or use PROC TRANSPOSE ]. Print some excerpts. The order of the variables is given in this sas program . Some of the code in the program may also be of help. Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday Nov 14. Graphics 1a Re-produce (or if you think you can, improve on) three of the graphs shown in “Examples of graphs from Medical Journals.” These examples are in a pdf file on the main page. Use Excel for at least one of them, and R/Stata/SAS for at least one other. Do not go to extraordinary lengths to make them exactly like those shown – the authors, or the journals themselves, may have used more specialized graphics software. You may wish to annotate them by making (and sharing with us) notes on those steps/options that were not immediately obvious and that took you some effort to figure out. Insert all three into a single electronic document. 1b Browse some medical and epidemiologic journals and some magazines and newspapers published in the last 12 months, Identify the statistical graph you think is the worst, and the one you think is the best. Tell us how many graphs you looked at, and why you chose the two you did. If you find a helpful online guide or textbook on how to make good statistical graphs, please share the reference with us. [The bios601 site http://www.epi.mcgill.ca/hanley/bios601/DescriptiveStatistics/ has a link to the Textbook by Cleveland and the book “R Graphics” by Paul Murrell. If possible, electronically paste the graphs into the same electronic file you are using for 1a. 2 [OPTIONAL] The main page has a link to a lifetable workbook containing three sheets. Note that the ‘lifetable’ sheet in this workbook is used to calculate an abridged current life table based on the 1960 U.S. data. Use this sheet as a guideline, and create a current life-table (‘complete’, i.e., with 1-year age-intervals) for Canadian males, using the male population sizes, and numbers of deaths, by age, Canada 2001. [The calculations in columns O to W of the lifetable sheet are not relevant for this exercise]. Details on the elements of, and the construction of current lifetables can be found in the chapters (on website) from the textbooks by Bradford Hill and Selvin, and in the technical notes provided by the US National Center for Health Statistics in connection with US Lifetable 2000. See also the FAQ for 613 from 2005. The fact that the template is for an abridged life table, with mostly 5-year intervals, whereas the task is to construct a full lifetable with 1 year intervals, caused some people problems last year.. they realized something was wrong when the life expectancy values were way off! Since this is an exercise, and not a calculation for an insurance company that wants to have 4 sig. decimal places, don’t overly fuss about what values of ‘a’ you use for the early years.. they don’t influence the calculations THAT much: If you try different sets of values (such as 0.1 in first year and 0.5 thereafter) you will not find a big impact. But don’t take my word for it .. the beauty of a spreadsheet is that you can quickly see the consequences of different assumptions or ‘what ifs’. [In practice, in order not to be unduly influenced by mortality rates in a single calendar year (e.g. one that had a very bad influenza season), current lifetables are usually based on several years of mortality data. Otherwise, or if they are based on a small population, the quantities derived from them will exhibit considerable random fluctuations from year to year ] Once you have completed the table, use the charting facilities in Excel to plot the survival curve for the hypothetical (fictitious) male ‘cohort’ represented by the current lifetable. On a separate graph, use two histograms to show the distributions of the ages at death (i) for this hypothetical male ‘cohort’ and (ii) those males who died in 2001. To make it easy to compare them, superimpose the histograms or put them ‘side by side’ or ‘back to back’ within the same graph. Explain why the two differ in shape and location. Calculate/derive (and include them somewhere on the spreadsheet) the median and mean age at death in the hypothetical cohort and the corresponding statistics for the actual deaths in 2001. Possible Body Mass Indices This exercise investigates different definitions of Body Mass Index (BMI). BACKGROUND: With weight measured in Kilograms, and height in metres, BMI is usually defined as weight divided by the SQUARE of height, i.e., BMI = Wt / (Height*Height), or BMI = Wt/(height2) using, as SAS and several other programming languages do, the symbol for ‘raised to the power of’. [ NB: Excel uses ^ to denote this ] What’s special about the power of 2? Why not a power of 1 i.e., Weight/height? Why not 3, i.e., Weight/*(height3) ? Why not 2.5 i.e. Weight/(height2.5)? One of the statistical aims of a transformation of weight and height to BMI is that BMI be statistically less correlated with height, thereby separating height and height into two more useful components height and BMI. For example in predicting lung function (e.g. FEV1), it makes more sense to use height and BMI than height and weight, since weight has 2 components in it – it is partly height and partly BMI. Presumably, one would choose the power which minimizes the correlation. The task in this project is to investigate the influence of the power of height used in the ratio, and to see if the pattern of correlations with power is stable over different settings (datasets). DATA: To do this, use 2 of the 6 datasets on the 678 webpage: [usernane is c678 and p w is H**J44 ] Children aged 11-16 Alberta 1985 (under ‘Datasets’) 18 year olds in Berkeley longitudinal study, born 1928/29 (under ‘Datasets’) Dataset on bodyfat – 252 men (see documentation) (under ‘Datasets’) Pulse Rates before and after Exercise – Australian undergraduates in 1990’s (under ‘Projects’) Miss America dataset 1921-2000 (under ‘Resources’) Playboy dataset 1929-2000 (under ‘Resources’) METHODS: First create each of the two SAS datasets, and if height and weight are not already in metres and Kg, convert them to these units. Drop any irrelevant variables. Inside each dataset, create a variable giving the source of the data (we will merge the two – and eventually all six– datasets, so we need to be able to tell which one each observation came from). Combine the two datasets, i.e. ‘stack’ them one above the other in a single dataset. Print out some excerpts. For each subject in the combined dataset, create 5 versions of &lt; using the powers 1, 1.5, 2, 2.5 and 3. Calculate the correlation between the ‘BMI’ obtained with each of these powers, and height. Do this separately for the observations from the two different sources (the BY statement should help here). Report your CONCLUSIONS. Galton The objective of this exercise is to examine the relation between heights of parents and heights of their (adult) children, using recently ‘uncovered’ data from the Galton archives, You are asked to assess if Galton’s way of dealing with the fact that heights of males and females are quite different produces sharper correlations than we would obtain using ‘modern’ methods of dealing with this fact. As side issues, you are also asked to see whether the data suggest that stature plays “a sensible part in marriage selection” and to comment on the correlations of the heights in the 4 {father,son}, {father,daughter}, {mother,son} and {mother,daughter} pairings. BACKGROUND: Galton ‘transmuted’ female heights into their ‘male-equivalents’ by multiplying them by 1.08, and then using a single combined ‘uni-sex’ dataset of 900-something offspring and their parents. While some modern-day anayysts would simply calculate separate correlations for the male and female offspring (and then average the two correlations, as in a meta-analysis), most would use the combined dataset but ‘partial out’ the male-females differences using a multivariable analysis procedure. The various multivariable procedures in effect create a unisex dataset by adding a fixed number of inches to each female’s height (or, equivalently, in the words of one of our female PhD students, by ‘cutting the men down to size’). JH was impressed by the more elegant ‘proportional scaling’ in the ‘multiplicative model’ used by Galton, compared with the ‘just use the additive models most readiliy available in the software’ attitude that is common today. In 2001, he located the raw (untransmuted) data that allows us to compare the two approaches. DATA: For the purposes of this exercise, the data [see http://www.epi.mcgill.ca/hanley/galton ] are in two separate files: the heights# of 205 sets of parents ( parents.txt ) with families numbered 1-135, 136A, 136-204 the heights# of their 900-something* children ( offspring.txt ) with families numbered as above The data on eight families are deliberately omitted, to entice the scholar in you to get into the habit of looking at (and even double checking) the original data. Since here we are more interested in the computing part in this course, and because time is short, ignore this invitation to inspect the data – we already had a look at them in class. In practice, we often add in ‘missing data’ later, as there are always some problem cases, or lab tests that have to be repeated, or values that need to be checked, or subjects who didn’t get measured at the same time as others etc.. JH’s habit is to make the additions in the ‘source’ file (.txt or .xls or whatever) and re-run the entire SAS DATA step(s) to create the updated SAS dataset (temporary or permanent). If the existing SAS datset is already large, and took a lot of time to create, you might consider creating a small dataset with the new observations, and then stacking (using SE) the new one under the existing one – in a new file. SAS has fancier ways too, and others may do things differently! If your connection is too slow to view the photo of the first page of the Notebook, the title reads FAMILY HEIGHTS (add 60 inches to every entry in the Table) METHODS/RESULTS/COMMENTS: Categorize each father’s height into one of 3 subgroups (shortest 1/4, middle 1/2, tallest 1/4). Do likewise for mothers. Then, as Galton did [ Table III ], obtain the 2-way frequency distribution and assess whether “we may regard the married fold as picked out of the general population at haphazard”. Calculate the variance Var[F] and Var[M] of the fathers’ [F] and mothers’ [M] heights respectively. Then create a new variable consisting of the sum of F and M, and calculate Var[F+M]. Comment. Galton called this a “shrewder” test than the “ruder” one he used in 1. ( statistic-keyword VAR in PROC MEANS) When Galton first anayzed these data in 1885-1886, Galton and Pearson hadn’t yet invented the CORRelation coefficient. Calculate this coefficient and see how it compares with your impressions in 1 and 2. Create two versions of the transmuted mother’s heights, one using Galton’s and one using the modern-day (lazy-person’s, blackbox?) additive scaling [for the latter, use the observed difference in the average heights of fathers and mothers, which you can get by e.g., running PROC MEANS on the offspring dataset, either BY gender, or using gender as a CLASS variable]. In which version of the transmuted mothers’ heights is their SD more simlar to the SD of the fathers? ( statistic-keyword STD in PROC MEANS) Create the two corresponding versions of what Galton called the ‘mid-parent’ (ie the average of the height of the father and the height of the transmuted mother). Take mid-point to mean the half-way point (so in this case the average of the two) Create the corresponding two versions (additive and multiplicative scaling) of the offspring heights (note than sons’ heights remain ‘as is’). Address again, but now for daughters vs sons, the question raised at the end of 4. Merge the parental and offspring datasets created in steps 4 and 6, taking care to have the correct parents matched with each offspring (this is called a 1:many merge). Using the versions based on 1.08, round the offspring and mid-parent heighs to the nearest inch (or use the FLOOR function to just keep the integer part of the mid-parent height –you need not be as fussy as Galton was about the groupings of the mid-parent heights), and obtain a 2-way frequency distribution similar to that obtained by Galton [ Table I ]. Note that, opposite to we might do today, Galton put the parents on the vertical, and the offspring on the horizontal axis. ( The MOD INT FLOOR CEIL and ROUND functions can help you map observations into ‘bins’ ; we will later see a way to do so using loops) Galton called the offspring in the same row of his table a ‘filial array’. Find the median height for each filial array, and plot it, as Galton did, against the midpoint of the interval containing their midparent – you should have one datapoint for each array. Put the mid-parent values on the vertical, and the offspring on the horizontal axis. By eye, estimate the slope of the line of best fit to the datapoints. Mark your fitted line by ‘manually’ inserting two markers at the opposite corners of the plot. Does the slope of your fitted line agree with Galton’s summary of the degree of “regression to mediocrity”? [ Plate IX ] Note that Galton used datapoints for just 9 filial arrays, choosing to omit those in the bottom and top rows (those with the very shortest and the very tallest parents) because the data in these arrays were sparse. ( By using the binned parental height in the CLASS statement in PROC MEANS or PROC UNIVARIATE, directing the output to a new SAS dataset, and applying PROC PLOT to this new dataset, you can avoid having to do the plotting manually See more on this in the FAQ) Plot the individual unisex offspring heights (daughters additively transmuted) versus the mid-parent height (mothers transmuted). OVERLAY on it, with a different plotting symbol, the corresponding plot involving the multiplicatively transmuted offspring values (on the parent-axis, stay with Galton’s definition of a midparent). (see FAQ) Compare the two, and have a look at Galton’s fitted ellipse, corresponding to a bivariate normal distribution [ Plate X ]) {here, again, we would be more likely to plot the parents’ heights on the horizontal, and the offspring heights on the vertical axis}. For each of the following ‘offspring vs. mid-parent’ correlations, use the ‘mid-parent’ obtained using Galton’s multiplicative method. Calculate (a) the 2 correlations for the 2 unisex versions of the offspring data (b) the sex-specific correlations (i.e., daughters and sons separately) and (c) the single parent-offspring correlation, based on all offspring combined, and their untransmuted heights, ignoring the sex of the offspring. Comment on the correlations obtained, and on the instances where there are big disparities between them. [ a PLOT, with separate plotting symbols for sons and daughters, might help in the case of (c) ] Calculate the 4 correlations (i) father,son (ii) father,daughter, (iii) mother,son and (iv) mother,daughter. Comment on the pattern, and on why you think it turned out this way. Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday October 30. Lottery payoffs Detecting a fake Bernoulli sequenece Cell occupancy "],
["computing04.html", "Chapter 19 Computing: Session No. 4 19.1 Objectives 19.2 Exercises 19.3 Other Exercises (under construction)", " Chapter 19 Computing: Session No. 4 19.1 Objectives The ‘computing’ objective is to learn how to use R to simulate variation by (Monte Carlo) simulation. Tasks and R Functions/structures used include Read a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file&quot; read.table( ) Compactly display the internal structure of an R object, e.g. the abbreviated contents of (possibly nested) lists: str( ) repeating a procedure for subsets of a dataframe: by( ) loops using for . See here. rescaling a vector v so that it sums to 1: v = v / sum(v) adding elements of a vector or (say) a product of 2 vectors, or a square of a vector: `sum( ) compute the mean and standard deviation of the numbers in a vector: mean() and sd( ) computing the mean and SD of a (population) frequency distribution from first principles using the scaled frequencies as probabilities: sum(values*probabilities) extracting the first or last parts of a vector, matrix, table, or data frame: head( ) and tail( ) removing duplicate elements/rows from a vector, data frame or array: unique plotting: plot( ) adding one or more straight lines through the current plot: abline( ) joining multiple (x,y) points with line segments: lines( ) Splitting data into subsets, computing summary statistics for each, and returning the result in a convenient form: aggregate sampling from a set of values: sample( ) summing the values in a vector: sum( ) applying a function to each of the rows or columns of a matrix, or to some slices of higher-dimensional array: apply( ) computing the mean, variance and sd of a vector: mean( ), var( ), sd( ) defining a (homemade) function to ‘automate’ a procedure function tabulating the values in a vector, or crosstabulating the values in 2 or more vectors: table( ) The ‘statistical’ objective of this exercise is to learn-by-seeing some of the laws that describe/quantify the variation (variance, SD) of the sum, and more importantly, the difference of 2 random variables. The laws are the same no matter whether the difference is in some characteristic of 2 randomly selected individuals or in some statistic (an aggregate of individual values). Since we don’t get to see the individuals – or collections of individuals – that were nor selected, the laws are based on the theory of mathematical statistics, and thus not easy to visualize/imagine. So, we will use simulations (virtual thought experiments) to help us motivate and remember the laws, and to bring out the possibly-surprising ‘reasons why’ they take the form they do. The take-home message is that if the two random quantities involved in the difference are statistically independent of each other, the possible variations in the difference are larger than the ones in each of the two components, and that Variances Add; SDs don’t. The variance of a sum or difference is equal to the sum of the (variances of the) parts In the case of all things which have several parts and in which the totality is not, as it were, a mere heap, but the whole is something besides the parts, there is a cause; for even in bodies contact is the cause of unity in some cases, and in others viscosity or some other such quality. Aristotle’s Metaphysics. Book VIII, 1045a.8–10 1908 translation by W. D. Ross: The whole is greater than the part. Euclid, Elements, Book I, Common Notion (Euclid is expressing how a whole can be split into parts, and any one of those parts, compared to the whole, is less than the whole. And using the Segment Addition Postulate – If point P is between points A and B, then AP + PB = AB. Ipso facto, the sum of parts equal the whole. that the whole is not the same as the sum of its parts are useful in meeting the type just described; for a man who defines in this way seems to assert that the parts are the same as the whole. The arguments are particularly appropriate in cases where the process of putting the parts together is obvious, as in a house and other things of that sort: for there, clearly, you may have the parts and yet not have the whole, so that parts and whole cannot be the same.” Aristotle. Book VI. (all of the above are from this URl. Likewise, the SD of a sum or difference is less than the sum of the (SDs of the) parts Although the theoretical chapter does not address the laws for sums or differences of non-independent random variables, the simulations bring out the fact that the variance of the sum/difference could be bigger or smaller than the variances of the parts: it all depends on how the components are linked. 19.2 Exercises First, before we get to differences, an example of the sampling variability of sums and means. The sex-specific panels below shows examples of the sums of the lengths of 10 baby names, placed side by side. (each row includes 9 blank spaces, so that doesn’t change the variability: it just shifts all the sums by 9.) The fact that they are not randomly ordered, bur rather ordered by popularity, doesn’t matter that much, since – as the subsequent graph shows – popularity is not strongly related to the length of the name. Had we randomly ordered them, we would still see the ‘jagged’ line endings and the same anount of variation. (Ignore for now the last column, which shows a type of sampling that is to be discouraged, or that, if used, needs post-hoc adjustment of the results). 19.2.1 Ages of UK cars Refer to the exercise, and the dataset, in the first computing session. Using the year-of-manufacture frequency distribution you derived back then, calculate the mean year, and thus the mean age. From first principles, i.e., using devaitions from the mean year, calculate the variance – and thus the SD – of the distribution. Without actually converting every year-of-manufacture to an age, calculate the standard deviation of the ages. Even though the frequency distribution is not symmetric, how could the SD (or an estimate of it) be of help if you had to estimate the mean age from just a random sample of registered cars. 19.2.2 Lengths of Babies’ Names The 500 most popular baby names, Quebec, 2013-1018 from=&quot;https://www.retraitequebec.gouv.qc.ca/en/services-en-ligne-outils/banque-de-prenoms/Pages/recherche_par_popularite.aspx?AnRefBp=2018&amp;NbPre=0&amp;ChAff=PPMP&amp;NbPrePag=500&quot; ds = read.table( &quot;http://www.biostat.mcgill.ca/hanley/statbook/QuebecBabyNames2013to2018.txt&quot;, as.is=TRUE) str(ds) ## &#39;data.frame&#39;: 6000 obs. of 6 variables: ## $ Name : chr &quot;LEA&quot; &quot;EMMA&quot; &quot;OLIVIA&quot; &quot;FLORENCE&quot; ... ## $ Frequency: int 636 518 505 457 447 422 408 370 355 349 ... ## $ Length : int 3 4 6 8 5 3 7 8 5 7 ... ## $ Year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ Gender : chr &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... ## $ Rank : int 1 2 3 4 5 6 7 8 9 10 ... by(ds,list(g=ds$Gender,y=ds$Year),head,n=3) ## g: F ## y: 2013 ## Name Frequency Length Year Gender Rank ## 1 LEA 636 3 2013 F 1 ## 2 EMMA 518 4 2013 F 2 ## 3 OLIVIA 505 6 2013 F 3 ## ------------------------------------------------------------ ## g: M ## y: 2013 ## Name Frequency Length Year Gender Rank ## 501 WILLIAM 848 7 2013 M 1 ## 502 NATHAN 792 6 2013 M 2 ## 503 SAMUEL 716 6 2013 M 3 ## ------------------------------------------------------------ ## g: F ## y: 2014 ## Name Frequency Length Year Gender Rank ## 1001 EMMA 580 4 2014 F 1 ## 1002 LEA 579 3 2014 F 2 ## 1003 OLIVIA 520 6 2014 F 3 ## ------------------------------------------------------------ ## g: M ## y: 2014 ## Name Frequency Length Year Gender Rank ## 1501 WILLIAM 787 7 2014 M 1 ## 1502 THOMAS 742 6 2014 M 2 ## 1503 FELIX 721 5 2014 M 3 ## ------------------------------------------------------------ ## g: F ## y: 2015 ## Name Frequency Length Year Gender Rank ## 2001 EMMA 631 4 2015 F 1 ## 2002 LEA 540 3 2015 F 2 ## 2003 OLIVIA 482 6 2015 F 3 ## ------------------------------------------------------------ ## g: M ## y: 2015 ## Name Frequency Length Year Gender Rank ## 2501 WILLIAM 767 7 2015 M 1 ## 2502 THOMAS 762 6 2015 M 2 ## 2503 JACOB 671 5 2015 M 3 ## ------------------------------------------------------------ ## g: F ## y: 2016 ## Name Frequency Length Year Gender Rank ## 3001 EMMA 643 4 2016 F 1 ## 3002 LEA 517 3 2016 F 2 ## 3003 OLIVIA 516 6 2016 F 3 ## ------------------------------------------------------------ ## g: M ## y: 2016 ## Name Frequency Length Year Gender Rank ## 3501 WILLIAM 798 7 2016 M 1 ## 3502 THOMAS 701 6 2016 M 2 ## 3503 LIAM 666 4 2016 M 3 ## ------------------------------------------------------------ ## g: F ## y: 2017 ## Name Frequency Length Year Gender Rank ## 4001 EMMA 620 4 2017 F 1 ## 4002 LEA 557 3 2017 F 2 ## 4003 ALICE 519 5 2017 F 3 ## ------------------------------------------------------------ ## g: M ## y: 2017 ## Name Frequency Length Year Gender Rank ## 4501 WILLIAM 714 7 2017 M 1 ## 4502 LOGAN 675 5 2017 M 2 ## 4503 LIAM 634 4 2017 M 3 ## ------------------------------------------------------------ ## g: F ## y: 2018 ## Name Frequency Length Year Gender Rank ## 5001 EMMA 612 4 2018 F 1 ## 5002 ALICE 525 5 2018 F 2 ## 5003 OLIVIA 490 6 2018 F 3 ## ------------------------------------------------------------ ## g: M ## y: 2018 ## Name Frequency Length Year Gender Rank ## 5501 WILLIAM 739 7 2018 M 1 ## 5502 LOGAN 636 5 2018 M 2 ## 5503 LIAM 629 4 2018 M 3 Figure 19.1: The closely spaced vertical lines show the frequency distributions of the popularity (ranks) of the 500 most popular names for babies born in Quebec in 2018. For example, more than 600 girls and more than 700 boys had the most popular name, while fewer than half this many babies had the 25th most popular name. Also shown are the names themselves, arranged row-wise, starting from the most popular (1. Emma 2. Alice, .. 1. William, 2. Logan, …). For each row, the two values shown in plain type are the sum/mean of the lengths of the 10 names in the row. The last number, in italics, is the length of 1 name selected from the row, by ‘sticking a pin’ (indicated by the black ^ symbol), at random in one of the letters in the row, and selecting the name that the pin landed on. The values in bold at the foot of each column are the mean and SD of the values in the column. par(mfrow=c(2,1),mar = c(5,5,0,0)) # splits graph into 2 x 1 grid for( g in unique(ds$Gender) ) { d = ds[ds$Gender == g,] COL=c(&quot;red&quot;,&quot;blue&quot;)[1 + (d$Gender==&quot;M&quot;)] plot(d$Length,d$Rank, pch=19, col= COL, cex=0.25, ylab=&quot;Rank&quot;, xlab = &quot;Number of Letters in Name&quot;, ylim=c(0,570) ) abline(h=seq(0,500,50),col=&quot;lightblue&quot;) lines(aggregate(d$Rank,by=list(Length=d$Length),mean)) Fr = aggregate(!is.na(d$Rank),by=list(Length=d$Length),sum) L = Fr$Length Fr = Fr$x text(min(L),550,&quot;Cum. %&quot;,adj=c(0.5,-0.1),cex=0.75) for(i in 1:length(L) ) text( L[i], 510, toString( round( 100*cumsum(Fr)/3000) [i] ), adj = c(0.5,-0.1), cex=0.75 ) } Figure 19.2: For girls (red) and boys (blue) separately, the mean ranks of names with a given length are plotted against the length. Each dot in a column is a name. The relative densities of the dots in a column indicates how common these name lengths are. Exercise Use the Length and Frequency columns in the ds dataframe (see above) to compute the (gender-specific) mean and SD of the lengths of the names of all of the Quebec babies born in 2018 having one of these names (For our purposes here, the few percent of babies whose names did not make it into the top 500 are excluded. If interested, they can be found at the link shown above.) Hint: you could rescale the Frequency vector so that it sums to 1, and then simply use the sum of its product with Length. You can think of the rescaled Frequency vector as probabilities. Rather than do these computations separately for each year, find a way to automate this. Hint: maybe put what you did to compute the mean and SD for one year into your own function and then use this function in a by statement. For example, if all you wanted was the number of babies each year, you could just use the inbuilt function sum, and apply it to the frequencies in each year, using the statement by(ds$Frequency,ds$Year,sum). Compare your SD calculations agaist the SD[lengths of 500 names] shown at the bottom left of the red and blue panels of names, and if they are different, explain why they are. Using the SD[lengths of 500 names] values shown, explain why the 2 SD’s in bold to the right of them (at the feet of the two columns) are so much bigger or smaller. Do these 2 SDs in bold ‘fit’ with the laws we are learning about? Explain. Use Monte Carlo simulations – and the fact that you KNOW the gender-specific mean and SDs of the lengths of names – to check out the theoretical formulae (laws) for the possible variation in the difference of two sample means. In other words, with \\(y\\) denoting the length of the name of a randomly selected baby, \\(n\\) the size of each sample, and \\(\\sigma\\) denoting the unit standard deviation, and \\(F\\) and \\(M\\) denoting female and male, show – to within Monte Carlo error – that \\[ Var[ \\bar{y}_{F} - \\bar{y}_{M} ] = \\frac{\\sigma^2_{F}}{n} + \\frac{\\sigma^2_{M}}{n}.\\] Use ‘\\(n\\)’s of size 5, 25, 100, and 400. Use the sample function with the rescaled frequencies as probabilities, and replace = TRUE. To keep the computing simpler, limit yourself to one year. Here is some starter code. You need to modify it to extend the sample size to 400, add to the function code so that it includes samples of females, and computes the \\(\\bar{y_F} - \\bar{y_M}\\) fifference. (the results matrix has 3 columns, only 1 of which gets filled under the present code.) You should try a small value of N.SIMS first, to be sure everyting is working, and then make it large so that you approach the ’expected’ values. Note: this code uses loops, which are slower, to fill the rows of the results matrix, and also samples from the list of 500, instead of from the (many fewer) distinct lengths. But for now, it is more transparent. Biostatisticians who run very complex simulations are always looking for ways (such as the apply functions ) to reduce the time. d = ds[ds$Year == 2018,] lengths.M = d$Length[d$Gender == &quot;M&quot;] # 500 long probs.M = d$Frequency[d$Gender == &quot;M&quot;] # ditto probs.M = probs.M/sum(probs.M) # ditto N.SIMS = 100 nn = c(5, 25,100) # the n&#39;s you want to use in the &#39;n&#39; loop mean.length.in.sample = function(n){ y.m = sample(lengths.M,size=n,replace=TRUE,prob=probs.M) ybar.m = mean(y.m) return( c(ybar.m)) } print(noquote( paste(&quot;Across&quot;, N.SIMS, &quot;simulated samples ...&quot;) )) ## [1] Across 100 simulated samples ... for(n in nn){ # loop over n results = matrix(NA,N.SIMS,1,3) # set up matrix # 3 col.s are for ybarM, ybarF, and diff. for(sim in 1:N.SIMS) results[sim,1] = # loop mean.length.in.sample(n) # fills just the ybarM&#39;s # ... expand E = apply(results,MARGIN=2,FUN=mean) # (approximate) # exact if N.SIMS = Infinity VAR = apply(results,MARGIN=2,FUN=var) # (approximate) print(noquote(&quot;&quot;)) print(noquote(paste(&quot;n = &quot;,n))) print(noquote(&quot;&quot;)) print(noquote( paste(&quot;Ave of F &amp; M ybar&#39;s: and of ybarF - ybarM:&quot;, paste(round(E,2),collapse=&quot; , &quot;) ) ) ) print(noquote(&quot;&quot;)) print(noquote( paste(&quot;Var of ybar&#39;s: and of their differences: &quot;, paste(round(VAR,4) ,collapse=&quot;, &quot;) ) ) ) print(noquote( paste(&quot;SD of ybar&#39;s: and of their differences: &quot;, paste(round(sqrt(VAR),2) ,collapse=&quot; , &quot;) ) ) ) } ## [1] ## [1] n = 5 ## [1] ## [1] Ave of F &amp; M ybar&#39;s: and of ybarF - ybarM: 5.68 ## [1] ## [1] Var of ybar&#39;s: and of their differences: 0.4809 ## [1] SD of ybar&#39;s: and of their differences: 0.69 ## [1] ## [1] n = 25 ## [1] ## [1] Ave of F &amp; M ybar&#39;s: and of ybarF - ybarM: 5.74 ## [1] ## [1] Var of ybar&#39;s: and of their differences: 0.0743 ## [1] SD of ybar&#39;s: and of their differences: 0.27 ## [1] ## [1] n = 100 ## [1] ## [1] Ave of F &amp; M ybar&#39;s: and of ybarF - ybarM: 5.69 ## [1] ## [1] Var of ybar&#39;s: and of their differences: 0.0192 ## [1] SD of ybar&#39;s: and of their differences: 0.14 19.2.3 Life Tables [2018] In an exercise in the previous session, you used the 22 [conditional] probabilities of dying within the (1, 4 or 5 year) age intervals shown in Table 3.9 of this Institut de la statistique du Quebec report to create a life table. The R code below goes a bit further, and creates an R function to interpolate and create a lifetable with increments of 1 year. This is turn allows you to use the inbuilt diff function to compute the 105 (unconditional) probabilities that a person in this fictional cohort will die in their y-th year, where y runs in steps of 1 , from 1st to 105th. Even though it isn’t always the case, assume for thse exercises that the deaths occur in the middle of each age-bin. Also, assume that lifetimes don’t extend past 105 i.e, so that the 105 probabilities add to 1 (check this!) Use these 105 probabilities to compute the (sex-specific) expected duration of life (or mean age at death, if you want to look at it the way the first actuaries did, when they called the tables Mortality Tables. Somewhere along the way, the name was switched to Life Tables.) Use these means, along with the probabilities, to calculate, from its definitional form, the (sex-specific) variance, and SD, of duration of life (age at death) It is a theorem in mathematical statistics that the area under the survival curve is the mean duration of life. Verify that this is so in this example, using two versions of the lifetable (i) the one computed from the 22 age-bins, and (ii) the interpolated one with 105. How does the L\\(_x\\) column in Table 3.9 relate to the calculations you made in (i)? Using the starter R code below, draw a sample of 100 lifetimes from one of the gender-specific ‘cohorts’, sort the lifelines, and plot them as horizontal lines. What pattern do you get if you up the number of lifelines, but can still ‘see’ the lines? What do these heuristics tell you about the theorem in mathematical statistics? If you want to see another version, look at this page. [8221 years lived by 100 persons; mean = 8221years/100persons = 1 82.21 years/person.] Consider 10 newborns (5 male and 5 female) from the fictitious cohort. Use simulation to compute the probability that, collectively, the total (or mean) of the 5 female lifetimes will exceed the total (or mean) of the 5 male lifetimes? Do you think there might be a faster (less computing-intensive) way to arrive at this probability? If so, would you have to make any assumptions, and how realistic would these assumtipons be? [BTW: statisticians who are nimble with convolutions would be able to provide an exact answer i.e., without Monte Carlo error] SequentialRisks=matrix( c(00465,00066,00035,00065,00167,00278,00298,00355,00431,00613,00980, 01577,02523,04027,06508,09955,16528,27068,43560,63605,80498,100000, 00388,00052,00040,00041,00082,00115,00142,00160,00239,00370,00642, 01089,01825,02881,04460,07066,11796,19942,34244,52976,72467,100000)/ 100000, 22,2) # 22 rows 2 columns Age = c(1,5,seq(10,105,5) ) par(mfrow=c(1,1),mar = c(5,5,0,0)) plot(-10,-10,xlim=c(0,110),ylim=c(0,1.02), xlab=&quot;Age&quot;,ylab=&quot;Proportion Stll Alive&quot;, cex.axis=1.5,cex.lab=1.5,col=&quot;white&quot;) abline(v=seq(0,110,10),col=&quot;lightblue&quot;) abline(h=seq(0.0,1,0.1),col=&quot;lightblue&quot;) Prob = matrix(NA,105,2) Age.boundaries = c(0,Age) for(i.col in 1:2) { # F then M loop S.at.end.interval = cumprod( # cumulative product 1 - SequentialRisks[,i.col] ) Proportion.Alive = c(1,S.at.end.interval) lines(Age.boundaries,Proportion.Alive,col=i.col) Risk = 1 - Proportion.Alive Risk = approx(Age.boundaries, Risk,0:105)$y Prob[,i.col] = diff(Risk) } # Check: marginal totals of Prob # MARGIN=2: 1 sum for each column apply(Prob,MARGIN=2,FUN=sum) ## [1] 1 1 # marginal totals of Prob Ages = 0.5 : 104.5 sum( Ages * Prob[,1] ) # Males ## [1] 80.62743 # .................. # Females # code to plot 25 lifelines, sorted from shortest to longest # Note that these lifelines are drawn from a simplified &#39;fake&#39; # distribution, just to demonstrate the plotting. if you use the # function with a large n, you will &#39;see&#39; the shape of the S curve. draw.from.2.str.lines.S.curve = function(n=25, x=c(.90,60,105)){ s1 = x[1]; t1 = x[2]; t2 = x[3] p = runif(n,0,1) t = (p &gt; s1)*t1*(p-s1)/(1-s1) + (p &lt;= s1)*(t1 + (t2-t1)*(s1-p)/s1 ) return(t) } n=25 y.vals = 1 - (1:n)/(n+1) longevity = draw.from.2.str.lines.S.curve(n, x=c(.90,60,105)) longevity = sort(longevity) segments(0, y.vals , longevity, y.vals, lwd=0.5, col=&quot;grey25&quot;) Figure 19.3: Lifetables based on mortality rates of Quebec males (blue) and females(red) in 2018. See above for source of the data. The horizontal ‘lifelines’ are sorted by length, having been randomly drawn from a simplified survival distribution – just to illustrate the use of the sort and segments functions in R. 19.2.4 Variable-length (parallel) parking spaces When setting up parallel parking on city streets, and out of necessity if spaces have individual parking meters, many municipalities use painted rectangles of a fixed dimension. We haven’t been able to find universal guidelines for these dimensions – cities tend to adopt their own, such as the ones here. Some authorities have pointed to a waste of space when these fixed-size spaces are used. Indeed JH has seen, in Melbourne, Australia, a street where drivers were asked to park in order, starting from the from end of a block, keeping just 1 metre from the car in front, until the entire block filled up. To see how many additional cars can be accommodated by this scheme, let’s compare how this system compares with a fixed-length box of 6.5 meters. Imagine a city block, 130 metres long, that contains \\(N\\) = 20 fixed-length parking spots. What if it were converted to the ‘just fill up from the one end’ system with no boundaries? See the example below. To study this, we have used the most popular makes and models of cars in the UK car-registrations database (déjà vu). For each model, we were able to find its length (in mm) from this database. In instances where the length of a car model changed over time, to reduce the otherwise-lengthy programming and computer-intensive data-extraction, we selected a ‘middle’ length. In the resulting data file, each of the 508 rows gives the make and the model, as well as its frequency in the car-registrations database, and its length in millimetres. [The datafile also contains the width, height and mass, which we will use elsewhere to investigate a ‘body mass index’ for cars] Read the data into an R dataframe. Select 25 cars randomly from the just-described frequency distribution, and determine whether, parked a meter apart, they fit into the 130-m long space. Repeat the simulation a sufficient number of times to approach the probablity that this system would provide at least 5 additional spaces. In a future exercise, we will approach this probability calculation by modelling rather than simulation. Can you imagine what model we will use? Hint: have a look at the distribution (over your simulations) of the total/mean length of 25 randomly selected cars. This same approach could be used to plan how many people it is safe to have in an elevator that can safely carry a load of say 1000 Km. Does it matter that the shape of the ‘weight’ distribution has a different shape than the car-lengths distribution? Are there any other factors that come into play here that not present in the car-sampling example? If they are, how will the affect the calculations? By the way, what shape does the car-lengths distribution have? How would you summarize the distribution? Suggest ways to measure/who the relation between car-length and popularity. ds=read.table(&quot;http://www.biostat.mcgill.ca/hanley/statbook/DimensionsOfPopularCars.txt&quot;,as.is=TRUE) str(ds) ## &#39;data.frame&#39;: 508 obs. of 6 variables: ## $ Make.Model : chr &quot;ABARTH 500&quot; &quot;ABARTH 595&quot; &quot;ABARTH 695&quot; &quot;ABARTH PUNTO&quot; ... ## $ No.Registered: int 5561 16300 523 703 5059 2503 6444 2268 5327 27605 ... ## $ Length.mm : int 3657 3660 3660 4065 4170 4438 4661 4413 4643 4351 ... ## $ Width.mm : int 1627 1627 1627 1721 1729 1743 1830 1830 1873 1798 ... ## $ Height.mm : int 1485 1485 1485 1478 1442 1430 1422 1372 1436 1465 ... ## $ Weight.Kg : int 1085 1050 1050 1185 1185 1285 1540 1420 1349 1280 ... # plot(ds$Length.mm,log(ds$No.Registered),cex=0.5,pch=19) N.SIMS=30 LENGTHS = ds$Length.mm/1000 Probs = ds$No.Registered/sum(ds$No.Registered) E.LENGTH = sum(LENGTHS*Probs) VAR = sum( (LENGTHS-E.LENGTH)^2 * Probs ) SD = sqrt(VAR) N = 20 W=6.5 for(n in 20:30){ mu = mean=n*E.LENGTH + n-1 prob.fit = pnorm(N*6.5,mean=mu,sd=sqrt(n)*SD) #print(round(c(n,E.LENGTH,mu,SD,prob.fit),2)) } par(mfrow=c(1,1),mar = c(5,5,0,0)) plot(-10,-10,xlim=c(0.5,N+1),ylim=c(0.01,1.01)*N.SIMS, xlab=&quot;Fixed-dimension Parking Spaces&quot;, ylab=&quot;Simulation No.&quot;, xaxp=c(1,N,N-1), cex.axis=1.5,cex.lab=1.5,col=&quot;white&quot;) abline(v=1/2 + 0:N,lwd=0.75,col=&quot;lightblue&quot;) gaps=(0:(n-1))/W n = N for(sim in 1:N.SIMS){ lengths = sample(LENGTHS,size=n,prob=Probs)/W ends = gaps+cumsum(lengths) starts = ends - lengths arrows(starts+1/2, rep(sim,n), ends+1/2, rep(sim,n),col=&quot;blue&quot;, lwd =1.25, length=0.05,angle=20, code=1 ) starts = 0:(n-1) ends = starts+lengths arrows(starts+1/2, rep(sim-0.35,n), ends+1/2, rep(sim-0.35,n),col=&quot;red&quot;, lwd =1.25, length=0.05,angle=20, code=1) } Figure 19.4: 30 simulations of the possible gains from using variable-length (parallel) parking spaces. Shown are 20 fixed-length spaces, each of length 6.5m. In each simulation, 20 cars are selected randomly from the just-described frequency distribution, and their lengths are shown as red lines within the fixed-length parking spaces, and as blues lines, with 1m spacing, in the variable-length scheme. 19.2.5 Galton’s data on family heights The background to these data can be found here So that he could combine the father’s and mother’s heights into a single ‘mid-parent’ height, against which to correlate the offspring height, Francis Galton was keen to establish that height plays a very small part in marriage selection. Here are the heights of 205 sets of parents, with families numbered 1-135, 136A, 136-204. They are takem directly from his notebook. [See JH’s notes on why 7 families are ommitted. Several authors have since extracted the data from JH’s webiste, but some (e.g. the authors of the mosaic package) extracted the incomplete version. The Galton dataset in the HistData package has all of the families.] Categorize each father’s height into one of 3 subgroups (shortest 1/4, middle 1/2, tallest 1/4). Do likewise for mothers. Then, as Galton did, obtain the 2-way frequency distribution and visually assess whether you agree with the conclusion “we may therefor regard…” he draws following his Table 9 Calculate the variance Var[F] and Var[M] of the fathers’ [F] and mothers’ [M] heights respectively. Then create a new variable consisting of the sum of F and M, and calculate Var[F+M]. Comment. Galton called this a “shrewder” test than the “ruder” (coarser) one he used in 1. If the two were unrelated (this is a stronger quality than ‘uncorrelated’), what would you expect? If the variance of the sum is greater than the sum of the variances of the parts, what does it suggest. What if he had subtracted them and calculated the variance of the differences? When Galton first analyzed these data in 1885-1886, Galton and Pearson hadn’t yet invented the correlation coefficient. Calculate this coefficient and see how it compares with your impressions in 1 and 2. Compare this correlation with the correlation between parental ages in the Penrose dataset. 19.2.6 Height differences of random M-F pairs Some new-to-statistics (and even some not so new-to) students reason that because the average male is 4-5 inches taller than the average female, the heights of randomly formed M-F pairs would be positively correlated. In Galton’s parental data, in how many of the couples was the man the taller member of the couple? If you scrambled his data, so as to pair a random father with a random mother, how often would this be the case? you would need to do it several times to get a sense of the proportion. Explain to these new-to-statistics students why their high correlation based on 4-5 inch average difference is not correct. Instead of simulating this probability, how otherwsie would you arrive at the probability/propprtion? If you chose to use a ‘model’, to do so, address the issue of how critical your assumptions are, and whether this approach is robust. data from NHIS on this topic caveats. why did Pearson get correlations that were so much stronger. https://fivethirtyeight.com/features/how-common-is-it-for-a-man-to-be-shorter-than-his-partner/ 19.2.7 Same-sex or opposite-sex? This 4-panel exercise is modelled on one JH used in the 607 course in 2001, but now using data from all US births in 2018 The data are in this data.frame, and shown in 4 panels numbered A-D. The birthweights within a panel pertain to infants of the same sex, but different panels may pertain to different sexes. ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ Panel : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Birthweight: int 2809 2186 2607 3231 3484 2836 863 3329 3628 3551 ... Figure 19.5: Birthweights of US males and females born in 2018. In each panel, each entry is one of the 100 (sex-specific) quantiles Q0.5%, Q1.5%, Q2.5%, Q49.5%, Q50.5%, etc, up to Q99.5%, so you can think of it as representing 1 percent of the births of that sex. In each panel, the Q’s are scrambled. By eye, by comparing all the entries in one panel with all of those in another, you may be able to discern if two panels have different means (or medians). But what can you conclude if you take just a sample from each of 2 panels and perform a formal null hypothesis test using the difference in the sample means or the sample medians, or the ranks? In 2001, students were asked to perform tests of the following 4 (obviously overlapping, so not completely independent) contrasts: \\(\\mu_A\\) vs. \\(\\mu_B\\) \\(\\quad\\) 2. \\(\\mu_C\\) vs. \\(\\mu_D\\) \\(\\quad\\) 3. \\(\\mu_A\\) vs. \\(\\mu_D\\) \\(\\quad\\) 4. \\(\\mu_B\\) vs. \\(\\mu_C\\) They were asked to use new (fresh, independent) samples of size \\(n\\) = 4 and \\(n\\) = 4 for each of the 4 tests. These sample sizes will (rightly) strike you as being much too small to reliably distinguish two populations whose means are not that far apart. Indeed, most of the students in 2001 were unable to correctly identify the opposite-sex panels But, when it comes to distinguishing same-sex panels from opposite-sex panels of adult heights, most of the students in 2001 were able to correctly identify the opposite-sex panels, with only a few mistaking the same-sex panels for opposite-sex ones [we are in charge of controlling the latter errors, whereas (with \\(n\\) fixed) Nature is in charge of the success rate with the former]. Although it isn’t realistic, students – like so many textbooks start with – were told that the Population Standard Deviation (\\(\\sigma\\)) was known. The main reason for being allowed this ‘inside knowledge’ (typically reserved for Gods and Oracles) was that it made the computation of the (z-) test statistic easier: students only had compute the two sample means, easily done by hand, and avoided having to compute sample standard deviations. [It also fits with the practice of teaching the simpler z-tests, with their fixed quantiles, first, and then moving to the sample-size-dependent t-based quantiles later]. For now, we will do the same, but later will also go on and carry out real-world tests that don’t rely on ‘inside’ information about standard deviations. [After all, if you know the Population Standard Deviation, then you must already know the Population Mean.] ‘ARITHMETIC’ of Testing if panels ‘1’ and ‘2’ have same mean Null Hypothesis: H\\(_0: \\ \\mu_1 = \\mu_2\\) [same sex, or, technically speaking, same mean] Alternative Hypothesis H\\(_{alt}: \\mu_1 \\ne \\mu_2\\) [different means, thus opposite sexes] Data: sample means \\(\\bar{y}_1\\) and \\(\\bar{y}_2,\\) bases on sample sizes \\(n_1\\) and \\(n_2\\) Test Statistic: \\[z = (\\bar{y}_1- \\bar{y}_2) / SD[\\bar{y}_1 - \\bar{y}_2],\\] where \\[SD[\\bar{y}_1 - \\bar{y}_2] = \\sqrt{\\sigma^2_1/n_1 + \\sigma^2_2/n_2} \\ .\\] Probability, P that, under \\(H_0\\), the test statistic would exceed the absolute value of the observed \\(z\\) statistic. [the absolute value is used when H\\(_{alt}\\) is ‘2-sided’.] Pre-specified probability, \\(\\alpha\\), below which H\\(_0\\) is ‘rejected’. [by convention, it is often taken to be 0.05; we will use \\(\\alpha\\) = 0.10, so that our simulations are almost sure to yield some ‘false positives’] In the case where \\(n_1 = n_2 = 4\\), and \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, we can work out ahead of time what minimum difference between \\(\\bar{y}_1\\) and \\(\\bar{y}_2\\) would lead us to reject the null (same-sex, or same mean) hypothesis. With \\(\\alpha\\) = 0.10, the ’critical value of \\(z\\) is \\(\\pm\\) 1.645, i.e, the two sample means would need to be at least 1.645 \\(\\times SD[\\bar{y}_1 - \\bar{y}_2]\\) apart. In this simplified and somewhat unrealistic example, where we don’t know if we are in the same-sex or opposite sex scenario, we don’t know whether we should use \\(\\sigma_F\\) = 574 grams and \\(\\sigma_M\\) = 600 grams, or 574 and 574, or 601 and 601. So we will use a ‘compromise’ \\(\\sigma_1\\) = \\(\\sigma_2\\) = 587. Substituting these into the \\(SD[\\bar{y}_1 - \\bar{y}_2]\\) formula yields an SD of \\(\\sqrt{587^2/4 + 587^2/4}\\) = 415 grams. Thus the critical distance between the two sample means needs to be 1.645 \\(\\times\\) 415 = 683 grams. For interest, here is what the critical distance would be for other sample sizes: \\(n_1 = n_2 = 16:\\) 341 grams;  \\(n_1 = n_2 = 64:\\) 171 grams;  \\(n_1 = n_2 = 256:\\) 85 grams. [Notice the root of n law in action.] Whereas, as an individual ‘investigator’, you might be luck or unlucky in which of the 4 comparisons you get right, the real issue is the probabilities of getting them right. In 2001 JH had each student in the large 607 class try to distinguish the panels, and he tabulated their results so that we can get a better sense of what these probabilities are. And better still, in 2020, you alone can use the computer to get an even better sense. We will use the same 2 x 2 table used in 2001, where the 2 rows are the 2 truths, i.e, the comparions was in fact a same-sex or opposite-sex comparison, and the 2 columns are the two possible ‘results’ of the statistical test, i.e., it was ‘negative’, with a P-value \\(\\ge\\) 0.1, (the diffrence in the sample means was not ‘statistically significant’) ; or ‘positive’ with a P-value \\(&lt;\\) 0.1, the difference was ‘statistically significant’, large enough that the null hypothesis of same-sex populations (same-population-means) was ‘rejected’. When you review each test result, you can only classify it by column, by ‘negative’ or ‘positive’. And in real life, that would be as far as you can go, since your are not privy to the ‘truth’. But, here, we have the luxury of knowing which comparisons are of each type, so we can cross-classify the results, and also examine the average performance of these tests over many may applications. In this way, we get a feel for the probabilities that they will perfrtom they way we want them to, and can see how the probabilities are modified by the amount of effort (sample size) we are willing to employ, as well as how small/large a signal, and how small/large the noise, we are up against. So, for now, in your one set of 4 hand-worked examples, you can put the 4 test results into two columns. Below, when we will do a large number of tests, we will reveal which comparisons are of which type, so you can doubly-classify them. Exercises For each of the 4 contrasts listed above, use a ‘reproducible’ random selection method to choose 4 infants from each of the two specified panels. [Note: since each entry represents almost 200,000 infects, the sampling from the 100 values (quantiles) should be with replacement. This may not matter a lot when you are sampling just 4 infants, but if you were sampling 16 or 64 or 256, it would matter]. For each contrast, compute the difference between the sample means and note whether it was a ‘statistically significant’ difference (i.e. one of 683 grams or more. These 4 ‘test tesults’ can be the first entries in your 2 x 2 table below, once the identity of each 4 panel (the ‘truth’) is revealed. (2-way) distribution of results of statistical tests [columns] in relation to real situations[rows] Test Result Total ‘Negative’ ‘Positive’ \\(P \\ge 0.1\\) \\(P \\lt 0.1\\) SAME-SEX \\(number\\) \\(number\\) TRUTH OPPOSITE \\(number\\) \\(number\\) ……… ……… ……… TOTAL \\(no._{neg.}\\) \\(no._{pos.}\\) TOTAL The following R code can simulate the results obtained by many many investigators. For now, the ‘many’ is just 40, close to the numbers performed by students in 2001. Although this number can only give a rough idea, using sample sizes of size 4 doesn’t look that promising! No matter which contrast, only about 10% of the statistical tests are ‘positive’. Since we set the \\(\\alpha\\) value, we expect that about 10% of tests are positive when we are testing two same-sex panels, but we would hope that a far greater % would be positive when we are testing opposite-sex panels. SimulateOppositeSexTests= function(Values, n.simulated.investigations = 40, n = 4, SD.individuals = 583, contrasts = t(matrix(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;,&quot;D&quot;,&quot;A&quot;,&quot;D&quot;,&quot;B&quot;,&quot;C&quot;),2,4)) ) { n.contrasts = dim(contrasts)[1] # n.simulated.investigations per contrast *** MODIFY *** # n = sample size from each of 2 contrasted panels *** MODIFY *** results = matrix(NA,n.simulated.investigations,n.contrasts) alpha=0.10 # set by us # SD.individuals = xxx, not usual to KNOW the population SD SD.Mean.diff = SD.individuals * sqrt(1/n+1/n) # variance(diff) = Sum of Vars. critical.diff = abs( qnorm(alpha/2) * SD.Mean.diff ) #print( aggregate(Values, by=list(P=ds$Panel), mean) ) #print( aggregate(Values, by=list(P=ds$Panel), sd) ) for(i in 1:n.simulated.investigations){ for (j in 1:n.contrasts){ pair = contrasts[j,] sample.mean = rep(NA,2) for(pair.member in 1:2){ all.values.this.panel = Values[ ds$Panel==pair[pair.member] ] sample.mean[pair.member] = mean( sample(x=all.values.this.panel, size=n, replace = TRUE) ) } difference = sample.mean[1] - sample.mean[2] sig.difference = ( abs(difference) &gt; critical.diff ) results[i,j] = sig.difference } # end of contrasts } # end of investigation txt = paste(&quot;/&quot;,toString(n.simulated.investigations)) DIGITS = ceiling(log(n.simulated.investigations,base=10)) positives = format( apply(results,2,sum), digits = DIGITS) cat(paste(&quot;samples of size:&quot;,toString(n))) noquote( cbind( contrasts[,1],rep(&quot; vs. &quot;,n.contrasts), contrasts[,2], rep(&quot; &quot;,n.contrasts), positives, rep(txt,n.contrasts) ) ) } # end function SimulateOppositeSexTests(ds$Birthweight, n.simulated.investigations = 50, n = 4, SD.individuals = 583, contrasts = t(matrix(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;,&quot;D&quot;,&quot;A&quot;,&quot;D&quot;,&quot;B&quot;,&quot;C&quot;),2,4))) ## samples of size: 4 ## positives ## [1,] A vs. B 4 / 50 ## [2,] C vs. D 6 / 50 ## [3,] A vs. D 7 / 50 ## [4,] B vs. C 2 / 50 Exercises - continued Cross-classify the above results into the following 2 x 2 table, knowing which pairs of panels are same-sex and which are opposite-sex to tell you which row of the table a contrast belong to. Run the R code yourself, but with a much larger number of simulated investigations. Cross-classify your results. Even though you know there must be some non-zero difference in the sex-specific means population means, why are the results so dismal? See also the results in 2001. How much better is the performance if you increase the \\(n\\) to 64? 256? (2-way) distribution of results of statistical tests [columns] in relation to real situations[rows] Test Result Total ‘Negative’ ‘Positive’ \\(P \\ge 0.1\\) \\(P \\lt 0.1\\) SAME-SEX \\(number\\) \\(number\\) TRUTH OPPOSITE \\(number\\) \\(number\\) ……… ……… ……… TOTAL \\(no._{neg.}\\) \\(no._{pos.}\\) TOTAL Exercises - continued CAN YOU TELL THE GROWN-UPS APART? The following panels are 100 sex-specific quantiles of weight (kg) of the US Population aged 20-29 years in 2007 to 2008, based on NHANES surveys. JH used interpolation (and a small bit of extrapolation at the lower edges) to create the 100 quantiles, \\(Q_{0.005}\\), \\(Q_{0.015}\\), \\(\\dots\\) \\(Q_{0.995}\\). Aagin, they are identified as A-D, with the weights within a panel pertain to 20-29 year olds of the same sex, but different panels may pertain to different sexes. The layout is not necessarily the same as it was above for birthweights! Cross-classify the above results into the following 2 x 2 table, knowing which pairs of panels are same-sex and which are opposite-sex to tell you which row of the table a contrast belong to. ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ Panel : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Weight.Kg: num 62.5 47.5 113.5 149.5 108.5 ... Figure 19.6: Measured weights of US males and females aged 20-29 in 2007-2008. In each panel, each entry is one of the 100 (sex-specific) quantiles Q0.5%, Q1.5%, Q2.5%, Q49.5%, Q50.5%, etc, up to Q99.5%, so you can think of it as representing 1 percent of the population of that sex. In each panel, the Q’s are scrambled, and ‘jittered’. By eye, by comparing all the entries in one panel with all of those in another, you can readily tell if two panels have different means (or medians). But what can you conclude if you take just a sample of size 4 from each of 2 panels and perform a formal null hypothesis test using the difference in the sample means or the sample medians, or the ranks? Exercises - continued Your are asked to perform tests of the following 4 contrasts: \\(\\mu_A\\) vs. \\(\\mu_C\\) \\(\\quad\\) 2. \\(\\mu_B\\) vs. \\(\\mu_D\\) \\(\\quad\\) 3. \\(\\mu_A\\) vs. \\(\\mu_D\\) \\(\\quad\\) 4. \\(\\mu_B\\) vs. \\(\\mu_C\\) using new (fresh, independent) samples of size \\(n\\) = 4 and \\(n\\) = 4 for each of the 4 tests. NB: 1 and 2 are not the same as 1 and 2 for birthweight above. Start by manually performing the 4 tests. Begin by calculating what the critical distance between the two sample means needs to be. Use a ‘common’ \\(\\sigma\\) = 21.4 Kg for each sex. [Again, it should be obvious why having this ‘insider’ information is not realistic! It is just to simplify the test to its essentials; later, in practice, we will use ‘plug-in’ values instead of the ‘real’ \\(\\sigma\\) and use a larger (\\(n\\)-based) multiplier to allow for using ‘estimated’, and thus ‘uncertain’, \\(\\sigma\\)’s.] How confident are you in the results of these tests? (Remember, you KNOW that males are heavier than females; that is ‘prior’ information that the test doesn’t know: it just uses the data you give it.) Run the same R code you ran above, with a large number of simulated investigations. In real life, you don’t get to see ‘what might have been’; you just get to see the one pair of samples you selected. But here the (simulated) ‘long-run’ perfomance gives you sense of how sensitive your test procedure is [you already know how specific it is, by your choice of \\(\\alpha\\)]. Then, using this key cross-classify your results. How much better did the test do here, than it did with birthweights? Even though the \\(n\\)’s are still just 4 each, why are they better? SimulateOppositeSexTests(ds$Weight.Kg, n.simulated.investigations = 50, n = 4, SD.individuals = 21.4, contrasts = t(matrix(c(&quot;A&quot;,&quot;C&quot;, &quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;D&quot;,&quot;B&quot;,&quot;C&quot;),2,4))) ## samples of size: 4 ## positives ## [1,] A vs. C 9 / 50 ## [2,] B vs. D 8 / 50 ## [3,] A vs. D 13 / 50 ## [4,] B vs. C 5 / 50 CAN YOU TELL THE 20-SOMETHINGS APART BY THEIR HEIGHTS? ds = read.table( &quot;http://www.biostat.mcgill.ca/hanley/statbook/USheights100Qs20072008.txt&quot;, as.is=TRUE) str(ds) ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ Panel : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Height.cm: int 175 173 168 181 186 189 179 171 161 169 ... Figure 19.7: Measured heights (in cm) of US males and females aged 20-29 in 2007-2008. In each panel, each entry is one of the 100 (sex-specific) quantiles Q0.5%, Q1.5%, Q2.5%, Q49.5%, Q50.5%, etc, up to Q99.5%, so you can think of it as representing 1 percent of the population of that sex. In each panel, the Q’s are scrambled. Here are the test results SimulateOppositeSexTests(ds$Height.cm, n.simulated.investigations = 50, n = 4, SD.individuals = 7.3, contrasts = t(matrix(c(&quot;A&quot;,&quot;C&quot;, &quot;B&quot;,&quot;D&quot;,&quot;A&quot;,&quot;D&quot;,&quot;B&quot;,&quot;C&quot;),2,4))) ## samples of size: 4 ## positives ## [1,] A vs. C 9 / 50 ## [2,] B vs. D 5 / 50 ## [3,] A vs. D 32 / 50 ## [4,] B vs. C 32 / 50 Exercises - continued Using the (common) SD of 7.3 cm, carry out 4 manually-calculated tests of the same 4 contrasts as for adult weights. (although the computer-simulated ones are giving you a sense that height is a more sensitive distinguisher/separator of the sexes than adult weight is, carry out a larger set of simulated tests to be sure. Statistically speaking, why is height a more sensitive distinguisher/separator of the sexes? Hint: how far apart are the mean heights (a) in cms (b) as a percentage of the SD of the individuals of a given sex? What are the corresponding perecenatges in the case of birthweights? adult weights? What we are fetting at here is the signal-to-noise ratio or Effect Size \\[ \\frac{Signal}{Noise} = \\frac{\\mu_1 - \\mu_2}{\\sigma}.\\] In 2001, when it came to distinguishing same-sex panels from opposite-sex panels of adult heights, a clear majority (63/76) of the students were able to correctly identify the opposite-sex panels, with only a few (8/75) mistaking the same-sex panels for opposite-sex ones. To try to answer why we do not do as well with our data, compute the sex-specific means and SD’s of the 100 quantiles, and compare the effect sizes with those in 2001. You might also make boxplots to see if the shapes of the data differ [JH doesn’t remember how he made the 100 values in 2001, but for sure the ones above are closer to the real shapes.] To save you manually extracting them, the data from 2001 can be found here. Statistically speaking, why are the ‘false positive rates’ about the same with the new data and the 2001 data. Hint: Who determines them? Biologically speaking, why is adult height a more sensitive distinguisher/separator of the sexes than adult weight? Hint: think about the relative roles of Nature and Nurture – and while you are on this topic, look up the origins of the Nature versus Nurture phrase. 19.2.8 Box minus bowl 19.2.9 car parking fixed and variable 19.2.10 Correcting length-biased sampling ===================== 19.3 Other Exercises (under construction) Exercises Variation with and without constraints Pearson vs Fisher Ice Breakup Dates Here are some details on the Nenana Ice Classic More here 19.3.1 The 2018 Book of Guesses We are keen to establish the distribution of guesses, with the guessed times measured from midnight on December 31, 2017. Thus a guess of April 06, at 5:15 p.m. would be measured as 31 + 28 + 31 + 5 + 12/24 + (5+ 15/60)/24 = 95.71875 days into the year 2018. It would be tedious to apply optical character recognition (OCR) to each of the 1210 pages in order to be able to computerize all 240,000 guesses. Instead, you are asked to reconstruct the distribution of the guesses in two more economical ways: By determining, for (the beginning of) each day, from April 01 to June 01 inclusive, the proportion, p, of guesses that predede that date. [ In R, if p = 39.6% of the guesses were below 110 days, we would write this as pGuessDistribution(110) = 0.396. Thus, if we were dealing with the location of a value in a Gaussian (‘normal’) distribution, we would write pnorm(q=110, mean = , sd = ) ] Once you have determined them, plot these 62 p’s (on the vertical axis) against the numbers of elapsed days (90-152) on the horizontal axis. By determining the 1st, 2nd, … , 98th, 99th percentiles. These are specific examples of ‘quantiles’, or q’s. The q-th quantile is the value (here the elapsed number of days since the beginning of 2018) such that a proportion q of all values are below this value, and 1-q are above it. [ In R, if 40% of the guesses were below 110.2 days, we would write this as qGuessDistribution(p=0.4) = 110.2 days. Thus, if we were dealing with the 40th percentile of a Gaussian distribution with mean 130 and standard deviation 15, we would write qnorm(p=0.4, mean = 130, sd = 15). ] Once you have determined them, plot the 99 p’s (on the vertical axis) against the 99 (elapsed) times on the horizontal axis. Compare the Q\\(_{25}\\), Q\\(_{50}\\), and Q\\(_{75}\\) obtained directly with the ones obtained by interpolation of the curve showing the results of the other method. Compare the directly-obtained proportions of guesses that are before April 15, April 30, and May 15 with the ones obtained by interpolation of the curve showing the results of the other method. By successive subtractions, calculate the numbers of guesses in each 1-day bin, and make a histogram of them. From them, calculate the mean, the mode, and the standard deviation. To measure the spread of guesses, Galton, in his vox populi (wisdom of crowds) article, began with the interquartile range (IQR), i.e. the distance between Q75 and Q25, the 3rd and 1st quartiles. In any distribution, 1/2 the values are within what was called the ’probable error (PE) of the mean; i.e., it is equally probable that a randomly selected value would be inside or outside this middle-50 interval. Today, we use standard deviation (SD) instead of probable error. In a Gaussian distribution, some 68% of values are within 1 SD of the mean, whereas 50% of values are within 1 PE of the mean. We can use R to figure out how big a PE is in a Gaussian distribution compared with a SD. By setting the SD to 1, and the eman to 0, we have Q75 = qnorm(p = 0.75, mean=0, sd=1) round(Q75,2) ## [1] 0.67 i.e, a PE is roughtly 2/3rds of a SD. Galton – convert to SD. Geometric mean Amsterdam study How far off was the median guess in 2018 from the actual time? Answer in days, and (with reservations stated) as a percentage? Why did the experts at the country fair do so much better? Where were the punters in 2019 wrt the actual ? https://www.technologyreview.com/s/528941/forget-the-wisdom-of-crowds-neurobiologists-reveal-the-wisdom-of-the-confident/ https://www.all-about-psychology.com/the-wisdom-of-crowds.html http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf [Nenana Ice Classic] Tanana River 19.3.2 Trends over the last 100 years fill in the data since 200x. cbind with … https://www.adn.com/alaska-news/2019/04/14/nenana-ice-classic-tripod-goes-down-setting-record-for-earliest-river-break-up/ time trends morning vs afternoon ? 2019 extreme… how many SD’s from the line? Where were the punters in 2019 wrt the actual ? Sources: 1917-2003 data in textfile on course website; http://www.nenanaakiceclassic.com/ for data past 2003 ascii (txt) and excel files with data to 2003 Working in teams of two … *Create a dataframe containing the breakup data for the years 1917-2007. Possible ways to do so include: directly from the ascii (txt) file; from the Excel file *From the decimal portion of the Julian time, use R to create a frequency table of the hour of the day at which the breakup occurred. *From the month and day, use R to calculate your own version of the Julian day (and the decimal portion if you want to go further and use the hour and minute) *Is there visual evidence that over the last 91 years, the breakup is occurring at an earlier date? *Extract the date and ice thickness measurements for the years 1989-2007 from the website and use (your software of choice) R to create a single dataset with the 3 variable, year, day and thickness. From this, fit a separate trendline for each year, and calculate the variability of these within-year slopes. Galton’s data on family heights These data were gathered to examine the relation between heights of parents and heights of their (adult) children. They have been recently ‘uncovered’ from the Galton archives. As a first issue, for this exercise, you are also asked to see whether the parent data suggest that stature plays “a sensible part in marriage selection”. For the purposes of this exercise, the parent data [see http://www.epi.mcgill.ca/hanley/galton ] are in a file called parents.txt , with families numbered 1-135, 136A, 136-204 ( {the heights of the adult offspring will be used in a future exercise) Do the following tasks using R Categorize each father’s height into one of 3 bins (shortest 1/4, middle 1/2, tallest 1/4). Do likewise for mothers. Then, as Galton did [ Table III ], obtain the 2-way frequency distribution and assess whether “we may regard the married folk as picked out of the general population at haphazard”. Calculate the variance Var[F] and Var[M] of the fathers’ [F] and mothers’ [M] heights respectively. Then create a new variable consisting of the sum of F and M, and calculate Var[F+M]. Comment. Galton called this a “shrewder” test than the “ruder” one he used in 1. When Galton first anayzed these data in 1885-1886, Galton and Pearson hadn’t yet invented the correlation coefficient. Calculate this coefficient and see how it compares with your impressions in 1 and 2. Temperature perceptions Create 5 datasets from the questionnaire data on temperature perceptions etc. by importing directly from the Excel file applied to .csv version of Excel file); by first removing the first row (of variable names) and exporting the Excel file into a ’comma-separated-values&quot; (.csv) text file, then … reading the data in this .csv file via the INFILE and INPUT statements in a SAS DATA step, [SAS] INFILE ‘path’ DELIMITER =“,”; INPUT ID MALE $ MD $ EXAM TEMPOUTC TEMPINC TEMPOUTF TEMPINF TEMPFEEL TIME PLACE $ ; by reading the data in the text file temps_1.txt into the SAS dataset via the INFILE and INPUT statements. Notice that the ‘missing’ values use the SAS representation (.) for missing values. or the Stata dataset using the ‘infile’ command by reading the data in the text file temps_2.txt via [in SAS] the INFILE and INPUT statements in a DATA step or [in Stata] the ‘infix’ command. Here you will need to be careful, since ‘free-format’ will not work correctly (it is worth trying free format with this file, just to see what goes wrong!). When using the INFILE method, you can control some of the damage by using the ‘MISSOVER’ option in the INFILE statement: this keeps the INPUT statement from continuing on into the next data line in order to find the (in our example) 11 values implied by the variable list. JH uses this ‘defensive’ option in ALL of his INFILE statements. by cutting and pasting the contents of the text file temps_2.txt directly into the SAS or Stata program - in SASthe lines of data go immediately after the DATALINES statement, and there needs to be a line containing a semicolon to indicate the end of the data stream. In Stata, the lines of data go immediately after the infile or infix statement, and there needs to be a line containing the word ‘end’ to indicate the end of the data stream This Cut and Paste Method is NOT RECOMMENDED when the number of observations is large, as it is too all too easy to inadvertently alter the data, and the SAS/Stata porogram becomes quite long and unwieldy. It is Good Data Management Practice to separate the program statements from the data. [Run [in SAS] PROC MEANS [in Stata] the ‘describe’ command, on the numerical variables, and [in SAS] PROC FREQ or [in Stata] the ‘tabulate’ command, on the non-numerical variables, to check that the 5 datasets you created contain the same information. Also, get in the habit of viewing or printing several observations and checking the entries against the ‘source’. When using (i), have SAS show you the SAS statements generated by the wizard. Store these, and the DATA steps for (ii) to (v) in a single SAS program file (with suffix .sas). Annotate liberally using comments: in SAS, either begin with * ; or enclose with /* … */ in Stata ..begin the line with * or place the comment between /* and */ delimiters or begin the comment with // or begin the comment with /// Q2 Use one of these 5 datasets, and the appropriate [in SAS, PROCs (see Exploring Data under UCLA SAS Class Notes 2.0)], or [in Stata, the list comamnd, and the analyses from the Statistics menu] to list the names and characteristics of the variables list the first 5 observations in the dataset list the id # and the responses just to q3, w5 and q6, for all respondents, with respondents in the order: female MDs, male MDs, female non-MDs, male non-MDs. Indicate the [sub-]statement that is required to reverse this order. create a 2-way frequency table, showing the frequencies of respondents in each of the 2 (MD nonMD) x 2 (male female) = 4 ‘cells’ (one defintion of an epidemiologist is ‘an MD broken down by age and sex’). Turn off all the extra printed output, so that the table just has the cell frequencies and the row and column totals. compare the mean and median attitude to exams in MDs vs. non-MDs (hint: in SAS, the CLASS statement may help). Get SAS/Stata to limit the output to just the ‘n’, the min, the max, the mean and the median for each subgroup. And try to also get it to limit the number of decimal places of output (in SAS the MAXDEC option is implememnted in some procedures, but as far as JH can determine not in all) compare the mean temperature perceptions (q6) of male and female respondents [in SAS] create a low-res (‘typewriter’ resolution) scatterplot of the responses to q5 (vertical axis) vs. q4 (horizonatal axis), using a plotting symbol that shows whether the responsdent is a male or a female. If we have not covered how to show this ‘3rd dimension’, look at the ONLINE Documentation file {the guide for most of the procedures covered in this set of exercises is in the Base SAS Procedures Guide; other procedures are in sthe more advanced ‘STAT’ module}. You can specify the variable whose values are to mark each point on the plot. See PLOT statement in PROC PLOT, and the example with variables height weight and gender. [in Stata] use the (automatically hi-res) graphics capabilities available from the ‘Graphics’ menu [if SAS] Put all of the programs for Q1, and all of these program steps and output for Q2 in a single .txt file (JH will use a mono-spaced font such as Courier to view it – that way the alignment should be OK), with PROC statements interleaved with output, and a helpful 2-line title (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line (that way, lines won’t wrap-around when JH views the material). [if Stata] paste the results and graphics into Word. NOTE: To be fair to SAS, it CAN produce decent (and even some publication-quality) graphics. See http://www.ats.ucla.edu/stat/sas/topics/graphics.htm Then submit the text file electronically (i.e., by email) to JH by 9 am on Monday October 2. Natural history of prostate cancer Q1 The following data items are from an investigation into the natural history of (untreated) prostate cancer [ report (.pdf) by Albertsen Hanley Gleason and Barry in JAMA in September 1998 ]. id, dates of birth and diagnosis, Gleason score, date of last contact, status (1=dead, 0=alive), and – if dead – cause of death (see 2b below). data file (.txt) for a random 1/2 of the 767 patients Compute the distribution of age at diagnosis (5-year intervals) and year of diagnosis (5 year intervals). Also compute the mean and median ages at diagnosis. For each of the 20 cells in Table 2 (5 Gleason score categories x 4 age-at-dx categories), compute the number of man-years (M-Y) of observation number of deaths from prostate cancer(1), other causes(2), unknown causes(3) prostate cancer(1) death rate [ deaths per 100 M-Y ] proportion who survived at least 15 years. For a and b you can use the ‘sum’ option in PROC means; ie PROC MEANS data = … SUM; VAR vars you want to sum; BY the 2 variables that form the cross-classification. Also think of a count as a sum of 0s and 1s. For c (to avoid having to compute 20 rates by hand), you can ‘pipe’ i.e. re-direct the sums to a new sas datafile, where you can then divide one by other to get (20) rates. Use OUTPUT OUT = …. SUM= …names for two sums; On a single graph, plot the 5 Kaplan-Meier survival curves, one for each of the 5 Gleason score categories (PROC LIFETEST .. Online help is under the SAS STAT module, or see http://www.ats.ucla.edu/stat/sas/seminars/sas_survival/default.htm. For Stata, see http://www.ats.ucla.edu/stat/stata/seminars/stata_survival/default.htm. [OPTIONAL] In order to compare the death rates with those of U.S. men of the same age, for each combination of calendar year period (1970-1974, 1975-1979, …, 1994-1999) and 5 year age-interval (55-59, 60-64, … obtain the number of man-years of follow-up and the number of deaths. Do so by creating, from the record for each man, as many separate observations as the number of 5yr x 5yr “squares” that the man traverses diagonally through the Lexis diagram [ use the OUTPUT statement within the DATA step]. Then use PROC MEANS to aggregate the M-Y and deaths in each square. If you get stuck, here is some SAS code that does this, or see the algorithm given in Breslow and Day, Volume II, page ___ Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday Nov 7. Serial PSA values Q1 These two files contain PSA values [pre-] and [post-] treatment of prostate cancer *. Create a ‘wide’ PSA file of 25 log-base-2 PSA values per man (some will be missing, if PSA not measured 25 times). Print some excerpts. (b)From the dataset created in (a), create a long file, with just the observations containing the non-missing log-base-2 PSA values [OUTPUT statement in DATA step]. Print and plot some excerpts. From the dataset created in (b), create a wide file [ RETAIN, first. and last. helpful here; or use PROC TRANSPOSE ]. Print some excerpts. The order of the variables is given in this sas program . Some of the code in the program may also be of help. Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday Nov 14. Graphics 1a Re-produce (or if you think you can, improve on) three of the graphs shown in “Examples of graphs from Medical Journals.” These examples are in a pdf file on the main page. Use Excel for at least one of them, and R/Stata/SAS for at least one other. Do not go to extraordinary lengths to make them exactly like those shown – the authors, or the journals themselves, may have used more specialized graphics software. You may wish to annotate them by making (and sharing with us) notes on those steps/options that were not immediately obvious and that took you some effort to figure out. Insert all three into a single electronic document. 1b Browse some medical and epidemiologic journals and some magazines and newspapers published in the last 12 months, Identify the statistical graph you think is the worst, and the one you think is the best. Tell us how many graphs you looked at, and why you chose the two you did. If you find a helpful online guide or textbook on how to make good statistical graphs, please share the reference with us. [The bios601 site http://www.epi.mcgill.ca/hanley/bios601/DescriptiveStatistics/ has a link to the Textbook by Cleveland and the book “R Graphics” by Paul Murrell. If possible, electronically paste the graphs into the same electronic file you are using for 1a. 2 [OPTIONAL] The main page has a link to a lifetable workbook containing three sheets. Note that the ‘lifetable’ sheet in this workbook is used to calculate an abridged current life table based on the 1960 U.S. data. Use this sheet as a guideline, and create a current life-table (‘complete’, i.e., with 1-year age-intervals) for Canadian males, using the male population sizes, and numbers of deaths, by age, Canada 2001. [The calculations in columns O to W of the lifetable sheet are not relevant for this exercise]. Details on the elements of, and the construction of current lifetables can be found in the chapters (on website) from the textbooks by Bradford Hill and Selvin, and in the technical notes provided by the US National Center for Health Statistics in connection with US Lifetable 2000. See also the FAQ for 613 from 2005. The fact that the template is for an abridged life table, with mostly 5-year intervals, whereas the task is to construct a full lifetable with 1 year intervals, caused some people problems last year.. they realized something was wrong when the life expectancy values were way off! Since this is an exercise, and not a calculation for an insurance company that wants to have 4 sig. decimal places, don’t overly fuss about what values of ‘a’ you use for the early years.. they don’t influence the calculations THAT much: If you try different sets of values (such as 0.1 in first year and 0.5 thereafter) you will not find a big impact. But don’t take my word for it .. the beauty of a spreadsheet is that you can quickly see the consequences of different assumptions or ‘what ifs’. [In practice, in order not to be unduly influenced by mortality rates in a single calendar year (e.g. one that had a very bad influenza season), current lifetables are usually based on several years of mortality data. Otherwise, or if they are based on a small population, the quantities derived from them will exhibit considerable random fluctuations from year to year ] Once you have completed the table, use the charting facilities in Excel to plot the survival curve for the hypothetical (fictitious) male ‘cohort’ represented by the current lifetable. On a separate graph, use two histograms to show the distributions of the ages at death (i) for this hypothetical male ‘cohort’ and (ii) those males who died in 2001. To make it easy to compare them, superimpose the histograms or put them ‘side by side’ or ‘back to back’ within the same graph. Explain why the two differ in shape and location. Calculate/derive (and include them somewhere on the spreadsheet) the median and mean age at death in the hypothetical cohort and the corresponding statistics for the actual deaths in 2001. Possible Body Mass Indices This exercise investigates different definitions of Body Mass Index (BMI). BACKGROUND: With weight measured in Kilograms, and height in metres, BMI is usually defined as weight divided by the SQUARE of height, i.e., BMI = Wt / (Height*Height), or BMI = Wt/(height2) using, as SAS and several other programming languages do, the symbol for ‘raised to the power of’. [ NB: Excel uses ^ to denote this ] What’s special about the power of 2? Why not a power of 1 i.e., Weight/height? Why not 3, i.e., Weight/*(height3) ? Why not 2.5 i.e. Weight/(height2.5)? One of the statistical aims of a transformation of weight and height to BMI is that BMI be statistically less correlated with height, thereby separating height and height into two more useful components height and BMI. For example in predicting lung function (e.g. FEV1), it makes more sense to use height and BMI than height and weight, since weight has 2 components in it – it is partly height and partly BMI. Presumably, one would choose the power which minimizes the correlation. The task in this project is to investigate the influence of the power of height used in the ratio, and to see if the pattern of correlations with power is stable over different settings (datasets). DATA: To do this, use 2 of the 6 datasets on the 678 webpage: [usernane is c678 and p w is H**J44 ] Children aged 11-16 Alberta 1985 (under ‘Datasets’) 18 year olds in Berkeley longitudinal study, born 1928/29 (under ‘Datasets’) Dataset on bodyfat – 252 men (see documentation) (under ‘Datasets’) Pulse Rates before and after Exercise – Australian undergraduates in 1990’s (under ‘Projects’) Miss America dataset 1921-2000 (under ‘Resources’) Playboy dataset 1929-2000 (under ‘Resources’) METHODS: First create each of the two SAS datasets, and if height and weight are not already in metres and Kg, convert them to these units. Drop any irrelevant variables. Inside each dataset, create a variable giving the source of the data (we will merge the two – and eventually all six– datasets, so we need to be able to tell which one each observation came from). Combine the two datasets, i.e. ‘stack’ them one above the other in a single dataset. Print out some excerpts. For each subject in the combined dataset, create 5 versions of &lt; using the powers 1, 1.5, 2, 2.5 and 3. Calculate the correlation between the ‘BMI’ obtained with each of these powers, and height. Do this separately for the observations from the two different sources (the BY statement should help here). Report your CONCLUSIONS. Galton The objective of this exercise is to examine the relation between heights of parents and heights of their (adult) children, using recently ‘uncovered’ data from the Galton archives, You are asked to assess if Galton’s way of dealing with the fact that heights of males and females are quite different produces sharper correlations than we would obtain using ‘modern’ methods of dealing with this fact. As side issues, you are also asked to see whether the data suggest that stature plays “a sensible part in marriage selection” and to comment on the correlations of the heights in the 4 {father,son}, {father,daughter}, {mother,son} and {mother,daughter} pairings. BACKGROUND: Galton ‘transmuted’ female heights into their ‘male-equivalents’ by multiplying them by 1.08, and then using a single combined ‘uni-sex’ dataset of 900-something offspring and their parents. While some modern-day anayysts would simply calculate separate correlations for the male and female offspring (and then average the two correlations, as in a meta-analysis), most would use the combined dataset but ‘partial out’ the male-females differences using a multivariable analysis procedure. The various multivariable procedures in effect create a unisex dataset by adding a fixed number of inches to each female’s height (or, equivalently, in the words of one of our female PhD students, by ‘cutting the men down to size’). JH was impressed by the more elegant ‘proportional scaling’ in the ‘multiplicative model’ used by Galton, compared with the ‘just use the additive models most readiliy available in the software’ attitude that is common today. In 2001, he located the raw (untransmuted) data that allows us to compare the two approaches. DATA: For the purposes of this exercise, the data [see http://www.epi.mcgill.ca/hanley/galton ] are in two separate files: the heights# of 205 sets of parents ( parents.txt ) with families numbered 1-135, 136A, 136-204 the heights# of their 900-something* children ( offspring.txt ) with families numbered as above The data on eight families are deliberately omitted, to entice the scholar in you to get into the habit of looking at (and even double checking) the original data. Since here we are more interested in the computing part in this course, and because time is short, ignore this invitation to inspect the data – we already had a look at them in class. In practice, we often add in ‘missing data’ later, as there are always some problem cases, or lab tests that have to be repeated, or values that need to be checked, or subjects who didn’t get measured at the same time as others etc.. JH’s habit is to make the additions in the ‘source’ file (.txt or .xls or whatever) and re-run the entire SAS DATA step(s) to create the updated SAS dataset (temporary or permanent). If the existing SAS datset is already large, and took a lot of time to create, you might consider creating a small dataset with the new observations, and then stacking (using SE) the new one under the existing one – in a new file. SAS has fancier ways too, and others may do things differently! If your connection is too slow to view the photo of the first page of the Notebook, the title reads FAMILY HEIGHTS (add 60 inches to every entry in the Table) METHODS/RESULTS/COMMENTS: Categorize each father’s height into one of 3 subgroups (shortest 1/4, middle 1/2, tallest 1/4). Do likewise for mothers. Then, as Galton did [ Table III ], obtain the 2-way frequency distribution and assess whether “we may regard the married fold as picked out of the general population at haphazard”. Calculate the variance Var[F] and Var[M] of the fathers’ [F] and mothers’ [M] heights respectively. Then create a new variable consisting of the sum of F and M, and calculate Var[F+M]. Comment. Galton called this a “shrewder” test than the “ruder” one he used in 1. ( statistic-keyword VAR in PROC MEANS) When Galton first anayzed these data in 1885-1886, Galton and Pearson hadn’t yet invented the CORRelation coefficient. Calculate this coefficient and see how it compares with your impressions in 1 and 2. Create two versions of the transmuted mother’s heights, one using Galton’s and one using the modern-day (lazy-person’s, blackbox?) additive scaling [for the latter, use the observed difference in the average heights of fathers and mothers, which you can get by e.g., running PROC MEANS on the offspring dataset, either BY gender, or using gender as a CLASS variable]. In which version of the transmuted mothers’ heights is their SD more simlar to the SD of the fathers? ( statistic-keyword STD in PROC MEANS) Create the two corresponding versions of what Galton called the ‘mid-parent’ (ie the average of the height of the father and the height of the transmuted mother). Take mid-point to mean the half-way point (so in this case the average of the two) Create the corresponding two versions (additive and multiplicative scaling) of the offspring heights (note than sons’ heights remain ‘as is’). Address again, but now for daughters vs sons, the question raised at the end of 4. Merge the parental and offspring datasets created in steps 4 and 6, taking care to have the correct parents matched with each offspring (this is called a 1:many merge). Using the versions based on 1.08, round the offspring and mid-parent heighs to the nearest inch (or use the FLOOR function to just keep the integer part of the mid-parent height –you need not be as fussy as Galton was about the groupings of the mid-parent heights), and obtain a 2-way frequency distribution similar to that obtained by Galton [ Table I ]. Note that, opposite to we might do today, Galton put the parents on the vertical, and the offspring on the horizontal axis. ( The MOD INT FLOOR CEIL and ROUND functions can help you map observations into ‘bins’ ; we will later see a way to do so using loops) Galton called the offspring in the same row of his table a ‘filial array’. Find the median height for each filial array, and plot it, as Galton did, against the midpoint of the interval containing their midparent – you should have one datapoint for each array. Put the mid-parent values on the vertical, and the offspring on the horizontal axis. By eye, estimate the slope of the line of best fit to the datapoints. Mark your fitted line by ‘manually’ inserting two markers at the opposite corners of the plot. Does the slope of your fitted line agree with Galton’s summary of the degree of “regression to mediocrity”? [ Plate IX ] Note that Galton used datapoints for just 9 filial arrays, choosing to omit those in the bottom and top rows (those with the very shortest and the very tallest parents) because the data in these arrays were sparse. ( By using the binned parental height in the CLASS statement in PROC MEANS or PROC UNIVARIATE, directing the output to a new SAS dataset, and applying PROC PLOT to this new dataset, you can avoid having to do the plotting manually See more on this in the FAQ) Plot the individual unisex offspring heights (daughters additively transmuted) versus the mid-parent height (mothers transmuted). OVERLAY on it, with a different plotting symbol, the corresponding plot involving the multiplicatively transmuted offspring values (on the parent-axis, stay with Galton’s definition of a midparent). (see FAQ) Compare the two, and have a look at Galton’s fitted ellipse, corresponding to a bivariate normal distribution [ Plate X ]) {here, again, we would be more likely to plot the parents’ heights on the horizontal, and the offspring heights on the vertical axis}. For each of the following ‘offspring vs. mid-parent’ correlations, use the ‘mid-parent’ obtained using Galton’s multiplicative method. Calculate (a) the 2 correlations for the 2 unisex versions of the offspring data (b) the sex-specific correlations (i.e., daughters and sons separately) and (c) the single parent-offspring correlation, based on all offspring combined, and their untransmuted heights, ignoring the sex of the offspring. Comment on the correlations obtained, and on the instances where there are big disparities between them. [ a PLOT, with separate plotting symbols for sons and daughters, might help in the case of (c) ] Calculate the 4 correlations (i) father,son (ii) father,daughter, (iii) mother,son and (iv) mother,daughter. Comment on the pattern, and on why you think it turned out this way. Put all of the program steps and output into a single .txt file. JH will use a mono-spaced font such as Courier to view it – that way the alignment should be ok. Interleave DATA and PROC statements with output and conclusions, and use helpful titles (produced by SAS, but to your specifications) over top of each output. Get SAS to set up the output so that there are no more that 65 horizontal characters per line – that way, lines won’t wrap-around even when the font used to view your file is increased. Show relevant excerpts rather than entire listings of datafiles. Annotate liberally. Submit the text file electronically (i.e., by email) to JH by 9 am on Monday October 30. Lottery payoffs sampling of children in school Detecting a fake Bernoulli sequence Cell occupancy "],
["dalite.html", "Chapter 20 DALITE 20.1 Aim 20.2 How it works", " Chapter 20 DALITE 20.1 Aim 20.2 How it works "]
]
